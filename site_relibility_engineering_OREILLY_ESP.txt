Prólogo

La historia de Google es una historia de crecimiento. Es una de las grandes historias de éxito de la industria informática, que marca un cambio hacia un negocio centrado en TI. Google fue una de las primeras empresas en definir lo que significaba la alineación entre negocios y TI en la práctica, y posteriormente informó el concepto de DevOps para una comunidad de TI más amplia. Este libro ha sido escrito por un amplio grupo de personas que hicieron que esa transición se convirtiera en realidad.

Google creció en un momento en que el papel tradicional del administrador de sistemas estaba siendo transformado. Cuestionó la administración de sistemas, como si dijera: no podemos permitirnos seguir la tradición como autoridad, tenemos que pensar de nuevo y no tenemos tiempo para esperar a que todos los demás se pongan al día. En la introducción a Principios de administración de redes y sistemas [Bur99], afirmé que la administración de sistemas era una forma de ingeniería humana-computadora. Esto fue rechazado enérgicamente por algunos revisores, que dijeron "todavía no estamos en la etapa en que podemos llamarlo ingeniería". En ese momento, sentí que el campo se había perdido, atrapado en su propia cultura de magos, y no podía ver una forma de avanzar. Luego, Google dibujó una línea en el silicio, forzando que ese destino se convirtiera en realidad. El papel revisado se llamó SRE, o Ingeniero de Confiabilidad del Sitio. Algunos de mis amigos estuvieron entre los primeros de esta nueva generación de ingenieros; lo formalizaron utilizando software y automatización. Inicialmente, eran muy secretos, y lo que sucedió dentro y fuera de Google era muy diferente: la experiencia de Google era única. Con el tiempo, la información y los métodos han fluido en ambas direcciones. Este libro muestra una voluntad de dejar que el pensamiento SRE salga de las sombras.

Aquí, no solo vemos cómo Google construyó su legendaria infraestructura, sino también cómo estudió, aprendió y cambió de opinión sobre las herramientas y las tecnologías en el camino. También nosotros podemos enfrentar desafíos abrumadores con un espíritu abierto. La naturaleza tribal de la cultura de TI a menudo atrapa a los practicantes en posiciones dogmáticas que frenan a la industria. Si Google superó esta inercia, también podemos hacerlo.

Este libro es una colección de ensayos de una sola empresa, con una visión común. El hecho de que las contribuciones estén alineadas en torno a un objetivo común de la empresa es lo que lo hace especial. Hay temas comunes y personajes comunes (sistemas de software) que reaparecen en varios capítulos. Vemos opciones de Diferentes perspectivas, y sabemos que se correlacionan para resolver intereses en competencia. Los artículos no son piezas rigurosas y académicas; son relatos personales, escritos con orgullo, en una variedad de estilos personales y desde la perspectiva de habilidades individuales. Están escritos con valentía y con una honestidad intelectual que es refrescante y poco común en la literatura industrial. Algunos afirman "nunca hagas esto, siempre haz aquello", otros son más filosóficos y tentativos, reflejando la variedad de personalidades dentro de una cultura de TI, y cómo eso también juega un papel en la historia. Nosotros, a su vez, los leemos con la humildad de observadores que no fueron parte del viaje y no tienen toda la información sobre los numerosos desafíos en conflicto. Nuestras muchas preguntas son el verdadero legado del volumen: ¿Por qué no hicieron X? ¿Qué pasaría si hubieran hecho Y? ¿Cómo miraremos hacia atrás en esto en el futuro? Es comparando nuestras propias ideas con la lógica aquí que podemos medir nuestros propios pensamientos y experiencias.

Lo más impresionante de todo sobre este libro es su mera existencia. Hoy en día, escuchamos una cultura descarada de "solo muéstrame el código". Una cultura de "no hagas preguntas" ha crecido alrededor del código abierto, donde la comunidad en lugar de la experiencia es la campeona. Google es una empresa que se atrevió a pensar en los problemas desde principios fundamentales y a emplear a los mejores talentos con una gran proporción de doctorados. Las herramientas eran solo componentes en procesos, trabajando junto con cadenas de software, personas y datos. Nada aquí nos dice cómo resolver problemas universalmente, pero ese es el punto. Historias como estas son mucho más valiosas que el código o diseños que resultaron en ellas. Las implementaciones son efímeras, pero la lógica documentada es invaluable. Rara vez tenemos acceso a este tipo de información.

Este, entonces, es el relato de cómo una empresa lo hizo. El hecho de que sea muchas historias superpuestas nos muestra que escalar es mucho más que solo una ampliación fotográfica de una arquitectura de computadora de texto. Se trata de escalar un proceso empresarial, en lugar de solo la maquinaria. Esta lección sola vale su peso en papel electrónico.

No nos involucramos mucho en la revisión crítica de nosotros mismos en el mundo de la TI; como tal, hay mucha reinvención y repetición. Durante muchos años, solo había la comunidad de la conferencia USENIX LISA discutiendo la infraestructura de TI, más algunas conferencias sobre sistemas operativos. Hoy en día es muy diferente, sin embargo, este libro todavía se siente como una oferta rara: una documentación detallada del paso de Google a través de una época decisiva. La historia no es para copiar— aunque tal vez para emular— pero puede inspirar el próximo paso para todos nosotros. Hay una honestidad intelectual única en estas páginas, expresando tanto liderazgo como humildad. Estas son historias de esperanzas, miedos, éxitos y fracasos. Saludo el coraje de los autores y editores al permitir tal candor, para que nosotros, que no somos parte de las experiencias prácticas, también podamos beneficiarnos de las lecciones aprendidas dentro del capullo.

Mark Burgess


Prefacio

La ingeniería de software tiene esto en común con tener hijos: el trabajo antes del nacimiento es doloroso y difícil, pero el trabajo después del nacimiento es donde se invierte la mayor parte del esfuerzo. Sin embargo, la ingeniería de software como disciplina dedica mucho más tiempo a hablar sobre el primer período en lugar del segundo, a pesar de que se estima que el 40-90% de los costos totales de un sistema se incurren después del nacimiento. El modelo popular de la industria que concibe el software desplegado y operativo como "estabilizado" en producción y, por lo tanto, necesitando mucha menos atención de los ingenieros de software, es incorrecto. A través de esta lente, vemos que si la ingeniería de software tiende a enfocarse en diseñar y construir sistemas de software, debe haber otra disciplina que se enfoque en todo el ciclo de vida de los objetos de software, desde la concepción hasta la descomisión pacífica. Esta disciplina utiliza - y necesita utilizar - una amplia gama de habilidades, pero tiene preocupaciones separadas de otros tipos de ingenieros. Hoy en día, nuestra respuesta es la disciplina que Google llama Ingeniería de Confiabilidad del Sitio (SRE).

Entonces, ¿qué es exactamente la Ingeniería de Confiabilidad del Sitio (SRE)? Admitimos que no es un nombre particularmente claro para lo que hacemos - casi todos los ingenieros de confiabilidad del sitio en Google se les pregunta qué es exactamente eso y qué hacen en realidad, con regularidad.

Desempaquetando el término un poco, primero y principalmente, los SRE son ingenieros. Aplicamos los principios de la ciencia de la computación y la ingeniería al diseño y desarrollo de sistemas de cómputo: generalmente, grandes y distribuidos. A veces, nuestra tarea es escribir el software para esos sistemas junto con nuestros compañeros de desarrollo de productos; a veces, nuestra tarea es construir todas las piezas adicionales que esos sistemas necesitan, como copias de seguridad o equilibrio de carga, idealmente para que puedan ser reutilizadas en sistemas; y a veces, nuestra tarea es descubrir cómo aplicar soluciones existentes a nuevos problemas.

A continuación, nos enfocamos en la confiabilidad del sistema. Ben Treynor Sloss, vicepresidente de Google para operaciones 24/7, originador del término SRE, afirma que la confiabilidad es la característica más fundamental de cualquier producto: un sistema no es muy útil si nadie puede usarlo. Debido a que la confiabilidad es tan crítica, los SRE se enfocan en encontrar formas de mejorar el diseño y la operación de los sistemas para hacerlos más escalables, más confiables y más eficientes. Sin embargo, solo invertimos esfuerzo en esta dirección hasta cierto punto: cuando los sistemas son "lo suficientemente confiables", invertimos nuestros esfuerzos en agregar características o construir nuevos productos.

Finalmente, los SRE se enfocan en operar servicios construidos sobre nuestros sistemas de cómputo distribuidos, ya sean servicios de almacenamiento a escala planetaria, correo electrónico para cientos de millones de usuarios o donde Google comenzó, la búsqueda web. El "sitio" en nuestro nombre originalmente se refería al papel de SRE en mantener el sitio web google.com en funcionamiento, aunque ahora ejecutamos muchos más servicios, muchos de los cuales no son sitios web - desde infraestructura interna como Bigtable hasta productos para desarrolladores externos como la plataforma Google Cloud.

Aunque hemos representado a SRE como una disciplina amplia, no es sorprendente que haya surgido en el mundo en constante movimiento de los servicios web, y tal vez en su origen deba algo a las peculiaridades de nuestra infraestructura. Es igualmente no sorprendente que de todas las características post-despliegue del software que podríamos elegir para dedicar atención especial, la confiabilidad sea la que consideramos primaria.

A pesar de surgir en Google y en la comunidad de servicios web en general, creemos que esta disciplina tiene lecciones aplicables a otras comunidades y organizaciones. Este libro es un intento de explicar cómo hacemos las cosas: tanto para que otras organizaciones puedan aprovechar lo que hemos aprendido, como para que podamos definir mejor el papel y lo que significa el término. Con ese fin, hemos organizado el libro para que los principios generales y las prácticas más específicas Para ese fin, hemos organizado el libro de manera que los principios generales y las prácticas más específicas estén separados cuando sea posible, y cuando sea apropiado discutir un tema particular con información específica de Google, confiamos en que el lector nos permitirá hacerlo y no tendrá miedo de sacar conclusiones útiles sobre su propio entorno.

También hemos proporcionado algunos materiales de orientación - una descripción del entorno de producción de Google y un mapa entre algunos de nuestros software internos y software público - que deberían ayudar a contextualizar lo que estamos diciendo y hacerlo más directamente utilizable.

En última instancia, por supuesto, un software y una ingeniería de sistemas más orientados a la confiabilidad son inherentemente buenos. Sin embargo, reconocemos que las organizaciones más pequeñas pueden estar preguntándose cómo pueden aprovechar al máximo la experiencia representada aquí: al igual que la seguridad, cuanto antes se preocupe por la confiabilidad, mejor. Esto implica que aunque una organización pequeña tenga muchas preocupaciones urgentes y las opciones de software que tome pueden diferir de las que tomó Google, todavía vale la pena implementar un soporte de confiabilidad ligero desde el principio, porque es menos costoso ampliar una estructura más adelante que introducir una que no está presente. La gestión contiene un número de mejores prácticas para la formación, la comunicación y las reuniones que hemos encontrado que funcionan bien para nosotros, muchas de las cuales deberían ser inmediatamente utilizables por su organización.

Pero para tamaños entre una startup y una multinacional, probablemente ya hay alguien en su organización que está haciendo trabajo de SRE, sin que necesariamente se llame así o se reconozca como tal. Otra forma de comenzar a mejorar la confiabilidad para su organización es reconocer formalmente ese trabajo, o encontrar a esas personas y fomentar lo que hacen - recompensarlo. Son personas que se encuentran en la encrucijada entre una forma de ver el mundo y otra: como Newton, que a veces se llama no el primer físico del mundo, sino el último alquimista.

Y tomando la vista histórica, ¿quién, entonces, mirando hacia atrás, podría ser el primer SRE? Nos gusta pensar que Margaret Hamilton, trabajando en el programa Apolo en préstamo de MIT, tenía todos los rasgos significativos del primer SRE. En sus propias palabras, "parte de la cultura era aprender de todos y de todo, incluyendo de lo que menos se esperaría".

Un caso en punto fue cuando su joven hija Lauren vino a trabajar con ella un día, mientras algunos del equipo estaban ejecutando escenarios de misión en la computadora de simulación híbrida. Como los niños pequeños hacen, Lauren se fue explorando y causó que una "misión" se estrellara al seleccionar las teclas DSKY de una manera inesperada, alertando al equipo sobre lo que sucedería si el programa de prelanzamiento, P01, fuera seleccionado inadvertidamente por un astronauta real durante una misión real, durante el curso real. (Lanzar P01 inadvertidamente en una misión real sería un problema mayor, porque borra los datos de navegación, y la computadora no estaba equipada para pilotar la nave con datos de navegación inexistentes).

Con los instintos de un SRE, Margaret presentó una solicitud de cambio de programa para agregar un código de verificación de errores especial en el software de vuelo a bordo en caso de que un astronauta seleccionara P01 inadvertidamente durante una misión real. Pero este movimiento fue considerado innecesario por los "superiores" de la NASA: ¡por supuesto, eso nunca podría suceder! Entonces, en lugar de agregar un código de verificación de errores, Margaret actualizó la documentación de las especificaciones de la misión para decir lo equivalente a "No seleccione P01 durante el vuelo". (Al parecer, la actualización fue divertida para muchos en el proyecto, quienes habían sido informados muchas veces de que los astronautas no cometerían errores - después de todo, habían sido entrenados para ser perfectos).

Bueno, la salvaguarda sugerida por Margaret solo fue considerada innecesaria hasta la siguiente misión, en el Apolo 8, justo días después de la actualización de las especificaciones. Durante el curso medio del cuarto día de vuelo con los astronautas Jim Lovell, William Anders y Frank Borman a bordo, Jim Lovell seleccionó P01 por error - como sucede, en Navidad - creando mucha confusión para todos los involucrados. Este fue un problema crítico, porque en ausencia de una solución alternativa, no había datos de navegación, lo que significaba que los astronautas nunca regresarían a casa. Afortunadamente, la actualización de la documentación había llamado explícitamente a esta posibilidad, y fue invaluable para averiguar cómo subir datos útiles y recuperar la misión, con poco tiempo que perder.

Como dice Margaret, "una comprensión exhaustiva de cómo operar los sistemas no fue suficiente para prevenir errores humanos", y la solicitud de cambio para agregar software de detección y recuperación de errores al programa de prelanzamiento P01 fue aprobada poco después.

Aunque el incidente del Apolo 8 ocurrió hace décadas, hay mucho en los párrafos anteriores directamente relevante para la vida de los ingenieros hoy en día, y mucho que seguirá siendo directamente relevante en el futuro. Por lo tanto, para los sistemas que cuidan, para los grupos en los que trabajan o para las organizaciones que están construyendo, por favor, tengan en cuenta el Camino SRE: la exhaustividad y la dedicación, la creencia en el valor de la preparación y la documentación, y la conciencia de lo que podría salir mal, junto con un fuerte deseo de prevenirlo. ¡Bienvenidos a nuestra profesión emergente!

Este libro es una serie de ensayos escritos por miembros y ex miembros de la organización de Ingeniería de Confiabilidad del Sitio (SRE) de Google. Es más similar a las actas de una conferencia que a un libro estándar escrito por un autor o un pequeño número de autores. Cada capítulo está diseñado para ser leído como parte de un todo coherente, pero se puede obtener mucho leyendo sobre cualquier tema que te interese en particular. (Si hay otros artículos que apoyan o informan el texto, los citamos para que puedas seguir adelante).

No necesitas leer en un orden particular, aunque sugerimos al menos comenzar con los capítulos "El entorno de producción en Google, desde la perspectiva de un SRE" y "Aceptando el riesgo", que describen el entorno de producción de Google y esbozan cómo SRE aborda el riesgo, respectivamente. (El riesgo es, en muchos sentidos, la calidad clave de nuestra profesión). Leer de principio a fin es, por supuesto, también útil y posible; nuestros capítulos están agrupados temáticamente en Principios, Prácticas y Gestión. Cada uno tiene una pequeña introducción que destaca de qué tratan las piezas individuales y hace referencia a otros artículos publicados por SREs de Google, que cubren temas específicos en más detalle. Además, el sitio web complementario de este libro, https://g.co/SREBook, tiene una serie de recursos útiles.

Esperamos que esto sea al menos tan útil e interesante para ti como lo fue para nosotros reunirlo.

— Los editores


Parte I - Introducción

Esta sección proporciona algunas orientaciones generales sobre qué es SRE y por qué es diferente de las prácticas más convencionales de la industria de TI.

Ben Treynor Sloss, el vicepresidente senior que supervisa las operaciones técnicas en Google - y el originador del término "Ingeniería de Confiabilidad del Sitio" (SRE) - proporciona su visión sobre qué significa SRE, cómo funciona y cómo se compara con otras formas de hacer las cosas en la industria, en la Introducción.

Proporcionamos una guía sobre el entorno de producción en Google en El entorno de producción en Google, desde la perspectiva de un SRE, como una forma de ayudarte a familiarizarte con la gran cantidad de nuevos términos y sistemas que vas a conocer en el resto del libro.

Capítulo 1 - Introducción

En este capítulo, Ben Treynor Sloss presenta su visión sobre qué es SRE y cómo funciona. También compara SRE con otras formas de hacer las cosas en la industria de TI y explica por qué SRE es diferente.

El capítulo comienza con una definición de SRE y una explicación de cómo se originó el término. Luego, Treynor Sloss describe los principios clave de SRE, incluyendo la importancia de la confiabilidad, la escalabilidad y la eficiencia.

También se discute la relación entre SRE y la ingeniería de software, y cómo SRE se enfoca en la operación y el mantenimiento de los sistemas en lugar de solo en su desarrollo. Treynor Sloss también destaca la importancia de la colaboración y la comunicación entre los equipos de SRE y otros equipos en la organización.

Finalmente, el capítulo concluye con una discusión sobre los beneficios de SRE y cómo puede ayudar a las organizaciones a mejorar la confiabilidad y la eficiencia de sus sistemas.

Capítulo 1 - Introducción

Escrito por Benjamin Treynor Sloss
Editado por Betsy Beyer

La esperanza no es una estrategia.
Dicho tradicional de SRE

Es una verdad universalmente reconocida que los sistemas no se ejecutan solos. ¿Cómo, entonces, debería ejecutarse un sistema, particularmente un sistema de cómputo complejo que opera a gran escala?

El enfoque de Sysadmin para la gestión de servicios
Históricamente, las empresas han empleado administradores de sistemas para ejecutar sistemas de cómputo complejos.

Este enfoque de administrador de sistemas, o sysadmin, implica ensamblar componentes de software existentes y desplegarlos para que trabajen juntos para producir un servicio. Los sysadmins luego son responsables de ejecutar el servicio y responder a eventos y actualizaciones a medida que ocurren. A medida que el sistema crece en complejidad y volumen de tráfico, generando un aumento correspondiente en eventos y actualizaciones, el equipo de sysadmins crece para absorber el trabajo adicional. Debido a que el papel de sysadmin requiere un conjunto de habilidades muy diferente al de los desarrolladores de productos, los desarrolladores y los sysadmins están divididos en equipos discretos: "desarrollo" y "operaciones" o "ops".

El modelo de gestión de servicios de sysadmin tiene varias ventajas. Para las empresas que deciden cómo ejecutar y dotar de personal a un servicio, este enfoque es relativamente fácil de implementar: como un paradigma industrial familiar, hay muchos ejemplos de los que aprender y emular. Hay un talento disponible en todo el mundo. Hay una variedad de herramientas, componentes de software (de estantería o no) y empresas de integración disponibles para ayudar a ejecutar esos sistemas ensamblados, por lo que un equipo de sysadmins novatos no tiene que reinventar la rueda y diseñar un sistema desde cero.

El enfoque de sysadmin y la división de desarrollo/ops que lo acompaña tienen una serie de desventajas y trampas. Estas se dividen en dos categorías: costos directos e indirectos.

Los costos directos no son sutiles ni ambiguos. Ejecutar un servicio con un equipo que depende de la intervención manual para la gestión de cambios y el manejo de eventos se vuelve caro a medida que el servicio y/o el tráfico al servicio crecen, porque el tamaño del equipo debe escalar con la carga generada por el sistema.

Los costos indirectos de la división de desarrollo/ops pueden ser sutiles, pero a menudo son más costosos para la organización que los costos directos. Estos costos surgen del hecho de que los dos equipos son muy diferentes en antecedentes, habilidades y incentivos. Utilizan un vocabulario diferente para describir situaciones; llevan diferentes suposiciones sobre riesgos y posibilidades de soluciones técnicas; tienen diferentes suposiciones sobre el nivel de estabilidad del producto objetivo. La división entre los grupos puede fácilmente convertirse en una de no solo incentivos, sino también comunicación, objetivos y eventualmente, confianza y respeto. Este resultado es una patología.

Los equipos de operaciones tradicionales y sus contrapartes en el desarrollo de productos a menudo terminan en conflicto, más visible sobre cuán rápido se puede lanzar software a producción. En su núcleo, los equipos de desarrollo quieren lanzar nuevas características y verlas adoptadas por los usuarios. En su núcleo, los equipos de operaciones quieren asegurarse de que el servicio no se rompa mientras están sujetando la página. Debido a que la mayoría de las interrupciones son causadas por algún tipo de cambio, un lanzamiento de características nuevo, un lanzamiento de configuración nuevo o un tipo nuevo de tráfico de usuarios, los objetivos de los dos equipos están fundamentalmente en tensión.

Ambos grupos entienden que es inaceptable expresar sus intereses en los términos más brutales posibles ("Queremos lanzar cualquier cosa, en cualquier momento, sin obstáculos" versus "No queremos cambiar nada en el sistema una vez que funciona"). Y porque su vocabulario y suposiciones de riesgo difieren, ambos grupos a menudo recurren a una forma familiar de guerra de trincheras para avanzar en sus intereses. El equipo de operaciones intenta salvaguardar el sistema en ejecución contra el riesgo de cambio introduciendo puertas de lanzamiento y cambio. 



GOOGLE APPROACH TO SERVICE MANAGEMENT: SITE RELIABILITY ENGINEERING

Por El conflicto no es una parte inevitable de ofrecer un servicio de software. Google ha elegido ejecutar nuestros sistemas con un enfoque diferente: nuestros equipos de Ingeniería de Confiabilidad del Sitio (SRE) se centran en contratar ingenieros de software para ejecutar nuestros productos y crear sistemas para realizar el trabajo que normalmente sería realizado, a menudo de manera manual, por administradores de sistemas.

¿Qué es exactamente la Ingeniería de Confiabilidad del Sitio, como se ha definido en Google? Mi explicación es simple: SRE es lo que sucede cuando se le pide a un ingeniero de software que diseñe un equipo de operaciones. Cuando me uní a Google en 2003 y me encargué de ejecutar un "Equipo de Producción" de siete ingenieros, toda mi vida hasta ese momento había sido ingeniería de software. Así que diseñé y gestioné el grupo de la manera en que querría que funcionara si yo mismo trabajara como SRE. Ese grupo ha madurado desde entonces para convertirse en el equipo de SRE de Google actual, que sigue siendo fiel a sus orígenes como lo imaginó un ingeniero de software de toda la vida.

Un bloque de construcción fundamental del enfoque de Google para la gestión de servicios es la composición de cada equipo de SRE. En general, los SRE se pueden dividir en dos categorías principales.

50-60% son ingenieros de software de Google, o más precisamente, personas que han sido contratadas a través del procedimiento estándar para ingenieros de software de Google. El otro 40-50% son candidatos que estaban muy cerca de las calificaciones de ingeniería de software de Google (es decir, 85-99% del conjunto de habilidades requeridas), y que además tenían un conjunto de habilidades técnicas que es útil para SRE pero es raro para la mayoría de los ingenieros de software. Por lejos, los conocimientos de internos del sistema UNIX y la experiencia en redes (Capa 1 a Capa 3) son los dos tipos más comunes de habilidades técnicas alternativas que buscamos.

Común a todos los SRE es la creencia y la aptitud para desarrollar sistemas de software para resolver problemas complejos. Dentro de SRE, seguimos de cerca el progreso de carrera de ambos grupos, y hasta la fecha no hemos encontrado ninguna diferencia práctica en el rendimiento entre ingenieros de las dos vías. De hecho, el fondo diverso del equipo de SRE a menudo resulta en sistemas inteligentes y de alta calidad que son claramente el producto de la síntesis de varios conjuntos de habilidades.

El resultado de nuestro enfoque para contratar personal para SRE es que terminamos con un equipo de personas que (a) rápidamente se aburren de realizar tareas manuales, y (b) tienen las habilidades necesarias para escribir software que reemplace su trabajo manual anterior, incluso cuando la solución es complicada. Los SREs también comparten un trasfondo académico e intelectual con el resto de la organización de desarrollo. Por lo tanto, SRE realiza fundamentalmente el trabajo que históricamente ha sido hecho por un equipo de operaciones, pero utilizando ingenieros con experiencia en software, apostando por el hecho de que estos ingenieros están inherentemente predispuestos y tienen la capacidad para diseñar e implementar automatización mediante software para reemplazar el trabajo humano.

Por diseño, es crucial que los equipos de SRE se enfoquen en la ingeniería. Sin una ingeniería constante, la carga de operaciones aumenta y los equipos necesitarán más personas solo para mantenerse al día con la carga de trabajo. Eventualmente, un grupo tradicional enfocado en operaciones escala linealmente con el tamaño del servicio: si los productos apoyados por el servicio tienen éxito, la carga operativa crecerá con el tráfico. Esto significa contratar más personas para hacer las mismas tareas una y otra vez.

Para evitar este destino, el equipo encargado de gestionar un servicio necesita programar o se ahogará. Por lo tanto, Google impone un límite del 50% al trabajo operativo agregado para todos los SREs—tickets, guardias, tareas manuales, etc. Este límite asegura que el equipo de SRE tenga suficiente tiempo en su agenda para hacer que el servicio sea estable y operable. Este límite es un límite superior; con el tiempo, si se les deja a su suerte, el equipo de SRE debería terminar con muy poca carga operativa y casi dedicarse por completo a tareas de desarrollo, porque el servicio básicamente se ejecuta y repara solo: queremos sistemas que sean automáticos, no solo automatizados. En la práctica, la escala y las nuevas características mantienen a los SREs en alerta.

La regla general de Google es que un equipo de SRE debe dedicar el 50% restante de su tiempo realmente al desarrollo. Entonces, ¿cómo hacemos cumplir ese umbral? En primer lugar, tenemos que medir cómo se gasta el tiempo de los SRE. Con esa medición en mano, aseguramos que los equipos que consistentemente dedican menos del 50% de su tiempo al trabajo de desarrollo cambien sus prácticas. A menudo, esto significa trasladar parte de la carga operativa de vuelta al equipo de desarrollo, o agregar personal al equipo sin asignarles responsabilidades operativas adicionales. Mantener conscientemente este equilibrio entre el trabajo operativo y el de desarrollo nos permite asegurarnos de que los SREs tengan el ancho de banda para involucrarse en la ingeniería creativa y autónoma, mientras retienen la sabiduría adquirida del lado operativo de la gestión de un servicio.

Hemos encontrado que el enfoque de Google SRE para operar sistemas a gran escala tiene muchas ventajas. Debido a que los SREs están modificando directamente el código en su búsqueda de hacer que los sistemas de Google se ejecuten por sí mismos, los equipos de SRE se caracterizan por una innovación rápida y una gran aceptación del cambio. Dichos equipos son relativamente económicos—apoyar el mismo servicio con un equipo orientado a operaciones requeriría un número significativamente mayor de personas. En cambio, el número de SREs necesarios para ejecutar, mantener y mejorar un sistema escala de manera sublineal con el tamaño del sistema. Finalmente, SRE no solo evita la disfuncionalidad de la división dev/ops, sino que esta estructura también mejora nuestros equipos de desarrollo de productos: las transferencias fáciles entre desarrollo de productos y equipos de SRE entrenan transversalmente a todo el grupo, y mejoran las habilidades de los desarrolladores que de otra manera podrían tener dificultades para aprender a construir un sistema distribuido de millones de núcleos.

A pesar de estas ganancias netas, el modelo SRE se caracteriza por su propio conjunto distintivo de desafíos. Un desafío continuo que enfrenta Google es la contratación de SREs: SRE no solo compite por los mismos candidatos que el pipeline de contratación de desarrollo de productos, sino que el hecho de que establecemos un estándar de contratación tan alto en términos de habilidades tanto de codificación como de ingeniería de sistemas significa que nuestro grupo de candidatos es necesariamente pequeño. Dado que nuestra disciplina es relativamente nueva y única, no existe mucha información en la industria sobre cómo construir y gestionar un equipo de SRE (aunque esperamos que este libro avance en esa dirección). Y una vez que un equipo de SRE está en su lugar, sus enfoques potencialmente poco ortodoxos para la gestión de servicios requieren un fuerte apoyo de la gerencia. Por ejemplo, la decisión de detener lanzamientos durante el resto del trimestre una vez que se ha agotado el presupuesto de errores podría no ser bien recibida por un equipo de desarrollo de productos, a menos que sea ordenada por su gerencia.

¿DevOps o SRE?

El término "DevOps" surgió en la industria a finales de 2008 y, al momento de escribir este texto (principios de 2016), aún se encuentra en evolución. Sus principios fundamentales—la participación de la función de TI en cada fase del diseño y desarrollo de un sistema, la fuerte dependencia de la automatización en lugar del esfuerzo humano, y la aplicación de prácticas y herramientas de ingeniería a tareas operativas—son consistentes con muchos de los principios y prácticas de SRE. Se podría considerar DevOps como una generalización de varios principios fundamentales de SRE para un rango más amplio de organizaciones, estructuras de gestión y personal. De manera equivalente, se podría ver a SRE como una implementación específica de DevOps con algunas extensiones idiosincráticas.

Aunque los matices de los flujos de trabajo, las prioridades y las operaciones diarias varían de un equipo de SRE a otro, todos comparten un conjunto de responsabilidades básicas para los servicios que apoyan y se adhieren a los mismos principios fundamentales. En general, un equipo de SRE es responsable de la disponibilidad, latencia, rendimiento, eficiencia, gestión de cambios, monitoreo, respuesta a emergencias y planificación de la capacidad de sus servicios. Hemos codificado reglas de compromiso y principios sobre cómo los equipos de SRE interactúan con su entorno, no solo con el entorno de producción, sino también con los equipos de desarrollo de productos, los equipos de pruebas, los usuarios, y más. Estas reglas y prácticas de trabajo nos ayudan a mantener nuestro enfoque en el trabajo de ingeniería, en lugar del trabajo de operaciones.

Asegurando un Enfoque Duradero en la Ingeniería

Como ya se ha mencionado, Google limita el trabajo operativo de los SREs al 50% de su tiempo. El tiempo restante debe dedicarse a utilizar sus habilidades de codificación en proyectos. En la práctica, esto se logra monitoreando la cantidad de trabajo operativo que realizan los SREs y redirigiendo el exceso de trabajo operativo a los equipos de desarrollo de productos: reasignando errores y tickets a los gerentes de desarrollo, reintegrando a los desarrolladores en las rotaciones de guardia, y así sucesivamente. La redirección termina cuando la carga operativa vuelve a bajar al 50% o menos. Esto también proporciona un mecanismo de retroalimentación eficaz, guiando a los desarrolladores a construir sistemas que no necesiten intervención manual. Este enfoque funciona bien cuando toda la organización, tanto SRE como desarrollo, entiende por qué existe el mecanismo de válvula de seguridad y apoya el objetivo de no tener eventos de desbordamiento porque el producto no genera suficiente carga operativa para requerirlo.

Cuando se enfocan en el trabajo operativo, en promedio, los SREs deberían recibir un máximo de dos eventos por turno de guardia de 8 a 12 horas. Este volumen objetivo da al ingeniero de guardia suficiente tiempo para manejar el evento con precisión y rapidez, limpiar y restaurar el servicio normal, y luego realizar un análisis post-mortem. Si ocurren más de dos eventos regularmente por turno de guardia, los problemas no pueden investigarse a fondo y los ingenieros están lo suficientemente abrumados como para impedirles aprender de estos eventos. Un escenario de fatiga por llamadas tampoco mejorará con la escala. Por el contrario, si los SREs de guardia consistentemente reciben menos de un evento por turno, mantenerlos en alerta es una pérdida de tiempo.

Se deben escribir análisis post-mortem para todos los incidentes significativos, independientemente de si activaron una llamada o no; los análisis post-mortem que no desencadenaron una llamada son aún más valiosos, ya que probablemente señalan claras lagunas en el monitoreo. Esta investigación debe establecer lo que ocurrió en detalle, encontrar todas las causas raíz del evento y asignar acciones para corregir el problema o mejorar cómo se abordará la próxima vez. Google opera bajo una cultura de post-mortem sin culpas, con el objetivo de exponer fallos y aplicar ingeniería para corregirlos, en lugar de evitarlos o minimizarlos.

Persiguiendo la Máxima Velocidad de Cambio Sin Violar el SLO de un Servicio

Los equipos de desarrollo de productos y SRE pueden disfrutar de una relación de trabajo productiva eliminando el conflicto estructural en sus objetivos respectivos. El conflicto estructural se da entre el ritmo de innovación y la estabilidad del producto, y como se describió anteriormente, este conflicto a menudo se expresa de manera indirecta. En SRE, llevamos este conflicto al primer plano y lo resolvemos mediante la introducción de un presupuesto de errores (error budget).

El presupuesto de errores surge de la observación de que el 100% no es el objetivo de confiabilidad correcto para prácticamente todo (siendo los marcapasos y los frenos antibloqueo notables excepciones). En general, para cualquier servicio o sistema de software, el 100% no es el objetivo de confiabilidad adecuado porque ningún usuario puede notar la diferencia entre un sistema que está disponible el 100% del tiempo y uno que lo está el 99.999%. Hay muchos otros sistemas en el camino entre el usuario y el servicio (su portátil, su WiFi doméstico, su ISP, la red eléctrica...) y esos sistemas colectivamente tienen una disponibilidad mucho menor que el 99.999%. Por lo tanto, la diferencia marginal entre el 99.999% y el 100% se pierde en el ruido de otras indisponibilidades, y el usuario no recibe ningún beneficio del enorme esfuerzo requerido para agregar ese último 0.001% de disponibilidad.

Si el 100% es el objetivo de confiabilidad incorrecto para un sistema, entonces, ¿cuál es el objetivo correcto? En realidad, esta no es una pregunta técnica en absoluto, sino una pregunta de producto, que debería considerar las siguientes cuestiones:

¿Qué nivel de disponibilidad hará que los usuarios estén satisfechos, dado cómo usan el producto?
¿Qué alternativas están disponibles para los usuarios que están insatisfechos con la disponibilidad del producto?
¿Qué sucede con el uso que los usuarios hacen del producto en diferentes niveles de disponibilidad?
El negocio o el producto deben establecer el objetivo de disponibilidad del sistema. Una vez que se establece ese objetivo, el presupuesto de errores es uno menos el objetivo de disponibilidad. Un servicio que tiene una disponibilidad del 99.99% es 0.01% no disponible. Ese 0.01% de indisponibilidad permitida es el presupuesto de errores del servicio. Podemos gastar ese presupuesto en lo que queramos, siempre y cuando no lo excedamos.

Entonces, ¿cómo queremos gastar el presupuesto de errores? El equipo de desarrollo quiere lanzar funciones y atraer nuevos usuarios. Idealmente, gastaríamos todo nuestro presupuesto de errores tomando riesgos con las cosas que lanzamos para lanzarlas rápidamente. Este premisa básica describe todo el modelo de presupuestos de errores. Tan pronto como las actividades de SRE se conceptualizan en este marco, liberar el presupuesto de errores mediante tácticas como implementaciones por fases y experimentos al 1% puede optimizar para lanzamientos más rápidos.

El uso de un presupuesto de errores resuelve el conflicto estructural de incentivos entre el desarrollo y SRE. El objetivo de SRE ya no es "cero interrupciones"; más bien, los SREs y los desarrolladores de productos apuntan a gastar el presupuesto de errores obteniendo la máxima velocidad de características. Este cambio hace toda la diferencia. Una interrupción ya no es algo "malo", es una parte esperada del proceso de innovación y una ocurrencia que tanto los equipos de desarrollo como de SRE manejan en lugar de temer.

Monitoreo

El monitoreo es uno de los medios principales por los cuales los propietarios de servicios realizan un seguimiento de la salud y disponibilidad de un sistema. Como tal, la estrategia de monitoreo debe construirse con cuidado. Un enfoque clásico y común para el monitoreo es observar un valor o condición específica y luego activar una alerta por correo electrónico cuando ese valor se excede o esa condición ocurre. Sin embargo, este tipo de alerta por correo electrónico no es una solución efectiva: un sistema que requiere que un humano lea un correo electrónico y decida si se necesita o no algún tipo de acción en respuesta es fundamentalmente defectuoso. El monitoreo nunca debería requerir que un humano interprete ninguna parte del dominio de alertas. En su lugar, el software debería hacer la interpretación, y los humanos solo deberían ser notificados cuando necesiten tomar acción.

Hay tres tipos de salidas de monitoreo válidas:

Alertas

Indican que un humano necesita tomar acción inmediatamente en respuesta a algo que está sucediendo o está a punto de suceder, para mejorar la situación.
Tickets

Indican que un humano necesita tomar acción, pero no de inmediato. El sistema no puede manejar la situación automáticamente, pero si un humano toma acción en unos pocos días, no ocurrirá ningún daño.
Registros (Logging)

Nadie necesita mirar esta información, pero se registra para propósitos diagnósticos o forenses. La expectativa es que nadie lea los registros a menos que algo más los impulse a hacerlo.

El resultado de nuestro enfoque para contratar a SRE (Site Reliability Engineers) es que terminamos con un equipo de personas que (a) rápidamente se aburrirán de realizar tareas manualmente, y (b) tienen las habilidades necesarias para escribir software que reemplace su trabajo manual anterior, incluso cuando la solución es complicada. Los SREs también terminan compartiendo un trasfondo académico e intelectual con el resto de la organización de desarrollo. Por lo tanto, el trabajo de SRE consiste fundamentalmente en realizar tareas que históricamente han sido realizadas por un equipo de operaciones, pero utilizando ingenieros con experiencia en software, y confiando en el hecho de que estos ingenieros están inherentemente predispuestos y tienen la capacidad de diseñar e implementar automatización con software para reemplazar el trabajo humano.

Por diseño, es crucial que los equipos de SRE estén enfocados en la ingeniería. Sin una ingeniería constante, la carga operativa aumenta y los equipos necesitarán más personas solo para mantenerse al día con la carga de trabajo. Eventualmente, un grupo tradicional enfocado en operaciones se escala de manera lineal con el tamaño del servicio: si los productos respaldados por el servicio tienen éxito, la carga operativa crecerá con el tráfico. Eso significa contratar a más personas para realizar las mismas tareas una y otra vez.

Para evitar este destino, el equipo encargado de gestionar un servicio necesita programar o se ahogará. Por lo tanto, Google establece un límite del 50% en el trabajo "operativo" agregado para todos los SREs: tickets, guardias, tareas manuales, etc. Este límite asegura que el equipo de SRE tenga suficiente tiempo en su agenda para hacer que el servicio sea estable y operable. Este límite es un techo; con el tiempo, si se les deja a su propio ritmo, el equipo de SRE debería terminar con muy poca carga operativa y dedicarse casi por completo a tareas de desarrollo, porque el servicio básicamente se ejecuta y repara solo: queremos sistemas que sean automáticos, no solo automatizados. En la práctica, la escala y las nuevas características mantienen a los SREs alerta.

La regla general de Google es que un equipo de SRE debe dedicar el 50% restante de su tiempo a hacer desarrollo. Entonces, ¿cómo hacemos cumplir ese umbral? En primer lugar, tenemos que medir cómo se gasta el tiempo de los SRE. Con esa medición en mano, nos aseguramos de que los equipos que consistentemente dedican menos del 50% de su tiempo a trabajo de desarrollo cambien sus prácticas. A menudo, esto significa trasladar parte de la carga operativa de nuevo al equipo de desarrollo, o agregar personal al equipo sin asignarles responsabilidades operativas adicionales. Mantener conscientemente este equilibrio entre el trabajo operativo y de desarrollo nos permite asegurarnos de que los SREs tengan el ancho de banda para participar en una ingeniería creativa y autónoma, mientras retienen la sabiduría adquirida en el lado operativo de la gestión de un servicio.

Hemos descubierto que el enfoque de los SRE de Google para operar sistemas a gran escala tiene muchas ventajas. Debido a que los SREs modifican directamente el código en su esfuerzo por hacer que los sistemas de Google se gestionen solos, los equipos de SRE se caracterizan por su rápida innovación y una gran aceptación del cambio. Tales equipos son relativamente económicos; apoyar el mismo servicio con un equipo orientado a operaciones requeriría un número significativamente mayor de personas. En cambio, el número de SREs necesarios para operar, mantener y mejorar un sistema escala de manera sublineal con el tamaño del sistema. Finalmente, no solo SRE elude la disfuncionalidad de la división entre desarrollo y operaciones, sino que esta estructura también mejora a nuestros equipos de desarrollo de productos: las transferencias fáciles entre los equipos de desarrollo de productos y SRE permiten entrenar a todo el grupo de manera cruzada y mejorar las habilidades de los desarrolladores que de otro modo podrían tener dificultades para aprender a construir un sistema distribuido de millones de núcleos.

A pesar de estas ganancias netas, el modelo de SRE se caracteriza por su propio conjunto de desafíos. Un desafío constante que enfrenta Google es la contratación de SREs: no solo SRE compite por los mismos candidatos que el canal de contratación de desarrollo de productos, sino que el hecho de que establecemos un listón tan alto en términos de habilidades de codificación e ingeniería de sistemas significa que nuestra piscina de contratación es necesariamente pequeña. Como nuestra disciplina es relativamente nueva y única, no existe mucha información en la industria sobre cómo construir y gestionar un equipo de SRE (¡aunque esperamos que este libro avance en esa dirección!). Y una vez que un equipo de SRE está en su lugar, sus enfoques potencialmente poco ortodoxos para la gestión de servicios requieren un fuerte apoyo de la dirección. Por ejemplo, la decisión de detener lanzamientos durante el resto del trimestre una vez que se agota un presupuesto de errores podría no ser bien recibida por un equipo de desarrollo de productos a menos que esté ordenada por su dirección.

¿DevOps o SRE?

El término "DevOps" surgió en la industria a finales de 2008 y, al momento de escribir esto (principios de 2016), todavía está en un estado de cambio. Sus principios básicos —la participación de la función de TI en cada fase del diseño y desarrollo de un sistema, la fuerte dependencia de la automatización en lugar del esfuerzo humano, la aplicación de prácticas y herramientas de ingeniería a las tareas de operaciones— son consistentes con muchos de los principios y prácticas de SRE. Se podría ver a DevOps como una generalización de varios principios fundamentales de SRE a un rango más amplio de organizaciones, estructuras de gestión y personal. De manera equivalente, se podría ver a SRE como una implementación específica de DevOps con algunas extensiones idiosincráticas.

Principios fundamentales de SRE

Si bien los matices de los flujos de trabajo, las prioridades y las operaciones diarias varían de un equipo de SRE a otro, todos comparten un conjunto de responsabilidades básicas para los servicios que respaldan y adhieren a los mismos principios fundamentales. En general, un equipo de SRE es responsable de la disponibilidad, latencia, rendimiento, eficiencia, gestión de cambios, monitoreo, respuesta a emergencias y planificación de capacidad de sus servicios. Hemos codificado reglas de compromiso y principios sobre cómo los equipos de SRE interactúan con su entorno, no solo el entorno de producción, sino también los equipos de desarrollo de productos, los equipos de pruebas, los usuarios, y más. Esas reglas y prácticas de trabajo nos ayudan a mantener nuestro enfoque en el trabajo de ingeniería, en lugar del trabajo operativo.

Asegurando un Enfoque Duradero en la Ingeniería

Como ya se mencionó, Google limita el trabajo operativo para los SREs al 50% de su tiempo. El tiempo restante debe dedicarse a usar sus habilidades de codificación en proyectos. En la práctica, esto se logra monitoreando la cantidad de trabajo operativo realizado por los SREs y redirigiendo el exceso de trabajo operativo a los equipos de desarrollo de productos: reasignando errores y tickets a los gerentes de desarrollo, [re]integrando a los desarrolladores en las rotaciones de guardias, y así sucesivamente. La redirección termina cuando la carga operativa vuelve al 50% o menos. Esto también proporciona un mecanismo de retroalimentación efectivo, guiando a los desarrolladores a construir sistemas que no necesiten intervención manual. Este enfoque funciona bien cuando toda la organización —SRE y desarrollo por igual— entiende por qué existe el mecanismo de válvula de seguridad y apoya el objetivo de no tener eventos de desbordamiento porque el producto no genera suficiente carga operativa para requerirlo.

Cuando se enfocan en el trabajo operativo, en promedio, los SREs deberían recibir un máximo de dos eventos por turno de guardia de 8 a 12 horas. Este volumen objetivo da al ingeniero de guardia suficiente tiempo para manejar el evento con precisión y rapidez, limpiar y restaurar el servicio normal, y luego realizar un análisis postmortem. Si ocurren más de dos eventos regularmente por turno de guardia, los problemas no pueden ser investigados a fondo y los ingenieros están lo suficientemente abrumados como para evitar que aprendan de estos eventos. Un escenario de fatiga de paginación tampoco mejorará con la escala. Por el contrario, si los SREs de guardia reciben consistentemente menos de un evento por turno, mantenerlos en alerta es una pérdida de su tiempo.

Los postmortems deben redactarse para todos los incidentes significativos, independientemente de si activaron una alerta o no; los postmortems que no activaron una alerta son aún más valiosos, ya que probablemente señalan claras brechas en la monitorización. Esta investigación debe establecer en detalle lo que sucedió, encontrar todas las causas raíz del evento y asignar acciones para corregir el problema o mejorar cómo se aborda la próxima vez. Google opera bajo una cultura de postmortem sin culpa, con el objetivo de exponer fallas y aplicar ingeniería para corregir estas fallas, en lugar de evitarlas o minimizarlas.

Perseguir la Máxima Velocidad de Cambio Sin Violar el SLO de un Servicio
Los equipos de desarrollo de productos y SRE pueden disfrutar de una relación de trabajo productiva eliminando el conflicto estructural en sus respectivos objetivos. El conflicto estructural está entre la velocidad de innovación y la estabilidad del producto, y como se describió anteriormente, este conflicto a menudo se expresa de manera indirecta. En SRE, llevamos este conflicto al primer plano y luego lo resolvemos con la introducción de un presupuesto de errores.

El presupuesto de errores surge de la observación de que el 100% es el objetivo de confiabilidad incorrecto para prácticamente todo (siendo los marcapasos y los frenos antibloqueo excepciones notables). En general, para cualquier servicio o sistema de software, el 100% no es el objetivo de confiabilidad correcto porque ningún usuario puede notar la diferencia entre un sistema que está disponible el 100% del tiempo y uno que está disponible el 99.999% del tiempo. Hay muchos otros sistemas en el camino entre el usuario y el servicio (su computadora portátil, su WiFi en casa, su ISP, la red eléctrica…) y esos sistemas, colectivamente, tienen una disponibilidad mucho menor al 99.999%. Por lo tanto, la diferencia marginal entre el 99.999% y el 100% se pierde en el ruido de otras indisponibilidades, y el usuario no recibe ningún beneficio del enorme esfuerzo requerido para agregar ese último 0.001% de disponibilidad.

Si el 100% es el objetivo de confiabilidad incorrecto para un sistema, entonces, ¿cuál es el objetivo de confiabilidad correcto? En realidad, esta no es una pregunta técnica en absoluto, es una pregunta de producto, que debe tener en cuenta las siguientes consideraciones:

¿Con qué nivel de disponibilidad estarán satisfechos los usuarios, dado cómo usan el producto?
¿Qué alternativas tienen los usuarios que están insatisfechos con la disponibilidad del producto?
¿Qué sucede con el uso que hacen los usuarios del producto a diferentes niveles de disponibilidad?
El negocio o el equipo de producto debe establecer el objetivo de disponibilidad del sistema. Una vez que se establece ese objetivo, el presupuesto de errores es uno menos el objetivo de disponibilidad. Un servicio que tiene un 99.99% de disponibilidad tiene un 0.01% de indisponibilidad. Esa indisponibilidad permitida del 0.01% es el presupuesto de errores del servicio. Podemos gastar ese presupuesto en lo que queramos, siempre que no lo excedamos.

Entonces, ¿cómo queremos gastar el presupuesto de errores? El equipo de desarrollo quiere lanzar funciones y atraer nuevos usuarios. Idealmente, gastaríamos todo nuestro presupuesto de errores asumiendo riesgos con las cosas que lanzamos para lanzarlas rápidamente. Este principio básico describe todo el modelo de presupuestos de errores. Tan pronto como las actividades de SRE se conceptualizan en este marco, liberar el presupuesto de errores a través de tácticas como lanzamientos por fases y experimentos del 1% puede optimizar los lanzamientos más rápidos.

El uso de un presupuesto de errores resuelve el conflicto estructural de incentivos entre desarrollo y SRE. El objetivo de SRE ya no es "cero interrupciones"; más bien, los SREs y los desarrolladores de productos tienen como objetivo gastar el presupuesto de errores para obtener la máxima velocidad de funciones. Este cambio marca toda la diferencia. Una interrupción ya no es algo "malo"; es una parte esperada del proceso de innovación, y un evento que tanto los equipos de desarrollo como los de SRE gestionan en lugar de temer.

Monitorización
La monitorización es uno de los medios principales por los cuales los propietarios de un servicio mantienen el control de la salud y la disponibilidad de un sistema. Como tal, la estrategia de monitorización debe ser construida con cuidado. Un enfoque clásico y común de la monitorización es observar un valor o condición específica, y luego activar una alerta por correo electrónico cuando ese valor se excede o esa condición ocurre. Sin embargo, este tipo de alerta por correo electrónico no es una solución efectiva: un sistema que requiere que un humano lea un correo electrónico y decida si se necesita o no tomar algún tipo de acción en respuesta está fundamentalmente defectuoso. La monitorización nunca debe requerir que un humano interprete ninguna parte del dominio de alertas. En su lugar, el software debe hacer la interpretación, y los humanos solo deben ser notificados cuando necesitan tomar acción.

Existen tres tipos de resultados válidos de monitorización:

Alertas
Indican que un humano necesita tomar acción de inmediato en respuesta a algo que está ocurriendo o que está a punto de ocurrir, para mejorar la situación.

Tickets
Indican que un humano necesita tomar acción, pero no de inmediato. El sistema no puede manejar la situación automáticamente, pero si un humano toma acción en unos días, no se producirá ningún daño.

Registro (Logging)
Nadie necesita revisar esta información, pero se registra para propósitos de diagnóstico o forenses. Se espera que nadie lea los registros a menos que algo más los impulse a hacerlo.

Respuesta a Emergencias
La confiabilidad es una función del tiempo medio hasta la falla (MTTF) y el tiempo medio de reparación (MTTR). La métrica más relevante para evaluar la efectividad de la respuesta a emergencias es qué tan rápido el equipo de respuesta puede devolver el sistema a su estado saludable, es decir, el MTTR.

Los humanos añaden latencia. Incluso si un sistema experimenta más fallas, un sistema que puede evitar emergencias que requieran intervención humana tendrá mayor disponibilidad que un sistema que requiere intervención manual. Cuando los humanos son necesarios, hemos descubierto que pensar y registrar las mejores prácticas de antemano en un "manual de procedimientos" produce una mejora de aproximadamente 3x en el MTTR en comparación con la estrategia de "improvisar". El ingeniero de guardia que es un héroe todoterreno funciona, pero el ingeniero de guardia con experiencia, armado con un manual de procedimientos, funciona mucho mejor. Aunque ningún manual, por completo que sea, sustituye a ingenieros inteligentes capaces de pensar de manera improvisada, los pasos claros y exhaustivos para la resolución de problemas son valiosos cuando se responde a una alerta de alta prioridad o sensible al tiempo. Por lo tanto, Google SRE confía en los manuales de procedimientos para guardias, además de ejercicios como la "Rueda de la Desgracia," para preparar a los ingenieros a reaccionar ante eventos de guardia.

Gestión de Cambios
SRE ha descubierto que aproximadamente el 70% de las interrupciones son debidas a cambios en un sistema en vivo. Las mejores prácticas en este ámbito utilizan la automatización para lograr lo siguiente:

Implementación de despliegues progresivos.
Detección rápida y precisa de problemas.
Reversión segura de cambios cuando surgen problemas.
Este trío de prácticas minimiza eficazmente el número total de usuarios y operaciones expuestos a cambios defectuosos. Al eliminar a los humanos del proceso, estas prácticas evitan los problemas normales de fatiga, familiaridad/desdén e inatención a tareas altamente repetitivas. Como resultado, tanto la velocidad de lanzamiento como la seguridad aumentan.

Pronóstico de Demanda y Planificación de Capacidad
El pronóstico de demanda y la planificación de capacidad se pueden ver como garantizar que haya suficiente capacidad y redundancia para satisfacer la demanda proyectada futura con la disponibilidad requerida. No hay nada particularmente especial sobre estos conceptos, excepto que un número sorprendente de servicios y equipos no toman las medidas necesarias para asegurar que la capacidad requerida esté disponible en el momento necesario.

La planificación de capacidad debe tener en cuenta tanto el crecimiento orgánico (que proviene de la adopción natural del producto y el uso por parte de los clientes) como el crecimiento inorgánico (que resulta de eventos como lanzamientos de características, campañas de marketing u otros cambios impulsados por el negocio).

Varios pasos son obligatorios en la planificación de capacidad:

Un pronóstico de demanda orgánica preciso, que se extienda más allá del tiempo de anticipación requerido para adquirir capacidad.
Una incorporación precisa de fuentes de demanda inorgánica en el pronóstico de demanda.
Pruebas de carga regulares del sistema para correlacionar la capacidad bruta (servidores, discos, etc.) con la capacidad del servicio.
Dado que la capacidad es crítica para la disponibilidad, naturalmente sigue que el equipo de SRE debe estar a cargo de la planificación de capacidad, lo que significa que también deben estar a cargo de la provisión.

Aprovisionamiento
El aprovisionamiento combina tanto la gestión de cambios como la planificación de capacidad. En nuestra experiencia, el aprovisionamiento debe realizarse rápidamente y solo cuando sea necesario, ya que la capacidad es costosa. Este ejercicio también debe hacerse correctamente o la capacidad no funcionará cuando se necesite. Añadir nueva capacidad a menudo implica iniciar una nueva instancia o ubicación, hacer modificaciones significativas a los sistemas existentes (archivos de configuración, balanceadores de carga, redes), y validar que la nueva capacidad funcione y entregue resultados correctos. Por lo tanto, es una operación más arriesgada que el cambio de carga, que se realiza con frecuencia varias veces por hora, y debe tratarse con un grado correspondiente de precaución adicional.

Eficiencia y Rendimiento
El uso eficiente de los recursos es importante siempre que un servicio se preocupe por el dinero. Dado que SRE controla en última instancia el aprovisionamiento, también debe involucrarse en cualquier trabajo relacionado con la utilización, ya que la utilización es una función de cómo funciona un servicio determinado y cómo se aprovisiona. Se deduce que prestar mucha atención a la estrategia de aprovisionamiento de un servicio, y por lo tanto a su utilización, proporciona una palanca muy grande sobre los costos totales del servicio.

El uso de recursos es una función de la demanda (carga), la capacidad y la eficiencia del software. Los SREs predicen la demanda, aprovisionan la capacidad y pueden modificar el software. Estos tres factores son una gran parte (aunque no la totalidad) de la eficiencia de un servicio. Los sistemas de software se vuelven más lentos a medida que se les añade carga. Una desaceleración en un servicio equivale a una pérdida de capacidad. En algún momento, un sistema que se ralentiza deja de servir, lo que corresponde a una lentitud infinita. Los SREs aprovisionan para cumplir con un objetivo de capacidad a una velocidad de respuesta específica, y por lo tanto están muy interesados en el rendimiento de un servicio. Los SREs y los desarrolladores de productos monitorearán y modificarán un servicio para mejorar su rendimiento, aumentando así la capacidad y mejorando la eficiencia.

El Fin del Comienzo
La Ingeniería de Fiabilidad del Sitio (SRE) representa una ruptura significativa con las mejores prácticas existentes en la industria para la gestión de servicios grandes y complicados. Motivada originalmente por la familiaridad—"como ingeniero de software, así es como querría invertir mi tiempo para realizar un conjunto de tareas repetitivas"—se ha convertido en mucho más: un conjunto de principios, un conjunto de prácticas, un conjunto de incentivos y un campo de esfuerzo dentro de la disciplina más amplia de la ingeniería de software. El resto del libro explora el Camino SRE en detalle.

Capítulo 2 - El Entorno de Producción en Google, desde la Perspectiva de un SRE
Escrito por JC van Winkel | Editado por Betsy Beyer

Los centros de datos de Google son muy diferentes de la mayoría de los centros de datos convencionales y granjas de servidores a pequeña escala. Estas diferencias presentan tanto problemas adicionales como oportunidades. Este capítulo analiza los desafíos y oportunidades que caracterizan a los centros de datos de Google e introduce la terminología que se utiliza a lo largo del libro.

Hardware
La mayoría de los recursos de computación de Google se encuentran en centros de datos diseñados por Google con distribución de energía, refrigeración, redes y hardware de computación propietarios (ver [Bar13]). A diferencia de los centros de datos de colocación "estándar", el hardware de computación en un centro de datos diseñado por Google es el mismo en toda la infraestructura. Para eliminar la confusión entre el hardware del servidor y el software del servidor, utilizamos la siguiente terminología a lo largo del libro:

Máquina: Una pieza de hardware (o quizás una máquina virtual).
Servidor: Una pieza de software que implementa un servicio.
Las máquinas pueden ejecutar cualquier servidor, por lo que no dedicamos máquinas específicas a programas de servidor específicos. No hay una máquina específica que ejecute nuestro servidor de correo, por ejemplo. En su lugar, la asignación de recursos se maneja mediante nuestro sistema operativo de clúster, Borg. Nos damos cuenta de que este uso de la palabra "servidor" es inusual. El uso común de la palabra confunde "binario que acepta conexión de red" con "máquina", pero diferenciar entre los dos es importante cuando se habla de computación en Google. Una vez que te acostumbras a nuestro uso del término "servidor", se vuelve más evidente por qué tiene sentido utilizar esta terminología especializada, no solo dentro de Google, sino también en el resto de este libro.

La Figura 2-1 ilustra la topología de un centro de datos de Google:

Decenas de máquinas se colocan en un rack.
Los racks están colocados en una fila.
Una o más filas forman un clúster.
Generalmente, un edificio de centro de datos alberga múltiples clústeres.
Múltiples edificios de centros de datos que están ubicados cerca uno del otro forman un campus.
Figura 2-1. Ejemplo de topología de un campus de centro de datos de Google

Las máquinas dentro de un centro de datos determinado necesitan poder comunicarse entre sí, por lo que creamos un conmutador virtual muy rápido con decenas de miles de puertos. Logramos esto conectando cientos de conmutadores construidos por Google en una red Clos [Clos53] llamada Júpiter [Sin15]. En su configuración más grande, Júpiter admite 1.3 Pbps de ancho de banda de bisección entre servidores.

Los centros de datos están conectados entre sí a través de nuestra columna vertebral global.

Red B4
La red B4 [Jai13] es una arquitectura de red definida por software que utiliza el protocolo de comunicaciones de estándar abierto OpenFlow. B4 proporciona un ancho de banda masivo a un número modesto de sitios y utiliza la asignación de ancho de banda elástica para maximizar el ancho de banda promedio [Kum15].

Software de Sistema que "Organiza" el Hardware
Nuestro hardware debe ser controlado y administrado por software que pueda manejar una escala masiva. Las fallas de hardware son un problema notable que gestionamos con software. Dado el gran número de componentes de hardware en un clúster, las fallas de hardware ocurren con bastante frecuencia. En un solo clúster en un año típico, miles de máquinas fallan y miles de discos duros se rompen; cuando multiplicamos esto por el número de clústeres que operamos globalmente, estos números se vuelven bastante asombrosos. Por lo tanto, queremos abstraer estos problemas para que no afecten a los usuarios, y los equipos que ejecutan nuestros servicios tampoco quieren ser molestados por fallas de hardware. Cada campus de centro de datos tiene equipos dedicados a mantener el hardware y la infraestructura del centro de datos.

Gestión de Máquinas
Borg, ilustrado en la Figura 2-2, es un sistema operativo de clúster distribuido [Ver15], similar a Apache Mesos. Borg gestiona sus trabajos a nivel de clúster. Borg es responsable de ejecutar los trabajos de los usuarios, que pueden ser servidores que funcionan indefinidamente o procesos por lotes como un MapReduce [Dea04]. Los trabajos pueden consistir en más de una (y a veces miles de) tareas idénticas, tanto por razones de confiabilidad como porque un solo proceso no puede manejar todo el tráfico del clúster.

Cuando Borg inicia un trabajo, encuentra máquinas para las tareas y les dice a las máquinas que inicien el programa del servidor. Borg luego monitorea continuamente estas tareas. Si una tarea falla, se elimina y se reinicia, posiblemente en una máquina diferente.

Debido a que las tareas se asignan de manera flexible entre las máquinas, no podemos simplemente confiar en direcciones IP y números de puerto para referirnos a las tareas. Resolvemos este problema con un nivel adicional de indirecta: cuando se inicia un trabajo, Borg asigna un nombre y un número de índice a cada tarea utilizando el Servicio de Nombres de Borg (BNS). En lugar de usar la dirección IP y el número de puerto, otros procesos se conectan a las tareas de Borg a través del nombre BNS, que se traduce en una dirección IP y un número de puerto por el BNS. Por ejemplo, la ruta de BNS podría ser una cadena como /bns/<cluster>/<usuario>/<nombre del trabajo>/<número de tarea>, que se resolvería en <dirección IP>:<puerto>.

Borg también es responsable de la asignación de recursos a los trabajos. Cada trabajo necesita especificar sus recursos requeridos (por ejemplo, 3 núcleos de CPU, 2 GiB de RAM). Usando la lista de requisitos para todos los trabajos, Borg puede distribuir las tareas en las máquinas de una manera óptima que también tenga en cuenta los dominios de fallas (por ejemplo: Borg no ejecutará todas las tareas de un trabajo en el mismo rack, ya que hacerlo significaría que el conmutador superior del rack sería un único punto de falla para ese trabajo).

Si una tarea intenta usar más recursos de los que solicitó, Borg mata la tarea y la reinicia (ya que una tarea que falla lentamente es generalmente preferible a una tarea que no se ha reiniciado en absoluto).

Almacenamiento
Las tareas pueden usar el disco local en las máquinas como un espacio de trabajo temporal, pero tenemos varias opciones de almacenamiento en clúster para almacenamiento permanente (e incluso el espacio de trabajo temporal eventualmente se trasladará al modelo de almacenamiento en clúster). Estos son comparables a Lustre y al Hadoop Distributed File System (HDFS), que son ambos sistemas de archivos de clúster de código abierto.

La capa de almacenamiento es responsable de ofrecer a los usuarios acceso fácil y confiable al almacenamiento disponible para un clúster. Como se muestra en la Figura 2-3, el almacenamiento tiene muchas capas:

La capa más baja se llama D (de disco, aunque D usa tanto discos giratorios como almacenamiento flash). D es un servidor de archivos que se ejecuta en casi todas las máquinas de un clúster. Sin embargo, los usuarios que desean acceder a sus datos no quieren tener que recordar qué máquina está almacenando sus datos, por lo que la siguiente capa entra en juego.
Una capa sobre D llamada Colossus crea un sistema de archivos de clúster que ofrece la semántica habitual de un sistema de archivos, así como replicación y encriptación. Colossus es el sucesor de GFS, el Google File System [Ghe03].
Hay varios servicios similares a bases de datos construidos sobre Colossus:
Bigtable [Cha06] es un sistema de base de datos NoSQL que puede manejar bases de datos de tamaño de petabytes. Un Bigtable es un mapa disperso, distribuido, persistente y multidimensional ordenado, indexado por clave de fila, clave de columna y marca de tiempo; cada valor en el mapa es una matriz de bytes sin interpretar. Bigtable admite la replicación eventual consistente entre centros de datos.
Spanner [Cor12] ofrece una interfaz similar a SQL para los usuarios que requieren consistencia real en todo el mundo.
Varios otros sistemas de bases de datos, como Blobstore, están disponibles. Cada una de estas opciones viene con su propio conjunto de compensaciones (ver "Integridad de Datos: Lo que Lees es lo que Escribiste").

El hardware de red de Google se controla de varias maneras. Como se mencionó anteriormente, utilizamos una red definida por software basada en OpenFlow. En lugar de usar hardware de enrutamiento "inteligente", confiamos en componentes de conmutación más económicos y simples, junto con un controlador central (duplicado) que precomputa las mejores rutas a través de la red. De esta manera, podemos trasladar las decisiones de enrutamiento, que requieren mucho cálculo, fuera de los enrutadores y usar hardware de conmutación simple.

El ancho de banda de la red debe asignarse de manera inteligente. Al igual que Borg limita los recursos de cómputo que una tarea puede usar, el Bandwidth Enforcer (BwE) gestiona el ancho de banda disponible para maximizar el promedio disponible. Optimizar el ancho de banda no solo tiene que ver con el costo: se ha demostrado que la ingeniería de tráfico centralizada resuelve varios problemas que son tradicionalmente extremadamente difíciles de resolver con una combinación de enrutamiento distribuido e ingeniería de tráfico [Kum15].

Algunos servicios tienen trabajos que se ejecutan en múltiples clústeres distribuidos por todo el mundo. Para minimizar la latencia en servicios distribuidos globalmente, queremos dirigir a los usuarios al centro de datos más cercano con capacidad disponible. Nuestro Global Software Load Balancer (GSLB) realiza balanceo de carga en tres niveles:

Balanceo geográfico de carga para solicitudes DNS (por ejemplo, a www.google.com).
Balanceo de carga a nivel de servicio de usuario (por ejemplo, YouTube o Google Maps).
Balanceo de carga a nivel de llamadas de procedimientos remotos (RPC) dentro de los centros de datos.
Los propietarios de servicios especifican un nombre simbólico para el servicio, una lista de direcciones BNS de servidores y la capacidad disponible en cada ubicación (generalmente medida en consultas por segundo). GSLB luego dirige el tráfico a las direcciones BNS.

Otros Componentes de Software del Sistema
Servicio de Bloqueo
El servicio de bloqueo Chubby [Bur06] proporciona una API similar a un sistema de archivos para mantener bloqueos. Chubby maneja estos bloqueos en ubicaciones de centros de datos y utiliza el protocolo Paxos para lograr consenso asincrónico (ver "Gestión de Estado Crítico: Consenso Distribuido para la Fiabilidad").

Chubby también desempeña un papel importante en la elección de maestro. Cuando un servicio tiene cinco réplicas de un trabajo por razones de confiabilidad, pero solo una réplica puede realizar el trabajo real, Chubby se usa para seleccionar qué réplica puede proceder. Los datos que deben ser consistentes son adecuados para almacenarse en Chubby. Por esta razón, BNS usa Chubby para almacenar el mapeo entre las rutas BNS y los pares de dirección IP
.

Monitoreo y Alertas
Queremos asegurarnos de que todos los servicios estén funcionando como se requiere. Por lo tanto, ejecutamos muchas instancias de nuestro programa de monitoreo Borgmon (ver "Alertas Prácticas a partir de Datos de Series Temporales"). Borgmon regularmente recopila métricas de los servidores monitoreados. Estas métricas pueden usarse instantáneamente para alertas y también almacenarse para generar vistas históricas (por ejemplo, gráficos). Podemos usar el monitoreo de las siguientes maneras:

Configurar alertas para problemas agudos.
Comparar comportamiento: ¿una actualización de software hizo que el servidor sea más rápido?
Examinar cómo evoluciona el consumo de recursos con el tiempo, lo cual es esencial para la planificación de capacidad.

Nuestra Infraestructura de Software
Nuestra arquitectura de software está diseñada para hacer el uso más eficiente posible de nuestra infraestructura de hardware. Nuestro código está fuertemente multihilo, lo que permite que una tarea utilice fácilmente varios núcleos. Para facilitar la creación de paneles de control, el monitoreo y la depuración, cada servidor tiene un servidor HTTP que proporciona diagnósticos y estadísticas para una tarea determinada.

Todos los servicios de Google se comunican usando una infraestructura de Remote Procedure Call (RPC) llamada Stubby; una versión de código abierto, gRPC, está disponible. Con frecuencia, se hace una llamada RPC incluso cuando se necesita realizar una llamada a una subrutina en el programa local. Esto facilita la refactorización de la llamada en un servidor diferente si se necesita más modularidad o cuando el código base del servidor crece. GSLB puede equilibrar la carga de las RPCs de la misma manera que equilibra los servicios visibles externamente.

Un servidor recibe solicitudes RPC de su frontend y envía RPCs a su backend. En términos tradicionales, el frontend se llama "cliente" y el backend se llama "servidor". Los datos se transfieren hacia y desde una RPC utilizando protocol buffers, a menudo abreviados como "protobufs", que son similares a Thrift de Apache. Los protocol buffers tienen muchas ventajas sobre XML para la serialización de datos estructurados: son más simples de usar, de 3 a 10 veces más pequeños, de 20 a 100 veces más rápidos y menos ambiguos.

Nuestro Entorno de Desarrollo
La velocidad de desarrollo es muy importante para Google, por lo que hemos creado un entorno de desarrollo completo que aprovecha nuestra infraestructura [Mor12b].

Aparte de algunos grupos que tienen sus propios repositorios de código abierto (por ejemplo, Android y Chrome), los ingenieros de software de Google trabajan desde un repositorio compartido [Pot16]. Esto tiene algunas implicaciones prácticas importantes para nuestros flujos de trabajo:

Si los ingenieros encuentran un problema en un componente fuera de su proyecto, pueden corregirlo, enviar los cambios propuestos (llamados "changelist" o CL) al propietario para su revisión y enviar el CL a la línea principal.
Los cambios en el código fuente de un proyecto propio de un ingeniero requieren una revisión. Todo el software es revisado antes de ser enviado.
Cuando se construye el software, la solicitud de construcción se envía a servidores de compilación en un centro de datos. Incluso las compilaciones grandes se ejecutan rápidamente, ya que muchos servidores de compilación pueden compilar en paralelo. Esta infraestructura también se utiliza para pruebas continuas. Cada vez que se envía un CL, se ejecutan pruebas en todo el software que puede depender de ese CL, directa o indirectamente. Si el marco determina que el cambio probablemente rompió otras partes del sistema, notifica al propietario del cambio enviado. Algunos proyectos usan un sistema de "push-on-green", donde una nueva versión se envía automáticamente a producción después de pasar las pruebas.

Shakespeare: Un Servicio de Ejemplo
Para proporcionar un modelo de cómo se desplegaría hipotéticamente un servicio en el entorno de producción de Google, veamos un servicio de ejemplo que interactúa con múltiples tecnologías de Google. Supongamos que queremos ofrecer un servicio que te permita determinar dónde se usa una palabra determinada en todas las obras de Shakespeare. Podemos dividir este sistema en dos partes:

Un componente por lotes que lee todos los textos de Shakespeare, crea un índice y escribe el índice en un Bigtable. Este trabajo solo necesita ejecutarse una vez o muy infrecuentemente (¡nunca se sabe si se puede descubrir un nuevo texto!).
Un frontend de aplicación que maneja las solicitudes de los usuarios finales. Este trabajo está siempre activo, ya que los usuarios en todas las zonas horarias querrán buscar en los libros de Shakespeare.
El componente por lotes es un MapReduce que comprende tres fases:

Fase de mapeo: Lee los textos de Shakespeare y los divide en palabras individuales. Esto es más rápido si lo realizan varios trabajadores en paralelo.
Fase de mezcla: Ordena las tuplas por palabra.
Fase de reducción: Crea una tupla de (palabra, lista de ubicaciones).
Cada tupla se escribe en una fila de Bigtable, usando la palabra como clave.

Vida de una Solicitud
La Figura 2-4 muestra cómo se atiende la solicitud de un usuario. Primero, el usuario apunta su navegador a shakespeare.google.com. Para obtener la dirección IP correspondiente, el dispositivo del usuario resuelve la dirección con su servidor DNS (1). Esta solicitud finalmente llega al servidor DNS de Google, que se comunica con GSLB (Global Software Load Balancer). GSLB, que rastrea la carga de tráfico entre los servidores frontend en distintas regiones, selecciona qué dirección IP del servidor enviar a este usuario.

El navegador se conecta al servidor HTTP en esta IP. Este servidor (llamado Google Frontend o GFE) es un proxy inverso que termina la conexión TCP (2). GFE busca qué servicio se requiere (búsqueda web, mapas o, en este caso, Shakespeare). Nuevamente, usando GSLB, el servidor encuentra un servidor frontend disponible para Shakespeare y le envía una RPC que contiene la solicitud HTTP (3).

El servidor de Shakespeare analiza la solicitud HTTP y construye un protobuf que contiene la palabra a buscar. Ahora, el servidor frontend de Shakespeare necesita contactar al servidor backend de Shakespeare. El servidor frontend contacta a GSLB para obtener la dirección BNS de un servidor backend adecuado y sin carga (4). Luego, ese servidor backend de Shakespeare contacta a un servidor Bigtable para obtener los datos solicitados (5).

La respuesta se escribe en el protobuf de respuesta y se devuelve al servidor backend de Shakespeare. El servidor backend entrega un protobuf que contiene los resultados al servidor frontend de Shakespeare, que ensambla el HTML y devuelve la respuesta al usuario.

Toda esta cadena de eventos se ejecuta en un abrir y cerrar de ojos, ¡solo unos cientos de milisegundos! Dado que hay muchas partes móviles involucradas, existen muchos puntos de falla potenciales; en particular, una falla en GSLB podría causar estragos. Sin embargo, las políticas rigurosas de pruebas de Google y la implementación cuidadosa, junto con nuestros métodos proactivos de recuperación de errores, como la degradación controlada, nos permiten ofrecer el servicio confiable que nuestros usuarios esperan. Después de todo, las personas utilizan regularmente www.google.com para verificar si su conexión a Internet está funcionando correctamente.























