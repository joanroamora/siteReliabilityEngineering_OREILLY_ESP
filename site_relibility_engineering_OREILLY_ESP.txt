Prólogo

La historia de Google es una historia de crecimiento. Es una de las grandes historias de éxito de la industria informática, que marca un cambio hacia un negocio centrado en TI. Google fue una de las primeras empresas en definir lo que significaba la alineación entre negocios y TI en la práctica, y posteriormente informó el concepto de DevOps para una comunidad de TI más amplia. Este libro ha sido escrito por un amplio grupo de personas que hicieron que esa transición se convirtiera en realidad.

Google creció en un momento en que el papel tradicional del administrador de sistemas estaba siendo transformado. Cuestionó la administración de sistemas, como si dijera: no podemos permitirnos seguir la tradición como autoridad, tenemos que pensar de nuevo y no tenemos tiempo para esperar a que todos los demás se pongan al día. En la introducción a Principios de administración de redes y sistemas [Bur99], afirmé que la administración de sistemas era una forma de ingeniería humana-computadora. Esto fue rechazado enérgicamente por algunos revisores, que dijeron "todavía no estamos en la etapa en que podemos llamarlo ingeniería". En ese momento, sentí que el campo se había perdido, atrapado en su propia cultura de magos, y no podía ver una forma de avanzar. Luego, Google dibujó una línea en el silicio, forzando que ese destino se convirtiera en realidad. El papel revisado se llamó SRE, o Ingeniero de Confiabilidad del Sitio. Algunos de mis amigos estuvieron entre los primeros de esta nueva generación de ingenieros; lo formalizaron utilizando software y automatización. Inicialmente, eran muy secretos, y lo que sucedió dentro y fuera de Google era muy diferente: la experiencia de Google era única. Con el tiempo, la información y los métodos han fluido en ambas direcciones. Este libro muestra una voluntad de dejar que el pensamiento SRE salga de las sombras.

Aquí, no solo vemos cómo Google construyó su legendaria infraestructura, sino también cómo estudió, aprendió y cambió de opinión sobre las herramientas y las tecnologías en el camino. También nosotros podemos enfrentar desafíos abrumadores con un espíritu abierto. La naturaleza tribal de la cultura de TI a menudo atrapa a los practicantes en posiciones dogmáticas que frenan a la industria. Si Google superó esta inercia, también podemos hacerlo.

Este libro es una colección de ensayos de una sola empresa, con una visión común. El hecho de que las contribuciones estén alineadas en torno a un objetivo común de la empresa es lo que lo hace especial. Hay temas comunes y personajes comunes (sistemas de software) que reaparecen en varios capítulos. Vemos opciones de Diferentes perspectivas, y sabemos que se correlacionan para resolver intereses en competencia. Los artículos no son piezas rigurosas y académicas; son relatos personales, escritos con orgullo, en una variedad de estilos personales y desde la perspectiva de habilidades individuales. Están escritos con valentía y con una honestidad intelectual que es refrescante y poco común en la literatura industrial. Algunos afirman "nunca hagas esto, siempre haz aquello", otros son más filosóficos y tentativos, reflejando la variedad de personalidades dentro de una cultura de TI, y cómo eso también juega un papel en la historia. Nosotros, a su vez, los leemos con la humildad de observadores que no fueron parte del viaje y no tienen toda la información sobre los numerosos desafíos en conflicto. Nuestras muchas preguntas son el verdadero legado del volumen: ¿Por qué no hicieron X? ¿Qué pasaría si hubieran hecho Y? ¿Cómo miraremos hacia atrás en esto en el futuro? Es comparando nuestras propias ideas con la lógica aquí que podemos medir nuestros propios pensamientos y experiencias.

Lo más impresionante de todo sobre este libro es su mera existencia. Hoy en día, escuchamos una cultura descarada de "solo muéstrame el código". Una cultura de "no hagas preguntas" ha crecido alrededor del código abierto, donde la comunidad en lugar de la experiencia es la campeona. Google es una empresa que se atrevió a pensar en los problemas desde principios fundamentales y a emplear a los mejores talentos con una gran proporción de doctorados. Las herramientas eran solo componentes en procesos, trabajando junto con cadenas de software, personas y datos. Nada aquí nos dice cómo resolver problemas universalmente, pero ese es el punto. Historias como estas son mucho más valiosas que el código o diseños que resultaron en ellas. Las implementaciones son efímeras, pero la lógica documentada es invaluable. Rara vez tenemos acceso a este tipo de información.

Este, entonces, es el relato de cómo una empresa lo hizo. El hecho de que sea muchas historias superpuestas nos muestra que escalar es mucho más que solo una ampliación fotográfica de una arquitectura de computadora de texto. Se trata de escalar un proceso empresarial, en lugar de solo la maquinaria. Esta lección sola vale su peso en papel electrónico.

No nos involucramos mucho en la revisión crítica de nosotros mismos en el mundo de la TI; como tal, hay mucha reinvención y repetición. Durante muchos años, solo había la comunidad de la conferencia USENIX LISA discutiendo la infraestructura de TI, más algunas conferencias sobre sistemas operativos. Hoy en día es muy diferente, sin embargo, este libro todavía se siente como una oferta rara: una documentación detallada del paso de Google a través de una época decisiva. La historia no es para copiar— aunque tal vez para emular— pero puede inspirar el próximo paso para todos nosotros. Hay una honestidad intelectual única en estas páginas, expresando tanto liderazgo como humildad. Estas son historias de esperanzas, miedos, éxitos y fracasos. Saludo el coraje de los autores y editores al permitir tal candor, para que nosotros, que no somos parte de las experiencias prácticas, también podamos beneficiarnos de las lecciones aprendidas dentro del capullo.

Mark Burgess


Prefacio

La ingeniería de software tiene esto en común con tener hijos: el trabajo antes del nacimiento es doloroso y difícil, pero el trabajo después del nacimiento es donde se invierte la mayor parte del esfuerzo. Sin embargo, la ingeniería de software como disciplina dedica mucho más tiempo a hablar sobre el primer período en lugar del segundo, a pesar de que se estima que el 40-90% de los costos totales de un sistema se incurren después del nacimiento. El modelo popular de la industria que concibe el software desplegado y operativo como "estabilizado" en producción y, por lo tanto, necesitando mucha menos atención de los ingenieros de software, es incorrecto. A través de esta lente, vemos que si la ingeniería de software tiende a enfocarse en diseñar y construir sistemas de software, debe haber otra disciplina que se enfoque en todo el ciclo de vida de los objetos de software, desde la concepción hasta la descomisión pacífica. Esta disciplina utiliza - y necesita utilizar - una amplia gama de habilidades, pero tiene preocupaciones separadas de otros tipos de ingenieros. Hoy en día, nuestra respuesta es la disciplina que Google llama Ingeniería de Confiabilidad del Sitio (SRE).

Entonces, ¿qué es exactamente la Ingeniería de Confiabilidad del Sitio (SRE)? Admitimos que no es un nombre particularmente claro para lo que hacemos - casi todos los ingenieros de confiabilidad del sitio en Google se les pregunta qué es exactamente eso y qué hacen en realidad, con regularidad.

Desempaquetando el término un poco, primero y principalmente, los SRE son ingenieros. Aplicamos los principios de la ciencia de la computación y la ingeniería al diseño y desarrollo de sistemas de cómputo: generalmente, grandes y distribuidos. A veces, nuestra tarea es escribir el software para esos sistemas junto con nuestros compañeros de desarrollo de productos; a veces, nuestra tarea es construir todas las piezas adicionales que esos sistemas necesitan, como copias de seguridad o equilibrio de carga, idealmente para que puedan ser reutilizadas en sistemas; y a veces, nuestra tarea es descubrir cómo aplicar soluciones existentes a nuevos problemas.

A continuación, nos enfocamos en la confiabilidad del sistema. Ben Treynor Sloss, vicepresidente de Google para operaciones 24/7, originador del término SRE, afirma que la confiabilidad es la característica más fundamental de cualquier producto: un sistema no es muy útil si nadie puede usarlo. Debido a que la confiabilidad es tan crítica, los SRE se enfocan en encontrar formas de mejorar el diseño y la operación de los sistemas para hacerlos más escalables, más confiables y más eficientes. Sin embargo, solo invertimos esfuerzo en esta dirección hasta cierto punto: cuando los sistemas son "lo suficientemente confiables", invertimos nuestros esfuerzos en agregar características o construir nuevos productos.

Finalmente, los SRE se enfocan en operar servicios construidos sobre nuestros sistemas de cómputo distribuidos, ya sean servicios de almacenamiento a escala planetaria, correo electrónico para cientos de millones de usuarios o donde Google comenzó, la búsqueda web. El "sitio" en nuestro nombre originalmente se refería al papel de SRE en mantener el sitio web google.com en funcionamiento, aunque ahora ejecutamos muchos más servicios, muchos de los cuales no son sitios web - desde infraestructura interna como Bigtable hasta productos para desarrolladores externos como la plataforma Google Cloud.

Aunque hemos representado a SRE como una disciplina amplia, no es sorprendente que haya surgido en el mundo en constante movimiento de los servicios web, y tal vez en su origen deba algo a las peculiaridades de nuestra infraestructura. Es igualmente no sorprendente que de todas las características post-despliegue del software que podríamos elegir para dedicar atención especial, la confiabilidad sea la que consideramos primaria.

A pesar de surgir en Google y en la comunidad de servicios web en general, creemos que esta disciplina tiene lecciones aplicables a otras comunidades y organizaciones. Este libro es un intento de explicar cómo hacemos las cosas: tanto para que otras organizaciones puedan aprovechar lo que hemos aprendido, como para que podamos definir mejor el papel y lo que significa el término. Con ese fin, hemos organizado el libro para que los principios generales y las prácticas más específicas Para ese fin, hemos organizado el libro de manera que los principios generales y las prácticas más específicas estén separados cuando sea posible, y cuando sea apropiado discutir un tema particular con información específica de Google, confiamos en que el lector nos permitirá hacerlo y no tendrá miedo de sacar conclusiones útiles sobre su propio entorno.

También hemos proporcionado algunos materiales de orientación - una descripción del entorno de producción de Google y un mapa entre algunos de nuestros software internos y software público - que deberían ayudar a contextualizar lo que estamos diciendo y hacerlo más directamente utilizable.

En última instancia, por supuesto, un software y una ingeniería de sistemas más orientados a la confiabilidad son inherentemente buenos. Sin embargo, reconocemos que las organizaciones más pequeñas pueden estar preguntándose cómo pueden aprovechar al máximo la experiencia representada aquí: al igual que la seguridad, cuanto antes se preocupe por la confiabilidad, mejor. Esto implica que aunque una organización pequeña tenga muchas preocupaciones urgentes y las opciones de software que tome pueden diferir de las que tomó Google, todavía vale la pena implementar un soporte de confiabilidad ligero desde el principio, porque es menos costoso ampliar una estructura más adelante que introducir una que no está presente. La gestión contiene un número de mejores prácticas para la formación, la comunicación y las reuniones que hemos encontrado que funcionan bien para nosotros, muchas de las cuales deberían ser inmediatamente utilizables por su organización.

Pero para tamaños entre una startup y una multinacional, probablemente ya hay alguien en su organización que está haciendo trabajo de SRE, sin que necesariamente se llame así o se reconozca como tal. Otra forma de comenzar a mejorar la confiabilidad para su organización es reconocer formalmente ese trabajo, o encontrar a esas personas y fomentar lo que hacen - recompensarlo. Son personas que se encuentran en la encrucijada entre una forma de ver el mundo y otra: como Newton, que a veces se llama no el primer físico del mundo, sino el último alquimista.

Y tomando la vista histórica, ¿quién, entonces, mirando hacia atrás, podría ser el primer SRE? Nos gusta pensar que Margaret Hamilton, trabajando en el programa Apolo en préstamo de MIT, tenía todos los rasgos significativos del primer SRE. En sus propias palabras, "parte de la cultura era aprender de todos y de todo, incluyendo de lo que menos se esperaría".

Un caso en punto fue cuando su joven hija Lauren vino a trabajar con ella un día, mientras algunos del equipo estaban ejecutando escenarios de misión en la computadora de simulación híbrida. Como los niños pequeños hacen, Lauren se fue explorando y causó que una "misión" se estrellara al seleccionar las teclas DSKY de una manera inesperada, alertando al equipo sobre lo que sucedería si el programa de prelanzamiento, P01, fuera seleccionado inadvertidamente por un astronauta real durante una misión real, durante el curso real. (Lanzar P01 inadvertidamente en una misión real sería un problema mayor, porque borra los datos de navegación, y la computadora no estaba equipada para pilotar la nave con datos de navegación inexistentes).

Con los instintos de un SRE, Margaret presentó una solicitud de cambio de programa para agregar un código de verificación de errores especial en el software de vuelo a bordo en caso de que un astronauta seleccionara P01 inadvertidamente durante una misión real. Pero este movimiento fue considerado innecesario por los "superiores" de la NASA: ¡por supuesto, eso nunca podría suceder! Entonces, en lugar de agregar un código de verificación de errores, Margaret actualizó la documentación de las especificaciones de la misión para decir lo equivalente a "No seleccione P01 durante el vuelo". (Al parecer, la actualización fue divertida para muchos en el proyecto, quienes habían sido informados muchas veces de que los astronautas no cometerían errores - después de todo, habían sido entrenados para ser perfectos).

Bueno, la salvaguarda sugerida por Margaret solo fue considerada innecesaria hasta la siguiente misión, en el Apolo 8, justo días después de la actualización de las especificaciones. Durante el curso medio del cuarto día de vuelo con los astronautas Jim Lovell, William Anders y Frank Borman a bordo, Jim Lovell seleccionó P01 por error - como sucede, en Navidad - creando mucha confusión para todos los involucrados. Este fue un problema crítico, porque en ausencia de una solución alternativa, no había datos de navegación, lo que significaba que los astronautas nunca regresarían a casa. Afortunadamente, la actualización de la documentación había llamado explícitamente a esta posibilidad, y fue invaluable para averiguar cómo subir datos útiles y recuperar la misión, con poco tiempo que perder.

Como dice Margaret, "una comprensión exhaustiva de cómo operar los sistemas no fue suficiente para prevenir errores humanos", y la solicitud de cambio para agregar software de detección y recuperación de errores al programa de prelanzamiento P01 fue aprobada poco después.

Aunque el incidente del Apolo 8 ocurrió hace décadas, hay mucho en los párrafos anteriores directamente relevante para la vida de los ingenieros hoy en día, y mucho que seguirá siendo directamente relevante en el futuro. Por lo tanto, para los sistemas que cuidan, para los grupos en los que trabajan o para las organizaciones que están construyendo, por favor, tengan en cuenta el Camino SRE: la exhaustividad y la dedicación, la creencia en el valor de la preparación y la documentación, y la conciencia de lo que podría salir mal, junto con un fuerte deseo de prevenirlo. ¡Bienvenidos a nuestra profesión emergente!

Este libro es una serie de ensayos escritos por miembros y ex miembros de la organización de Ingeniería de Confiabilidad del Sitio (SRE) de Google. Es más similar a las actas de una conferencia que a un libro estándar escrito por un autor o un pequeño número de autores. Cada capítulo está diseñado para ser leído como parte de un todo coherente, pero se puede obtener mucho leyendo sobre cualquier tema que te interese en particular. (Si hay otros artículos que apoyan o informan el texto, los citamos para que puedas seguir adelante).

No necesitas leer en un orden particular, aunque sugerimos al menos comenzar con los capítulos "El entorno de producción en Google, desde la perspectiva de un SRE" y "Aceptando el riesgo", que describen el entorno de producción de Google y esbozan cómo SRE aborda el riesgo, respectivamente. (El riesgo es, en muchos sentidos, la calidad clave de nuestra profesión). Leer de principio a fin es, por supuesto, también útil y posible; nuestros capítulos están agrupados temáticamente en Principios, Prácticas y Gestión. Cada uno tiene una pequeña introducción que destaca de qué tratan las piezas individuales y hace referencia a otros artículos publicados por SREs de Google, que cubren temas específicos en más detalle. Además, el sitio web complementario de este libro, https://g.co/SREBook, tiene una serie de recursos útiles.

Esperamos que esto sea al menos tan útil e interesante para ti como lo fue para nosotros reunirlo.

— Los editores


Parte I - Introducción

Esta sección proporciona algunas orientaciones generales sobre qué es SRE y por qué es diferente de las prácticas más convencionales de la industria de TI.

Ben Treynor Sloss, el vicepresidente senior que supervisa las operaciones técnicas en Google - y el originador del término "Ingeniería de Confiabilidad del Sitio" (SRE) - proporciona su visión sobre qué significa SRE, cómo funciona y cómo se compara con otras formas de hacer las cosas en la industria, en la Introducción.

Proporcionamos una guía sobre el entorno de producción en Google en El entorno de producción en Google, desde la perspectiva de un SRE, como una forma de ayudarte a familiarizarte con la gran cantidad de nuevos términos y sistemas que vas a conocer en el resto del libro.

Capítulo 1 - Introducción

En este capítulo, Ben Treynor Sloss presenta su visión sobre qué es SRE y cómo funciona. También compara SRE con otras formas de hacer las cosas en la industria de TI y explica por qué SRE es diferente.

El capítulo comienza con una definición de SRE y una explicación de cómo se originó el término. Luego, Treynor Sloss describe los principios clave de SRE, incluyendo la importancia de la confiabilidad, la escalabilidad y la eficiencia.

También se discute la relación entre SRE y la ingeniería de software, y cómo SRE se enfoca en la operación y el mantenimiento de los sistemas en lugar de solo en su desarrollo. Treynor Sloss también destaca la importancia de la colaboración y la comunicación entre los equipos de SRE y otros equipos en la organización.

Finalmente, el capítulo concluye con una discusión sobre los beneficios de SRE y cómo puede ayudar a las organizaciones a mejorar la confiabilidad y la eficiencia de sus sistemas.

Capítulo 1 - Introducción

Escrito por Benjamin Treynor Sloss
Editado por Betsy Beyer

La esperanza no es una estrategia.
Dicho tradicional de SRE

Es una verdad universalmente reconocida que los sistemas no se ejecutan solos. ¿Cómo, entonces, debería ejecutarse un sistema, particularmente un sistema de cómputo complejo que opera a gran escala?

El enfoque de Sysadmin para la gestión de servicios
Históricamente, las empresas han empleado administradores de sistemas para ejecutar sistemas de cómputo complejos.

Este enfoque de administrador de sistemas, o sysadmin, implica ensamblar componentes de software existentes y desplegarlos para que trabajen juntos para producir un servicio. Los sysadmins luego son responsables de ejecutar el servicio y responder a eventos y actualizaciones a medida que ocurren. A medida que el sistema crece en complejidad y volumen de tráfico, generando un aumento correspondiente en eventos y actualizaciones, el equipo de sysadmins crece para absorber el trabajo adicional. Debido a que el papel de sysadmin requiere un conjunto de habilidades muy diferente al de los desarrolladores de productos, los desarrolladores y los sysadmins están divididos en equipos discretos: "desarrollo" y "operaciones" o "ops".

El modelo de gestión de servicios de sysadmin tiene varias ventajas. Para las empresas que deciden cómo ejecutar y dotar de personal a un servicio, este enfoque es relativamente fácil de implementar: como un paradigma industrial familiar, hay muchos ejemplos de los que aprender y emular. Hay un talento disponible en todo el mundo. Hay una variedad de herramientas, componentes de software (de estantería o no) y empresas de integración disponibles para ayudar a ejecutar esos sistemas ensamblados, por lo que un equipo de sysadmins novatos no tiene que reinventar la rueda y diseñar un sistema desde cero.

El enfoque de sysadmin y la división de desarrollo/ops que lo acompaña tienen una serie de desventajas y trampas. Estas se dividen en dos categorías: costos directos e indirectos.

Los costos directos no son sutiles ni ambiguos. Ejecutar un servicio con un equipo que depende de la intervención manual para la gestión de cambios y el manejo de eventos se vuelve caro a medida que el servicio y/o el tráfico al servicio crecen, porque el tamaño del equipo debe escalar con la carga generada por el sistema.

Los costos indirectos de la división de desarrollo/ops pueden ser sutiles, pero a menudo son más costosos para la organización que los costos directos. Estos costos surgen del hecho de que los dos equipos son muy diferentes en antecedentes, habilidades y incentivos. Utilizan un vocabulario diferente para describir situaciones; llevan diferentes suposiciones sobre riesgos y posibilidades de soluciones técnicas; tienen diferentes suposiciones sobre el nivel de estabilidad del producto objetivo. La división entre los grupos puede fácilmente convertirse en una de no solo incentivos, sino también comunicación, objetivos y eventualmente, confianza y respeto. Este resultado es una patología.

Los equipos de operaciones tradicionales y sus contrapartes en el desarrollo de productos a menudo terminan en conflicto, más visible sobre cuán rápido se puede lanzar software a producción. En su núcleo, los equipos de desarrollo quieren lanzar nuevas características y verlas adoptadas por los usuarios. En su núcleo, los equipos de operaciones quieren asegurarse de que el servicio no se rompa mientras están sujetando la página. Debido a que la mayoría de las interrupciones son causadas por algún tipo de cambio, un lanzamiento de características nuevo, un lanzamiento de configuración nuevo o un tipo nuevo de tráfico de usuarios, los objetivos de los dos equipos están fundamentalmente en tensión.

Ambos grupos entienden que es inaceptable expresar sus intereses en los términos más brutales posibles ("Queremos lanzar cualquier cosa, en cualquier momento, sin obstáculos" versus "No queremos cambiar nada en el sistema una vez que funciona"). Y porque su vocabulario y suposiciones de riesgo difieren, ambos grupos a menudo recurren a una forma familiar de guerra de trincheras para avanzar en sus intereses. El equipo de operaciones intenta salvaguardar el sistema en ejecución contra el riesgo de cambio introduciendo puertas de lanzamiento y cambio. 



GOOGLE APPROACH TO SERVICE MANAGEMENT: SITE RELIABILITY ENGINEERING

Por El conflicto no es una parte inevitable de ofrecer un servicio de software. Google ha elegido ejecutar nuestros sistemas con un enfoque diferente: nuestros equipos de Ingeniería de Confiabilidad del Sitio (SRE) se centran en contratar ingenieros de software para ejecutar nuestros productos y crear sistemas para realizar el trabajo que normalmente sería realizado, a menudo de manera manual, por administradores de sistemas.

¿Qué es exactamente la Ingeniería de Confiabilidad del Sitio, como se ha definido en Google? Mi explicación es simple: SRE es lo que sucede cuando se le pide a un ingeniero de software que diseñe un equipo de operaciones. Cuando me uní a Google en 2003 y me encargué de ejecutar un "Equipo de Producción" de siete ingenieros, toda mi vida hasta ese momento había sido ingeniería de software. Así que diseñé y gestioné el grupo de la manera en que querría que funcionara si yo mismo trabajara como SRE. Ese grupo ha madurado desde entonces para convertirse en el equipo de SRE de Google actual, que sigue siendo fiel a sus orígenes como lo imaginó un ingeniero de software de toda la vida.

Un bloque de construcción fundamental del enfoque de Google para la gestión de servicios es la composición de cada equipo de SRE. En general, los SRE se pueden dividir en dos categorías principales.

50-60% son ingenieros de software de Google, o más precisamente, personas que han sido contratadas a través del procedimiento estándar para ingenieros de software de Google. El otro 40-50% son candidatos que estaban muy cerca de las calificaciones de ingeniería de software de Google (es decir, 85-99% del conjunto de habilidades requeridas), y que además tenían un conjunto de habilidades técnicas que es útil para SRE pero es raro para la mayoría de los ingenieros de software. Por lejos, los conocimientos de internos del sistema UNIX y la experiencia en redes (Capa 1 a Capa 3) son los dos tipos más comunes de habilidades técnicas alternativas que buscamos.

Común a todos los SRE es la creencia y la aptitud para desarrollar sistemas de software para resolver problemas complejos. Dentro de SRE, seguimos de cerca el progreso de carrera de ambos grupos, y hasta la fecha no hemos encontrado ninguna diferencia práctica en el rendimiento entre ingenieros de las dos vías. De hecho, el fondo diverso del equipo de SRE a menudo resulta en sistemas inteligentes y de alta calidad que son claramente el producto de la síntesis de varios conjuntos de habilidades.

El resultado de nuestro enfoque para contratar personal para SRE es que terminamos con un equipo de personas que (a) rápidamente se aburren de realizar tareas manuales, y (b) tienen las habilidades necesarias para escribir software que reemplace su trabajo manual anterior, incluso cuando la solución es complicada. Los SREs también comparten un trasfondo académico e intelectual con el resto de la organización de desarrollo. Por lo tanto, SRE realiza fundamentalmente el trabajo que históricamente ha sido hecho por un equipo de operaciones, pero utilizando ingenieros con experiencia en software, apostando por el hecho de que estos ingenieros están inherentemente predispuestos y tienen la capacidad para diseñar e implementar automatización mediante software para reemplazar el trabajo humano.

Por diseño, es crucial que los equipos de SRE se enfoquen en la ingeniería. Sin una ingeniería constante, la carga de operaciones aumenta y los equipos necesitarán más personas solo para mantenerse al día con la carga de trabajo. Eventualmente, un grupo tradicional enfocado en operaciones escala linealmente con el tamaño del servicio: si los productos apoyados por el servicio tienen éxito, la carga operativa crecerá con el tráfico. Esto significa contratar más personas para hacer las mismas tareas una y otra vez.

Para evitar este destino, el equipo encargado de gestionar un servicio necesita programar o se ahogará. Por lo tanto, Google impone un límite del 50% al trabajo operativo agregado para todos los SREs—tickets, guardias, tareas manuales, etc. Este límite asegura que el equipo de SRE tenga suficiente tiempo en su agenda para hacer que el servicio sea estable y operable. Este límite es un límite superior; con el tiempo, si se les deja a su suerte, el equipo de SRE debería terminar con muy poca carga operativa y casi dedicarse por completo a tareas de desarrollo, porque el servicio básicamente se ejecuta y repara solo: queremos sistemas que sean automáticos, no solo automatizados. En la práctica, la escala y las nuevas características mantienen a los SREs en alerta.

La regla general de Google es que un equipo de SRE debe dedicar el 50% restante de su tiempo realmente al desarrollo. Entonces, ¿cómo hacemos cumplir ese umbral? En primer lugar, tenemos que medir cómo se gasta el tiempo de los SRE. Con esa medición en mano, aseguramos que los equipos que consistentemente dedican menos del 50% de su tiempo al trabajo de desarrollo cambien sus prácticas. A menudo, esto significa trasladar parte de la carga operativa de vuelta al equipo de desarrollo, o agregar personal al equipo sin asignarles responsabilidades operativas adicionales. Mantener conscientemente este equilibrio entre el trabajo operativo y el de desarrollo nos permite asegurarnos de que los SREs tengan el ancho de banda para involucrarse en la ingeniería creativa y autónoma, mientras retienen la sabiduría adquirida del lado operativo de la gestión de un servicio.

Hemos encontrado que el enfoque de Google SRE para operar sistemas a gran escala tiene muchas ventajas. Debido a que los SREs están modificando directamente el código en su búsqueda de hacer que los sistemas de Google se ejecuten por sí mismos, los equipos de SRE se caracterizan por una innovación rápida y una gran aceptación del cambio. Dichos equipos son relativamente económicos—apoyar el mismo servicio con un equipo orientado a operaciones requeriría un número significativamente mayor de personas. En cambio, el número de SREs necesarios para ejecutar, mantener y mejorar un sistema escala de manera sublineal con el tamaño del sistema. Finalmente, SRE no solo evita la disfuncionalidad de la división dev/ops, sino que esta estructura también mejora nuestros equipos de desarrollo de productos: las transferencias fáciles entre desarrollo de productos y equipos de SRE entrenan transversalmente a todo el grupo, y mejoran las habilidades de los desarrolladores que de otra manera podrían tener dificultades para aprender a construir un sistema distribuido de millones de núcleos.

A pesar de estas ganancias netas, el modelo SRE se caracteriza por su propio conjunto distintivo de desafíos. Un desafío continuo que enfrenta Google es la contratación de SREs: SRE no solo compite por los mismos candidatos que el pipeline de contratación de desarrollo de productos, sino que el hecho de que establecemos un estándar de contratación tan alto en términos de habilidades tanto de codificación como de ingeniería de sistemas significa que nuestro grupo de candidatos es necesariamente pequeño. Dado que nuestra disciplina es relativamente nueva y única, no existe mucha información en la industria sobre cómo construir y gestionar un equipo de SRE (aunque esperamos que este libro avance en esa dirección). Y una vez que un equipo de SRE está en su lugar, sus enfoques potencialmente poco ortodoxos para la gestión de servicios requieren un fuerte apoyo de la gerencia. Por ejemplo, la decisión de detener lanzamientos durante el resto del trimestre una vez que se ha agotado el presupuesto de errores podría no ser bien recibida por un equipo de desarrollo de productos, a menos que sea ordenada por su gerencia.

¿DevOps o SRE?

El término "DevOps" surgió en la industria a finales de 2008 y, al momento de escribir este texto (principios de 2016), aún se encuentra en evolución. Sus principios fundamentales—la participación de la función de TI en cada fase del diseño y desarrollo de un sistema, la fuerte dependencia de la automatización en lugar del esfuerzo humano, y la aplicación de prácticas y herramientas de ingeniería a tareas operativas—son consistentes con muchos de los principios y prácticas de SRE. Se podría considerar DevOps como una generalización de varios principios fundamentales de SRE para un rango más amplio de organizaciones, estructuras de gestión y personal. De manera equivalente, se podría ver a SRE como una implementación específica de DevOps con algunas extensiones idiosincráticas.

Aunque los matices de los flujos de trabajo, las prioridades y las operaciones diarias varían de un equipo de SRE a otro, todos comparten un conjunto de responsabilidades básicas para los servicios que apoyan y se adhieren a los mismos principios fundamentales. En general, un equipo de SRE es responsable de la disponibilidad, latencia, rendimiento, eficiencia, gestión de cambios, monitoreo, respuesta a emergencias y planificación de la capacidad de sus servicios. Hemos codificado reglas de compromiso y principios sobre cómo los equipos de SRE interactúan con su entorno, no solo con el entorno de producción, sino también con los equipos de desarrollo de productos, los equipos de pruebas, los usuarios, y más. Estas reglas y prácticas de trabajo nos ayudan a mantener nuestro enfoque en el trabajo de ingeniería, en lugar del trabajo de operaciones.

Asegurando un Enfoque Duradero en la Ingeniería

Como ya se ha mencionado, Google limita el trabajo operativo de los SREs al 50% de su tiempo. El tiempo restante debe dedicarse a utilizar sus habilidades de codificación en proyectos. En la práctica, esto se logra monitoreando la cantidad de trabajo operativo que realizan los SREs y redirigiendo el exceso de trabajo operativo a los equipos de desarrollo de productos: reasignando errores y tickets a los gerentes de desarrollo, reintegrando a los desarrolladores en las rotaciones de guardia, y así sucesivamente. La redirección termina cuando la carga operativa vuelve a bajar al 50% o menos. Esto también proporciona un mecanismo de retroalimentación eficaz, guiando a los desarrolladores a construir sistemas que no necesiten intervención manual. Este enfoque funciona bien cuando toda la organización, tanto SRE como desarrollo, entiende por qué existe el mecanismo de válvula de seguridad y apoya el objetivo de no tener eventos de desbordamiento porque el producto no genera suficiente carga operativa para requerirlo.

Cuando se enfocan en el trabajo operativo, en promedio, los SREs deberían recibir un máximo de dos eventos por turno de guardia de 8 a 12 horas. Este volumen objetivo da al ingeniero de guardia suficiente tiempo para manejar el evento con precisión y rapidez, limpiar y restaurar el servicio normal, y luego realizar un análisis post-mortem. Si ocurren más de dos eventos regularmente por turno de guardia, los problemas no pueden investigarse a fondo y los ingenieros están lo suficientemente abrumados como para impedirles aprender de estos eventos. Un escenario de fatiga por llamadas tampoco mejorará con la escala. Por el contrario, si los SREs de guardia consistentemente reciben menos de un evento por turno, mantenerlos en alerta es una pérdida de tiempo.

Se deben escribir análisis post-mortem para todos los incidentes significativos, independientemente de si activaron una llamada o no; los análisis post-mortem que no desencadenaron una llamada son aún más valiosos, ya que probablemente señalan claras lagunas en el monitoreo. Esta investigación debe establecer lo que ocurrió en detalle, encontrar todas las causas raíz del evento y asignar acciones para corregir el problema o mejorar cómo se abordará la próxima vez. Google opera bajo una cultura de post-mortem sin culpas, con el objetivo de exponer fallos y aplicar ingeniería para corregirlos, en lugar de evitarlos o minimizarlos.

Persiguiendo la Máxima Velocidad de Cambio Sin Violar el SLO de un Servicio

Los equipos de desarrollo de productos y SRE pueden disfrutar de una relación de trabajo productiva eliminando el conflicto estructural en sus objetivos respectivos. El conflicto estructural se da entre el ritmo de innovación y la estabilidad del producto, y como se describió anteriormente, este conflicto a menudo se expresa de manera indirecta. En SRE, llevamos este conflicto al primer plano y lo resolvemos mediante la introducción de un presupuesto de errores (error budget).

El presupuesto de errores surge de la observación de que el 100% no es el objetivo de confiabilidad correcto para prácticamente todo (siendo los marcapasos y los frenos antibloqueo notables excepciones). En general, para cualquier servicio o sistema de software, el 100% no es el objetivo de confiabilidad adecuado porque ningún usuario puede notar la diferencia entre un sistema que está disponible el 100% del tiempo y uno que lo está el 99.999%. Hay muchos otros sistemas en el camino entre el usuario y el servicio (su portátil, su WiFi doméstico, su ISP, la red eléctrica...) y esos sistemas colectivamente tienen una disponibilidad mucho menor que el 99.999%. Por lo tanto, la diferencia marginal entre el 99.999% y el 100% se pierde en el ruido de otras indisponibilidades, y el usuario no recibe ningún beneficio del enorme esfuerzo requerido para agregar ese último 0.001% de disponibilidad.

Si el 100% es el objetivo de confiabilidad incorrecto para un sistema, entonces, ¿cuál es el objetivo correcto? En realidad, esta no es una pregunta técnica en absoluto, sino una pregunta de producto, que debería considerar las siguientes cuestiones:

¿Qué nivel de disponibilidad hará que los usuarios estén satisfechos, dado cómo usan el producto?
¿Qué alternativas están disponibles para los usuarios que están insatisfechos con la disponibilidad del producto?
¿Qué sucede con el uso que los usuarios hacen del producto en diferentes niveles de disponibilidad?
El negocio o el producto deben establecer el objetivo de disponibilidad del sistema. Una vez que se establece ese objetivo, el presupuesto de errores es uno menos el objetivo de disponibilidad. Un servicio que tiene una disponibilidad del 99.99% es 0.01% no disponible. Ese 0.01% de indisponibilidad permitida es el presupuesto de errores del servicio. Podemos gastar ese presupuesto en lo que queramos, siempre y cuando no lo excedamos.

Entonces, ¿cómo queremos gastar el presupuesto de errores? El equipo de desarrollo quiere lanzar funciones y atraer nuevos usuarios. Idealmente, gastaríamos todo nuestro presupuesto de errores tomando riesgos con las cosas que lanzamos para lanzarlas rápidamente. Este premisa básica describe todo el modelo de presupuestos de errores. Tan pronto como las actividades de SRE se conceptualizan en este marco, liberar el presupuesto de errores mediante tácticas como implementaciones por fases y experimentos al 1% puede optimizar para lanzamientos más rápidos.

El uso de un presupuesto de errores resuelve el conflicto estructural de incentivos entre el desarrollo y SRE. El objetivo de SRE ya no es "cero interrupciones"; más bien, los SREs y los desarrolladores de productos apuntan a gastar el presupuesto de errores obteniendo la máxima velocidad de características. Este cambio hace toda la diferencia. Una interrupción ya no es algo "malo", es una parte esperada del proceso de innovación y una ocurrencia que tanto los equipos de desarrollo como de SRE manejan en lugar de temer.

Monitoreo

El monitoreo es uno de los medios principales por los cuales los propietarios de servicios realizan un seguimiento de la salud y disponibilidad de un sistema. Como tal, la estrategia de monitoreo debe construirse con cuidado. Un enfoque clásico y común para el monitoreo es observar un valor o condición específica y luego activar una alerta por correo electrónico cuando ese valor se excede o esa condición ocurre. Sin embargo, este tipo de alerta por correo electrónico no es una solución efectiva: un sistema que requiere que un humano lea un correo electrónico y decida si se necesita o no algún tipo de acción en respuesta es fundamentalmente defectuoso. El monitoreo nunca debería requerir que un humano interprete ninguna parte del dominio de alertas. En su lugar, el software debería hacer la interpretación, y los humanos solo deberían ser notificados cuando necesiten tomar acción.

Hay tres tipos de salidas de monitoreo válidas:

Alertas

Indican que un humano necesita tomar acción inmediatamente en respuesta a algo que está sucediendo o está a punto de suceder, para mejorar la situación.
Tickets

Indican que un humano necesita tomar acción, pero no de inmediato. El sistema no puede manejar la situación automáticamente, pero si un humano toma acción en unos pocos días, no ocurrirá ningún daño.
Registros (Logging)

Nadie necesita mirar esta información, pero se registra para propósitos diagnósticos o forenses. La expectativa es que nadie lea los registros a menos que algo más los impulse a hacerlo.

El resultado de nuestro enfoque para contratar a SRE (Site Reliability Engineers) es que terminamos con un equipo de personas que (a) rápidamente se aburrirán de realizar tareas manualmente, y (b) tienen las habilidades necesarias para escribir software que reemplace su trabajo manual anterior, incluso cuando la solución es complicada. Los SREs también terminan compartiendo un trasfondo académico e intelectual con el resto de la organización de desarrollo. Por lo tanto, el trabajo de SRE consiste fundamentalmente en realizar tareas que históricamente han sido realizadas por un equipo de operaciones, pero utilizando ingenieros con experiencia en software, y confiando en el hecho de que estos ingenieros están inherentemente predispuestos y tienen la capacidad de diseñar e implementar automatización con software para reemplazar el trabajo humano.

Por diseño, es crucial que los equipos de SRE estén enfocados en la ingeniería. Sin una ingeniería constante, la carga operativa aumenta y los equipos necesitarán más personas solo para mantenerse al día con la carga de trabajo. Eventualmente, un grupo tradicional enfocado en operaciones se escala de manera lineal con el tamaño del servicio: si los productos respaldados por el servicio tienen éxito, la carga operativa crecerá con el tráfico. Eso significa contratar a más personas para realizar las mismas tareas una y otra vez.

Para evitar este destino, el equipo encargado de gestionar un servicio necesita programar o se ahogará. Por lo tanto, Google establece un límite del 50% en el trabajo "operativo" agregado para todos los SREs: tickets, guardias, tareas manuales, etc. Este límite asegura que el equipo de SRE tenga suficiente tiempo en su agenda para hacer que el servicio sea estable y operable. Este límite es un techo; con el tiempo, si se les deja a su propio ritmo, el equipo de SRE debería terminar con muy poca carga operativa y dedicarse casi por completo a tareas de desarrollo, porque el servicio básicamente se ejecuta y repara solo: queremos sistemas que sean automáticos, no solo automatizados. En la práctica, la escala y las nuevas características mantienen a los SREs alerta.

La regla general de Google es que un equipo de SRE debe dedicar el 50% restante de su tiempo a hacer desarrollo. Entonces, ¿cómo hacemos cumplir ese umbral? En primer lugar, tenemos que medir cómo se gasta el tiempo de los SRE. Con esa medición en mano, nos aseguramos de que los equipos que consistentemente dedican menos del 50% de su tiempo a trabajo de desarrollo cambien sus prácticas. A menudo, esto significa trasladar parte de la carga operativa de nuevo al equipo de desarrollo, o agregar personal al equipo sin asignarles responsabilidades operativas adicionales. Mantener conscientemente este equilibrio entre el trabajo operativo y de desarrollo nos permite asegurarnos de que los SREs tengan el ancho de banda para participar en una ingeniería creativa y autónoma, mientras retienen la sabiduría adquirida en el lado operativo de la gestión de un servicio.

Hemos descubierto que el enfoque de los SRE de Google para operar sistemas a gran escala tiene muchas ventajas. Debido a que los SREs modifican directamente el código en su esfuerzo por hacer que los sistemas de Google se gestionen solos, los equipos de SRE se caracterizan por su rápida innovación y una gran aceptación del cambio. Tales equipos son relativamente económicos; apoyar el mismo servicio con un equipo orientado a operaciones requeriría un número significativamente mayor de personas. En cambio, el número de SREs necesarios para operar, mantener y mejorar un sistema escala de manera sublineal con el tamaño del sistema. Finalmente, no solo SRE elude la disfuncionalidad de la división entre desarrollo y operaciones, sino que esta estructura también mejora a nuestros equipos de desarrollo de productos: las transferencias fáciles entre los equipos de desarrollo de productos y SRE permiten entrenar a todo el grupo de manera cruzada y mejorar las habilidades de los desarrolladores que de otro modo podrían tener dificultades para aprender a construir un sistema distribuido de millones de núcleos.

A pesar de estas ganancias netas, el modelo de SRE se caracteriza por su propio conjunto de desafíos. Un desafío constante que enfrenta Google es la contratación de SREs: no solo SRE compite por los mismos candidatos que el canal de contratación de desarrollo de productos, sino que el hecho de que establecemos un listón tan alto en términos de habilidades de codificación e ingeniería de sistemas significa que nuestra piscina de contratación es necesariamente pequeña. Como nuestra disciplina es relativamente nueva y única, no existe mucha información en la industria sobre cómo construir y gestionar un equipo de SRE (¡aunque esperamos que este libro avance en esa dirección!). Y una vez que un equipo de SRE está en su lugar, sus enfoques potencialmente poco ortodoxos para la gestión de servicios requieren un fuerte apoyo de la dirección. Por ejemplo, la decisión de detener lanzamientos durante el resto del trimestre una vez que se agota un presupuesto de errores podría no ser bien recibida por un equipo de desarrollo de productos a menos que esté ordenada por su dirección.

¿DevOps o SRE?

El término "DevOps" surgió en la industria a finales de 2008 y, al momento de escribir esto (principios de 2016), todavía está en un estado de cambio. Sus principios básicos —la participación de la función de TI en cada fase del diseño y desarrollo de un sistema, la fuerte dependencia de la automatización en lugar del esfuerzo humano, la aplicación de prácticas y herramientas de ingeniería a las tareas de operaciones— son consistentes con muchos de los principios y prácticas de SRE. Se podría ver a DevOps como una generalización de varios principios fundamentales de SRE a un rango más amplio de organizaciones, estructuras de gestión y personal. De manera equivalente, se podría ver a SRE como una implementación específica de DevOps con algunas extensiones idiosincráticas.

Principios fundamentales de SRE

Si bien los matices de los flujos de trabajo, las prioridades y las operaciones diarias varían de un equipo de SRE a otro, todos comparten un conjunto de responsabilidades básicas para los servicios que respaldan y adhieren a los mismos principios fundamentales. En general, un equipo de SRE es responsable de la disponibilidad, latencia, rendimiento, eficiencia, gestión de cambios, monitoreo, respuesta a emergencias y planificación de capacidad de sus servicios. Hemos codificado reglas de compromiso y principios sobre cómo los equipos de SRE interactúan con su entorno, no solo el entorno de producción, sino también los equipos de desarrollo de productos, los equipos de pruebas, los usuarios, y más. Esas reglas y prácticas de trabajo nos ayudan a mantener nuestro enfoque en el trabajo de ingeniería, en lugar del trabajo operativo.

Asegurando un Enfoque Duradero en la Ingeniería

Como ya se mencionó, Google limita el trabajo operativo para los SREs al 50% de su tiempo. El tiempo restante debe dedicarse a usar sus habilidades de codificación en proyectos. En la práctica, esto se logra monitoreando la cantidad de trabajo operativo realizado por los SREs y redirigiendo el exceso de trabajo operativo a los equipos de desarrollo de productos: reasignando errores y tickets a los gerentes de desarrollo, [re]integrando a los desarrolladores en las rotaciones de guardias, y así sucesivamente. La redirección termina cuando la carga operativa vuelve al 50% o menos. Esto también proporciona un mecanismo de retroalimentación efectivo, guiando a los desarrolladores a construir sistemas que no necesiten intervención manual. Este enfoque funciona bien cuando toda la organización —SRE y desarrollo por igual— entiende por qué existe el mecanismo de válvula de seguridad y apoya el objetivo de no tener eventos de desbordamiento porque el producto no genera suficiente carga operativa para requerirlo.

Cuando se enfocan en el trabajo operativo, en promedio, los SREs deberían recibir un máximo de dos eventos por turno de guardia de 8 a 12 horas. Este volumen objetivo da al ingeniero de guardia suficiente tiempo para manejar el evento con precisión y rapidez, limpiar y restaurar el servicio normal, y luego realizar un análisis postmortem. Si ocurren más de dos eventos regularmente por turno de guardia, los problemas no pueden ser investigados a fondo y los ingenieros están lo suficientemente abrumados como para evitar que aprendan de estos eventos. Un escenario de fatiga de paginación tampoco mejorará con la escala. Por el contrario, si los SREs de guardia reciben consistentemente menos de un evento por turno, mantenerlos en alerta es una pérdida de su tiempo.

Los postmortems deben redactarse para todos los incidentes significativos, independientemente de si activaron una alerta o no; los postmortems que no activaron una alerta son aún más valiosos, ya que probablemente señalan claras brechas en la monitorización. Esta investigación debe establecer en detalle lo que sucedió, encontrar todas las causas raíz del evento y asignar acciones para corregir el problema o mejorar cómo se aborda la próxima vez. Google opera bajo una cultura de postmortem sin culpa, con el objetivo de exponer fallas y aplicar ingeniería para corregir estas fallas, en lugar de evitarlas o minimizarlas.

Perseguir la Máxima Velocidad de Cambio Sin Violar el SLO de un Servicio
Los equipos de desarrollo de productos y SRE pueden disfrutar de una relación de trabajo productiva eliminando el conflicto estructural en sus respectivos objetivos. El conflicto estructural está entre la velocidad de innovación y la estabilidad del producto, y como se describió anteriormente, este conflicto a menudo se expresa de manera indirecta. En SRE, llevamos este conflicto al primer plano y luego lo resolvemos con la introducción de un presupuesto de errores.

El presupuesto de errores surge de la observación de que el 100% es el objetivo de confiabilidad incorrecto para prácticamente todo (siendo los marcapasos y los frenos antibloqueo excepciones notables). En general, para cualquier servicio o sistema de software, el 100% no es el objetivo de confiabilidad correcto porque ningún usuario puede notar la diferencia entre un sistema que está disponible el 100% del tiempo y uno que está disponible el 99.999% del tiempo. Hay muchos otros sistemas en el camino entre el usuario y el servicio (su computadora portátil, su WiFi en casa, su ISP, la red eléctrica…) y esos sistemas, colectivamente, tienen una disponibilidad mucho menor al 99.999%. Por lo tanto, la diferencia marginal entre el 99.999% y el 100% se pierde en el ruido de otras indisponibilidades, y el usuario no recibe ningún beneficio del enorme esfuerzo requerido para agregar ese último 0.001% de disponibilidad.

Si el 100% es el objetivo de confiabilidad incorrecto para un sistema, entonces, ¿cuál es el objetivo de confiabilidad correcto? En realidad, esta no es una pregunta técnica en absoluto, es una pregunta de producto, que debe tener en cuenta las siguientes consideraciones:

¿Con qué nivel de disponibilidad estarán satisfechos los usuarios, dado cómo usan el producto?
¿Qué alternativas tienen los usuarios que están insatisfechos con la disponibilidad del producto?
¿Qué sucede con el uso que hacen los usuarios del producto a diferentes niveles de disponibilidad?
El negocio o el equipo de producto debe establecer el objetivo de disponibilidad del sistema. Una vez que se establece ese objetivo, el presupuesto de errores es uno menos el objetivo de disponibilidad. Un servicio que tiene un 99.99% de disponibilidad tiene un 0.01% de indisponibilidad. Esa indisponibilidad permitida del 0.01% es el presupuesto de errores del servicio. Podemos gastar ese presupuesto en lo que queramos, siempre que no lo excedamos.

Entonces, ¿cómo queremos gastar el presupuesto de errores? El equipo de desarrollo quiere lanzar funciones y atraer nuevos usuarios. Idealmente, gastaríamos todo nuestro presupuesto de errores asumiendo riesgos con las cosas que lanzamos para lanzarlas rápidamente. Este principio básico describe todo el modelo de presupuestos de errores. Tan pronto como las actividades de SRE se conceptualizan en este marco, liberar el presupuesto de errores a través de tácticas como lanzamientos por fases y experimentos del 1% puede optimizar los lanzamientos más rápidos.

El uso de un presupuesto de errores resuelve el conflicto estructural de incentivos entre desarrollo y SRE. El objetivo de SRE ya no es "cero interrupciones"; más bien, los SREs y los desarrolladores de productos tienen como objetivo gastar el presupuesto de errores para obtener la máxima velocidad de funciones. Este cambio marca toda la diferencia. Una interrupción ya no es algo "malo"; es una parte esperada del proceso de innovación, y un evento que tanto los equipos de desarrollo como los de SRE gestionan en lugar de temer.

Monitorización
La monitorización es uno de los medios principales por los cuales los propietarios de un servicio mantienen el control de la salud y la disponibilidad de un sistema. Como tal, la estrategia de monitorización debe ser construida con cuidado. Un enfoque clásico y común de la monitorización es observar un valor o condición específica, y luego activar una alerta por correo electrónico cuando ese valor se excede o esa condición ocurre. Sin embargo, este tipo de alerta por correo electrónico no es una solución efectiva: un sistema que requiere que un humano lea un correo electrónico y decida si se necesita o no tomar algún tipo de acción en respuesta está fundamentalmente defectuoso. La monitorización nunca debe requerir que un humano interprete ninguna parte del dominio de alertas. En su lugar, el software debe hacer la interpretación, y los humanos solo deben ser notificados cuando necesitan tomar acción.

Existen tres tipos de resultados válidos de monitorización:

Alertas
Indican que un humano necesita tomar acción de inmediato en respuesta a algo que está ocurriendo o que está a punto de ocurrir, para mejorar la situación.

Tickets
Indican que un humano necesita tomar acción, pero no de inmediato. El sistema no puede manejar la situación automáticamente, pero si un humano toma acción en unos días, no se producirá ningún daño.

Registro (Logging)
Nadie necesita revisar esta información, pero se registra para propósitos de diagnóstico o forenses. Se espera que nadie lea los registros a menos que algo más los impulse a hacerlo.

Respuesta a Emergencias
La confiabilidad es una función del tiempo medio hasta la falla (MTTF) y el tiempo medio de reparación (MTTR). La métrica más relevante para evaluar la efectividad de la respuesta a emergencias es qué tan rápido el equipo de respuesta puede devolver el sistema a su estado saludable, es decir, el MTTR.

Los humanos añaden latencia. Incluso si un sistema experimenta más fallas, un sistema que puede evitar emergencias que requieran intervención humana tendrá mayor disponibilidad que un sistema que requiere intervención manual. Cuando los humanos son necesarios, hemos descubierto que pensar y registrar las mejores prácticas de antemano en un "manual de procedimientos" produce una mejora de aproximadamente 3x en el MTTR en comparación con la estrategia de "improvisar". El ingeniero de guardia que es un héroe todoterreno funciona, pero el ingeniero de guardia con experiencia, armado con un manual de procedimientos, funciona mucho mejor. Aunque ningún manual, por completo que sea, sustituye a ingenieros inteligentes capaces de pensar de manera improvisada, los pasos claros y exhaustivos para la resolución de problemas son valiosos cuando se responde a una alerta de alta prioridad o sensible al tiempo. Por lo tanto, Google SRE confía en los manuales de procedimientos para guardias, además de ejercicios como la "Rueda de la Desgracia," para preparar a los ingenieros a reaccionar ante eventos de guardia.

Gestión de Cambios
SRE ha descubierto que aproximadamente el 70% de las interrupciones son debidas a cambios en un sistema en vivo. Las mejores prácticas en este ámbito utilizan la automatización para lograr lo siguiente:

Implementación de despliegues progresivos.
Detección rápida y precisa de problemas.
Reversión segura de cambios cuando surgen problemas.
Este trío de prácticas minimiza eficazmente el número total de usuarios y operaciones expuestos a cambios defectuosos. Al eliminar a los humanos del proceso, estas prácticas evitan los problemas normales de fatiga, familiaridad/desdén e inatención a tareas altamente repetitivas. Como resultado, tanto la velocidad de lanzamiento como la seguridad aumentan.

Pronóstico de Demanda y Planificación de Capacidad
El pronóstico de demanda y la planificación de capacidad se pueden ver como garantizar que haya suficiente capacidad y redundancia para satisfacer la demanda proyectada futura con la disponibilidad requerida. No hay nada particularmente especial sobre estos conceptos, excepto que un número sorprendente de servicios y equipos no toman las medidas necesarias para asegurar que la capacidad requerida esté disponible en el momento necesario.

La planificación de capacidad debe tener en cuenta tanto el crecimiento orgánico (que proviene de la adopción natural del producto y el uso por parte de los clientes) como el crecimiento inorgánico (que resulta de eventos como lanzamientos de características, campañas de marketing u otros cambios impulsados por el negocio).

Varios pasos son obligatorios en la planificación de capacidad:

Un pronóstico de demanda orgánica preciso, que se extienda más allá del tiempo de anticipación requerido para adquirir capacidad.
Una incorporación precisa de fuentes de demanda inorgánica en el pronóstico de demanda.
Pruebas de carga regulares del sistema para correlacionar la capacidad bruta (servidores, discos, etc.) con la capacidad del servicio.
Dado que la capacidad es crítica para la disponibilidad, naturalmente sigue que el equipo de SRE debe estar a cargo de la planificación de capacidad, lo que significa que también deben estar a cargo de la provisión.

Aprovisionamiento
El aprovisionamiento combina tanto la gestión de cambios como la planificación de capacidad. En nuestra experiencia, el aprovisionamiento debe realizarse rápidamente y solo cuando sea necesario, ya que la capacidad es costosa. Este ejercicio también debe hacerse correctamente o la capacidad no funcionará cuando se necesite. Añadir nueva capacidad a menudo implica iniciar una nueva instancia o ubicación, hacer modificaciones significativas a los sistemas existentes (archivos de configuración, balanceadores de carga, redes), y validar que la nueva capacidad funcione y entregue resultados correctos. Por lo tanto, es una operación más arriesgada que el cambio de carga, que se realiza con frecuencia varias veces por hora, y debe tratarse con un grado correspondiente de precaución adicional.

Eficiencia y Rendimiento
El uso eficiente de los recursos es importante siempre que un servicio se preocupe por el dinero. Dado que SRE controla en última instancia el aprovisionamiento, también debe involucrarse en cualquier trabajo relacionado con la utilización, ya que la utilización es una función de cómo funciona un servicio determinado y cómo se aprovisiona. Se deduce que prestar mucha atención a la estrategia de aprovisionamiento de un servicio, y por lo tanto a su utilización, proporciona una palanca muy grande sobre los costos totales del servicio.

El uso de recursos es una función de la demanda (carga), la capacidad y la eficiencia del software. Los SREs predicen la demanda, aprovisionan la capacidad y pueden modificar el software. Estos tres factores son una gran parte (aunque no la totalidad) de la eficiencia de un servicio. Los sistemas de software se vuelven más lentos a medida que se les añade carga. Una desaceleración en un servicio equivale a una pérdida de capacidad. En algún momento, un sistema que se ralentiza deja de servir, lo que corresponde a una lentitud infinita. Los SREs aprovisionan para cumplir con un objetivo de capacidad a una velocidad de respuesta específica, y por lo tanto están muy interesados en el rendimiento de un servicio. Los SREs y los desarrolladores de productos monitorearán y modificarán un servicio para mejorar su rendimiento, aumentando así la capacidad y mejorando la eficiencia.

El Fin del Comienzo
La Ingeniería de Fiabilidad del Sitio (SRE) representa una ruptura significativa con las mejores prácticas existentes en la industria para la gestión de servicios grandes y complicados. Motivada originalmente por la familiaridad—"como ingeniero de software, así es como querría invertir mi tiempo para realizar un conjunto de tareas repetitivas"—se ha convertido en mucho más: un conjunto de principios, un conjunto de prácticas, un conjunto de incentivos y un campo de esfuerzo dentro de la disciplina más amplia de la ingeniería de software. El resto del libro explora el Camino SRE en detalle.

Capítulo 2 - El Entorno de Producción en Google, desde la Perspectiva de un SRE
Escrito por JC van Winkel | Editado por Betsy Beyer

Los centros de datos de Google son muy diferentes de la mayoría de los centros de datos convencionales y granjas de servidores a pequeña escala. Estas diferencias presentan tanto problemas adicionales como oportunidades. Este capítulo analiza los desafíos y oportunidades que caracterizan a los centros de datos de Google e introduce la terminología que se utiliza a lo largo del libro.

Hardware
La mayoría de los recursos de computación de Google se encuentran en centros de datos diseñados por Google con distribución de energía, refrigeración, redes y hardware de computación propietarios (ver [Bar13]). A diferencia de los centros de datos de colocación "estándar", el hardware de computación en un centro de datos diseñado por Google es el mismo en toda la infraestructura. Para eliminar la confusión entre el hardware del servidor y el software del servidor, utilizamos la siguiente terminología a lo largo del libro:

Máquina: Una pieza de hardware (o quizás una máquina virtual).
Servidor: Una pieza de software que implementa un servicio.
Las máquinas pueden ejecutar cualquier servidor, por lo que no dedicamos máquinas específicas a programas de servidor específicos. No hay una máquina específica que ejecute nuestro servidor de correo, por ejemplo. En su lugar, la asignación de recursos se maneja mediante nuestro sistema operativo de clúster, Borg. Nos damos cuenta de que este uso de la palabra "servidor" es inusual. El uso común de la palabra confunde "binario que acepta conexión de red" con "máquina", pero diferenciar entre los dos es importante cuando se habla de computación en Google. Una vez que te acostumbras a nuestro uso del término "servidor", se vuelve más evidente por qué tiene sentido utilizar esta terminología especializada, no solo dentro de Google, sino también en el resto de este libro.

La Figura 2-1 ilustra la topología de un centro de datos de Google:

Decenas de máquinas se colocan en un rack.
Los racks están colocados en una fila.
Una o más filas forman un clúster.
Generalmente, un edificio de centro de datos alberga múltiples clústeres.
Múltiples edificios de centros de datos que están ubicados cerca uno del otro forman un campus.
Figura 2-1. Ejemplo de topología de un campus de centro de datos de Google

Las máquinas dentro de un centro de datos determinado necesitan poder comunicarse entre sí, por lo que creamos un conmutador virtual muy rápido con decenas de miles de puertos. Logramos esto conectando cientos de conmutadores construidos por Google en una red Clos [Clos53] llamada Júpiter [Sin15]. En su configuración más grande, Júpiter admite 1.3 Pbps de ancho de banda de bisección entre servidores.

Los centros de datos están conectados entre sí a través de nuestra columna vertebral global.

Red B4
La red B4 [Jai13] es una arquitectura de red definida por software que utiliza el protocolo de comunicaciones de estándar abierto OpenFlow. B4 proporciona un ancho de banda masivo a un número modesto de sitios y utiliza la asignación de ancho de banda elástica para maximizar el ancho de banda promedio [Kum15].

Software de Sistema que "Organiza" el Hardware
Nuestro hardware debe ser controlado y administrado por software que pueda manejar una escala masiva. Las fallas de hardware son un problema notable que gestionamos con software. Dado el gran número de componentes de hardware en un clúster, las fallas de hardware ocurren con bastante frecuencia. En un solo clúster en un año típico, miles de máquinas fallan y miles de discos duros se rompen; cuando multiplicamos esto por el número de clústeres que operamos globalmente, estos números se vuelven bastante asombrosos. Por lo tanto, queremos abstraer estos problemas para que no afecten a los usuarios, y los equipos que ejecutan nuestros servicios tampoco quieren ser molestados por fallas de hardware. Cada campus de centro de datos tiene equipos dedicados a mantener el hardware y la infraestructura del centro de datos.

Gestión de Máquinas
Borg, ilustrado en la Figura 2-2, es un sistema operativo de clúster distribuido [Ver15], similar a Apache Mesos. Borg gestiona sus trabajos a nivel de clúster. Borg es responsable de ejecutar los trabajos de los usuarios, que pueden ser servidores que funcionan indefinidamente o procesos por lotes como un MapReduce [Dea04]. Los trabajos pueden consistir en más de una (y a veces miles de) tareas idénticas, tanto por razones de confiabilidad como porque un solo proceso no puede manejar todo el tráfico del clúster.

Cuando Borg inicia un trabajo, encuentra máquinas para las tareas y les dice a las máquinas que inicien el programa del servidor. Borg luego monitorea continuamente estas tareas. Si una tarea falla, se elimina y se reinicia, posiblemente en una máquina diferente.

Debido a que las tareas se asignan de manera flexible entre las máquinas, no podemos simplemente confiar en direcciones IP y números de puerto para referirnos a las tareas. Resolvemos este problema con un nivel adicional de indirecta: cuando se inicia un trabajo, Borg asigna un nombre y un número de índice a cada tarea utilizando el Servicio de Nombres de Borg (BNS). En lugar de usar la dirección IP y el número de puerto, otros procesos se conectan a las tareas de Borg a través del nombre BNS, que se traduce en una dirección IP y un número de puerto por el BNS. Por ejemplo, la ruta de BNS podría ser una cadena como /bns/<cluster>/<usuario>/<nombre del trabajo>/<número de tarea>, que se resolvería en <dirección IP>:<puerto>.

Borg también es responsable de la asignación de recursos a los trabajos. Cada trabajo necesita especificar sus recursos requeridos (por ejemplo, 3 núcleos de CPU, 2 GiB de RAM). Usando la lista de requisitos para todos los trabajos, Borg puede distribuir las tareas en las máquinas de una manera óptima que también tenga en cuenta los dominios de fallas (por ejemplo: Borg no ejecutará todas las tareas de un trabajo en el mismo rack, ya que hacerlo significaría que el conmutador superior del rack sería un único punto de falla para ese trabajo).

Si una tarea intenta usar más recursos de los que solicitó, Borg mata la tarea y la reinicia (ya que una tarea que falla lentamente es generalmente preferible a una tarea que no se ha reiniciado en absoluto).

Almacenamiento
Las tareas pueden usar el disco local en las máquinas como un espacio de trabajo temporal, pero tenemos varias opciones de almacenamiento en clúster para almacenamiento permanente (e incluso el espacio de trabajo temporal eventualmente se trasladará al modelo de almacenamiento en clúster). Estos son comparables a Lustre y al Hadoop Distributed File System (HDFS), que son ambos sistemas de archivos de clúster de código abierto.

La capa de almacenamiento es responsable de ofrecer a los usuarios acceso fácil y confiable al almacenamiento disponible para un clúster. Como se muestra en la Figura 2-3, el almacenamiento tiene muchas capas:

La capa más baja se llama D (de disco, aunque D usa tanto discos giratorios como almacenamiento flash). D es un servidor de archivos que se ejecuta en casi todas las máquinas de un clúster. Sin embargo, los usuarios que desean acceder a sus datos no quieren tener que recordar qué máquina está almacenando sus datos, por lo que la siguiente capa entra en juego.
Una capa sobre D llamada Colossus crea un sistema de archivos de clúster que ofrece la semántica habitual de un sistema de archivos, así como replicación y encriptación. Colossus es el sucesor de GFS, el Google File System [Ghe03].
Hay varios servicios similares a bases de datos construidos sobre Colossus:
Bigtable [Cha06] es un sistema de base de datos NoSQL que puede manejar bases de datos de tamaño de petabytes. Un Bigtable es un mapa disperso, distribuido, persistente y multidimensional ordenado, indexado por clave de fila, clave de columna y marca de tiempo; cada valor en el mapa es una matriz de bytes sin interpretar. Bigtable admite la replicación eventual consistente entre centros de datos.
Spanner [Cor12] ofrece una interfaz similar a SQL para los usuarios que requieren consistencia real en todo el mundo.
Varios otros sistemas de bases de datos, como Blobstore, están disponibles. Cada una de estas opciones viene con su propio conjunto de compensaciones (ver "Integridad de Datos: Lo que Lees es lo que Escribiste").

El hardware de red de Google se controla de varias maneras. Como se mencionó anteriormente, utilizamos una red definida por software basada en OpenFlow. En lugar de usar hardware de enrutamiento "inteligente", confiamos en componentes de conmutación más económicos y simples, junto con un controlador central (duplicado) que precomputa las mejores rutas a través de la red. De esta manera, podemos trasladar las decisiones de enrutamiento, que requieren mucho cálculo, fuera de los enrutadores y usar hardware de conmutación simple.

El ancho de banda de la red debe asignarse de manera inteligente. Al igual que Borg limita los recursos de cómputo que una tarea puede usar, el Bandwidth Enforcer (BwE) gestiona el ancho de banda disponible para maximizar el promedio disponible. Optimizar el ancho de banda no solo tiene que ver con el costo: se ha demostrado que la ingeniería de tráfico centralizada resuelve varios problemas que son tradicionalmente extremadamente difíciles de resolver con una combinación de enrutamiento distribuido e ingeniería de tráfico [Kum15].

Algunos servicios tienen trabajos que se ejecutan en múltiples clústeres distribuidos por todo el mundo. Para minimizar la latencia en servicios distribuidos globalmente, queremos dirigir a los usuarios al centro de datos más cercano con capacidad disponible. Nuestro Global Software Load Balancer (GSLB) realiza balanceo de carga en tres niveles:

Balanceo geográfico de carga para solicitudes DNS (por ejemplo, a www.google.com).
Balanceo de carga a nivel de servicio de usuario (por ejemplo, YouTube o Google Maps).
Balanceo de carga a nivel de llamadas de procedimientos remotos (RPC) dentro de los centros de datos.
Los propietarios de servicios especifican un nombre simbólico para el servicio, una lista de direcciones BNS de servidores y la capacidad disponible en cada ubicación (generalmente medida en consultas por segundo). GSLB luego dirige el tráfico a las direcciones BNS.

Otros Componentes de Software del Sistema
Servicio de Bloqueo
El servicio de bloqueo Chubby [Bur06] proporciona una API similar a un sistema de archivos para mantener bloqueos. Chubby maneja estos bloqueos en ubicaciones de centros de datos y utiliza el protocolo Paxos para lograr consenso asincrónico (ver "Gestión de Estado Crítico: Consenso Distribuido para la Fiabilidad").

Chubby también desempeña un papel importante en la elección de maestro. Cuando un servicio tiene cinco réplicas de un trabajo por razones de confiabilidad, pero solo una réplica puede realizar el trabajo real, Chubby se usa para seleccionar qué réplica puede proceder. Los datos que deben ser consistentes son adecuados para almacenarse en Chubby. Por esta razón, BNS usa Chubby para almacenar el mapeo entre las rutas BNS y los pares de dirección IP
.

Monitoreo y Alertas
Queremos asegurarnos de que todos los servicios estén funcionando como se requiere. Por lo tanto, ejecutamos muchas instancias de nuestro programa de monitoreo Borgmon (ver "Alertas Prácticas a partir de Datos de Series Temporales"). Borgmon regularmente recopila métricas de los servidores monitoreados. Estas métricas pueden usarse instantáneamente para alertas y también almacenarse para generar vistas históricas (por ejemplo, gráficos). Podemos usar el monitoreo de las siguientes maneras:

Configurar alertas para problemas agudos.
Comparar comportamiento: ¿una actualización de software hizo que el servidor sea más rápido?
Examinar cómo evoluciona el consumo de recursos con el tiempo, lo cual es esencial para la planificación de capacidad.

Nuestra Infraestructura de Software
Nuestra arquitectura de software está diseñada para hacer el uso más eficiente posible de nuestra infraestructura de hardware. Nuestro código está fuertemente multihilo, lo que permite que una tarea utilice fácilmente varios núcleos. Para facilitar la creación de paneles de control, el monitoreo y la depuración, cada servidor tiene un servidor HTTP que proporciona diagnósticos y estadísticas para una tarea determinada.

Todos los servicios de Google se comunican usando una infraestructura de Remote Procedure Call (RPC) llamada Stubby; una versión de código abierto, gRPC, está disponible. Con frecuencia, se hace una llamada RPC incluso cuando se necesita realizar una llamada a una subrutina en el programa local. Esto facilita la refactorización de la llamada en un servidor diferente si se necesita más modularidad o cuando el código base del servidor crece. GSLB puede equilibrar la carga de las RPCs de la misma manera que equilibra los servicios visibles externamente.

Un servidor recibe solicitudes RPC de su frontend y envía RPCs a su backend. En términos tradicionales, el frontend se llama "cliente" y el backend se llama "servidor". Los datos se transfieren hacia y desde una RPC utilizando protocol buffers, a menudo abreviados como "protobufs", que son similares a Thrift de Apache. Los protocol buffers tienen muchas ventajas sobre XML para la serialización de datos estructurados: son más simples de usar, de 3 a 10 veces más pequeños, de 20 a 100 veces más rápidos y menos ambiguos.

Nuestro Entorno de Desarrollo
La velocidad de desarrollo es muy importante para Google, por lo que hemos creado un entorno de desarrollo completo que aprovecha nuestra infraestructura [Mor12b].

Aparte de algunos grupos que tienen sus propios repositorios de código abierto (por ejemplo, Android y Chrome), los ingenieros de software de Google trabajan desde un repositorio compartido [Pot16]. Esto tiene algunas implicaciones prácticas importantes para nuestros flujos de trabajo:

Si los ingenieros encuentran un problema en un componente fuera de su proyecto, pueden corregirlo, enviar los cambios propuestos (llamados "changelist" o CL) al propietario para su revisión y enviar el CL a la línea principal.
Los cambios en el código fuente de un proyecto propio de un ingeniero requieren una revisión. Todo el software es revisado antes de ser enviado.
Cuando se construye el software, la solicitud de construcción se envía a servidores de compilación en un centro de datos. Incluso las compilaciones grandes se ejecutan rápidamente, ya que muchos servidores de compilación pueden compilar en paralelo. Esta infraestructura también se utiliza para pruebas continuas. Cada vez que se envía un CL, se ejecutan pruebas en todo el software que puede depender de ese CL, directa o indirectamente. Si el marco determina que el cambio probablemente rompió otras partes del sistema, notifica al propietario del cambio enviado. Algunos proyectos usan un sistema de "push-on-green", donde una nueva versión se envía automáticamente a producción después de pasar las pruebas.

Shakespeare: Un Servicio de Ejemplo
Para proporcionar un modelo de cómo se desplegaría hipotéticamente un servicio en el entorno de producción de Google, veamos un servicio de ejemplo que interactúa con múltiples tecnologías de Google. Supongamos que queremos ofrecer un servicio que te permita determinar dónde se usa una palabra determinada en todas las obras de Shakespeare. Podemos dividir este sistema en dos partes:

Un componente por lotes que lee todos los textos de Shakespeare, crea un índice y escribe el índice en un Bigtable. Este trabajo solo necesita ejecutarse una vez o muy infrecuentemente (¡nunca se sabe si se puede descubrir un nuevo texto!).
Un frontend de aplicación que maneja las solicitudes de los usuarios finales. Este trabajo está siempre activo, ya que los usuarios en todas las zonas horarias querrán buscar en los libros de Shakespeare.
El componente por lotes es un MapReduce que comprende tres fases:

Fase de mapeo: Lee los textos de Shakespeare y los divide en palabras individuales. Esto es más rápido si lo realizan varios trabajadores en paralelo.
Fase de mezcla: Ordena las tuplas por palabra.
Fase de reducción: Crea una tupla de (palabra, lista de ubicaciones).
Cada tupla se escribe en una fila de Bigtable, usando la palabra como clave.

Vida de una Solicitud
La Figura 2-4 muestra cómo se atiende la solicitud de un usuario. Primero, el usuario apunta su navegador a shakespeare.google.com. Para obtener la dirección IP correspondiente, el dispositivo del usuario resuelve la dirección con su servidor DNS (1). Esta solicitud finalmente llega al servidor DNS de Google, que se comunica con GSLB (Global Software Load Balancer). GSLB, que rastrea la carga de tráfico entre los servidores frontend en distintas regiones, selecciona qué dirección IP del servidor enviar a este usuario.

El navegador se conecta al servidor HTTP en esta IP. Este servidor (llamado Google Frontend o GFE) es un proxy inverso que termina la conexión TCP (2). GFE busca qué servicio se requiere (búsqueda web, mapas o, en este caso, Shakespeare). Nuevamente, usando GSLB, el servidor encuentra un servidor frontend disponible para Shakespeare y le envía una RPC que contiene la solicitud HTTP (3).

El servidor de Shakespeare analiza la solicitud HTTP y construye un protobuf que contiene la palabra a buscar. Ahora, el servidor frontend de Shakespeare necesita contactar al servidor backend de Shakespeare. El servidor frontend contacta a GSLB para obtener la dirección BNS de un servidor backend adecuado y sin carga (4). Luego, ese servidor backend de Shakespeare contacta a un servidor Bigtable para obtener los datos solicitados (5).

La respuesta se escribe en el protobuf de respuesta y se devuelve al servidor backend de Shakespeare. El servidor backend entrega un protobuf que contiene los resultados al servidor frontend de Shakespeare, que ensambla el HTML y devuelve la respuesta al usuario.

Toda esta cadena de eventos se ejecuta en un abrir y cerrar de ojos, ¡solo unos cientos de milisegundos! Dado que hay muchas partes móviles involucradas, existen muchos puntos de falla potenciales; en particular, una falla en GSLB podría causar estragos. Sin embargo, las políticas rigurosas de pruebas de Google y la implementación cuidadosa, junto con nuestros métodos proactivos de recuperación de errores, como la degradación controlada, nos permiten ofrecer el servicio confiable que nuestros usuarios esperan. Después de todo, las personas utilizan regularmente www.google.com para verificar si su conexión a Internet está funcionando correctamente.

Organización de Trabajo y Datos
Las pruebas de carga determinaron que nuestro servidor backend puede manejar alrededor de 100 consultas por segundo (QPS). Los ensayos realizados con un conjunto limitado de usuarios nos llevan a esperar una carga máxima de alrededor de 3,470 QPS, por lo que necesitamos al menos 35 tareas. Sin embargo, las siguientes consideraciones significan que necesitamos al menos 37 tareas en el trabajo, o N+2:

Durante las actualizaciones, una tarea a la vez no estará disponible, lo que deja 36 tareas.
Podría ocurrir una falla de máquina durante una actualización de tareas, dejando solo 35 tareas, justo lo suficiente para manejar la carga máxima.13
Un examen más detallado del tráfico de usuarios muestra que nuestro uso máximo está distribuido globalmente: 1,430 QPS de América del Norte, 290 de América del Sur, 1,400 de Europa y África, y 350 de Asia y Australia. En lugar de ubicar todos los backends en un solo sitio, los distribuimos por EE.UU., América del Sur, Europa y Asia. Al permitir una redundancia N+2 por región, terminamos con 17 tareas en EE.UU., 16 en Europa y 6 en Asia. Sin embargo, decidimos usar 4 tareas (en lugar de 5) en América del Sur, para reducir los costos de N+2 a N+1. En este caso, estamos dispuestos a tolerar un pequeño riesgo de mayor latencia a cambio de menores costos de hardware: si GSLB redirige el tráfico de un continente a otro cuando nuestro centro de datos en América del Sur está sobrecargado, podemos ahorrar el 20% de los recursos que gastaríamos en hardware. En las regiones más grandes, distribuimos las tareas entre dos o tres clústeres para mayor resiliencia.

Dado que los backends necesitan contactar con Bigtable para acceder a los datos, también necesitamos diseñar estratégicamente este elemento de almacenamiento. Un backend en Asia contactando a un Bigtable en EE.UU. añadiría una cantidad significativa de latencia, por lo que replicamos el Bigtable en cada región. La replicación de Bigtable nos ayuda de dos maneras: proporciona resiliencia en caso de que un servidor de Bigtable falle y reduce la latencia de acceso a los datos. Aunque Bigtable solo ofrece consistencia eventual, no es un problema importante porque no necesitamos actualizar el contenido con frecuencia.

Hemos introducido una gran cantidad de terminología aquí; aunque no es necesario recordarlo todo, es útil para enmarcar muchos de los otros sistemas a los que nos referiremos más adelante.

Notas:

Los centros de datos de Google, aunque en su mayoría homogéneos, pueden tener diferencias debido a la evolución del hardware en diferentes generaciones.
Algunos lectores pueden estar más familiarizados con Kubernetes, descendiente de Borg, un marco de orquestación de clústeres de contenedores de código abierto iniciado por Google en 2014.
Protocol Buffers es un mecanismo neutral en cuanto a lenguaje y plataforma para serializar datos estructurados.

Parte II - Principios
Esta sección examina los principios fundamentales sobre cómo suelen trabajar los equipos de SRE: los patrones, comportamientos y áreas de preocupación que influyen en el dominio general de las operaciones de SRE.

"Embracing Risk" (Abrazando el Riesgo)
El primer capítulo de esta sección, y el más importante para leer si deseas obtener una visión amplia de lo que hace SRE y cómo razonamos al respecto, es Embracing Risk. Examina SRE desde la perspectiva del riesgo: su evaluación, gestión y el uso de presupuestos de errores para proporcionar enfoques útiles y neutrales en la gestión de servicios.

"Service Level Objectives" (Objetivos de Nivel de Servicio)
Los objetivos de nivel de servicio (SLOs) son otra unidad conceptual fundamental para SRE. La industria a menudo agrupa conceptos dispares bajo el término general de acuerdos de nivel de servicio (SLAs), una tendencia que dificulta pensar claramente sobre estos conceptos. Este capítulo intenta desentrañar indicadores de objetivos y de acuerdos, examina cómo SRE utiliza cada uno de estos términos y ofrece algunas recomendaciones para encontrar métricas útiles para tus propias aplicaciones.

"Eliminating Toil" (Eliminando el Trabajo Tedioso)
Eliminar el trabajo tedioso es una de las tareas más importantes de SRE y es el tema del capítulo Eliminating Toil. Definimos toil como el trabajo operacional mundano y repetitivo que no aporta valor duradero y que escala linealmente con el crecimiento del servicio.

"Monitoring Distributed Systems" (Monitorización de Sistemas Distribuidos)
Tanto en Google como en otros lugares, la monitorización es un componente absolutamente esencial para hacer lo correcto en producción. Si no puedes monitorear un servicio, no sabes qué está sucediendo y, si estás ciego a lo que está ocurriendo, no puedes ser confiable. Este capítulo ofrece recomendaciones sobre qué y cómo monitorear, así como algunas mejores prácticas sin depender de implementaciones específicas.

"The Evolution of Automation at Google" (La Evolución de la Automatización en Google)
En este capítulo, examinamos el enfoque de SRE hacia la automatización y presentamos algunos estudios de caso sobre cómo SRE ha implementado la automatización, tanto con éxito como sin él.

"Release Engineering" (Ingeniería de Lanzamiento)
La ingeniería de lanzamiento suele ser tratada como una ocurrencia tardía en muchas compañías. Sin embargo, como aprenderás en este capítulo, la ingeniería de lanzamiento no solo es crítica para la estabilidad general del sistema—ya que la mayoría de las interrupciones resultan de la implementación de algún cambio—sino que también es la mejor manera de garantizar que los lanzamientos sean consistentes.

"Simplicity" (Simplicidad)
Un principio clave de cualquier ingeniería de software efectiva, y no solo de la ingeniería orientada a la confiabilidad, es la simplicidad. Una vez perdida, la simplicidad puede ser extremadamente difícil de recuperar. Sin embargo, como dice el viejo adagio: un sistema complejo que funciona necesariamente evolucionó a partir de un sistema simple que funciona. Este capítulo aborda el tema de la simplicidad en detalle.

Lectura Adicional de Google SRE
Aumentar la velocidad del producto de manera segura es un principio central para cualquier organización. En "Making Push On Green a Reality" [Kle14], publicado en octubre de 2014, mostramos que eliminar a los humanos del proceso de lanzamiento puede, paradójicamente, reducir el trabajo tedioso de los SREs mientras aumenta la confiabilidad del sistema.

Capítulo 3 - Abrazando el Riesgo
Escrito por Marc Alvidrez | Editado por Kavita Guliani

Podrías esperar que Google intente construir servicios 100% confiables, aquellos que nunca fallan. Sin embargo, resulta que, más allá de cierto punto, aumentar la confiabilidad puede ser peor para un servicio (y sus usuarios) en lugar de mejor. La confiabilidad extrema tiene un costo: maximizar la estabilidad limita la velocidad con la que se pueden desarrollar nuevas características y entregar productos a los usuarios, y aumenta drásticamente los costos, lo que a su vez reduce el número de características que un equipo puede permitirse ofrecer.

Además, los usuarios generalmente no notan la diferencia entre una alta confiabilidad y una confiabilidad extrema en un servicio, porque la experiencia del usuario está dominada por componentes menos confiables, como la red celular o el dispositivo que están usando. En pocas palabras, un usuario en un teléfono inteligente con un 99% de confiabilidad no puede notar la diferencia entre una confiabilidad del servicio de 99.99% y 99.999%. Con esto en mente, en lugar de simplemente maximizar el tiempo de actividad, Site Reliability Engineering (SRE) busca equilibrar el riesgo de no disponibilidad con los objetivos de innovación rápida y operaciones de servicio eficientes, para que la felicidad general de los usuarios—con características, servicio y rendimiento—se optimice.

Gestión del Riesgo
Los sistemas no confiables pueden erosionar rápidamente la confianza de los usuarios, por lo que queremos reducir la posibilidad de fallos. Sin embargo, la experiencia demuestra que a medida que construimos sistemas, el costo no aumenta linealmente con los incrementos en la confiabilidad: una mejora incremental en la confiabilidad puede costar 100 veces más que el incremento anterior. El costo tiene dos dimensiones:

El costo de recursos redundantes de máquinas y computación.
El costo asociado al equipo redundante, que, por ejemplo, nos permite desconectar sistemas para mantenimiento rutinario o imprevisto, o proporciona espacio para almacenar bloques de código de paridad que garantizan un mínimo de durabilidad de los datos.
Además, está el costo de oportunidad:

El costo que asume una organización cuando asigna recursos de ingeniería para construir sistemas o características que reducen el riesgo, en lugar de características visibles o utilizables directamente por los usuarios finales. Estos ingenieros ya no están trabajando en nuevas características o productos para los usuarios.
En SRE, gestionamos la confiabilidad del servicio principalmente gestionando el riesgo. Conceptualizamos el riesgo como un continuo. Le damos la misma importancia tanto a cómo ingenierizar una mayor confiabilidad en los sistemas de Google como a identificar el nivel adecuado de tolerancia para los servicios que ejecutamos. Esto nos permite realizar un análisis de costo-beneficio para determinar, por ejemplo, dónde en el continuo de riesgo (no lineal) deberíamos colocar servicios como Search, Ads, Gmail o Photos.

Nuestro objetivo es alinear explícitamente el riesgo de un servicio dado con el riesgo que el negocio está dispuesto a asumir. Nos esforzamos por hacer que un servicio sea lo suficientemente confiable, pero no más confiable de lo que necesita ser. Es decir, cuando establecemos un objetivo de disponibilidad del 99.99%, queremos excederlo, pero no mucho más: eso desperdiciaría oportunidades de añadir nuevas características al sistema, limpiar deuda técnica o reducir sus costos operativos. En cierto sentido, vemos el objetivo de disponibilidad como un mínimo y un máximo. La ventaja clave de este enfoque es que permite asumir riesgos explícitos y bien pensados.

Medición del Riesgo de Servicio
Como práctica estándar en Google, a menudo nos beneficiamos al identificar una métrica objetiva que represente la propiedad de un sistema que queremos optimizar. Al establecer un objetivo, podemos evaluar nuestro rendimiento actual y seguir las mejoras o degradaciones a lo largo del tiempo. Sin embargo, para el riesgo de servicio, no es inmediatamente claro cómo reducir todos los factores potenciales en una sola métrica. Las fallas de servicio pueden tener muchos efectos potenciales, como la insatisfacción del usuario, la pérdida de confianza, pérdidas directas o indirectas de ingresos, el impacto en la marca o reputación, y cobertura de prensa no deseada. Claramente, algunos de estos factores son muy difíciles de medir. Para hacer este problema manejable y consistente en los diferentes tipos de sistemas que operamos, nos enfocamos en el tiempo de inactividad no planificado.

Para la mayoría de los servicios, la forma más directa de representar la tolerancia al riesgo es en términos del nivel aceptable de tiempo de inactividad no planificado. Este tiempo de inactividad se refleja en el nivel deseado de disponibilidad del servicio, generalmente expresado en términos del número de "nueves" que nos gustaría proporcionar: 99.9%, 99.99% o 99.999% de disponibilidad. Cada nueve adicional corresponde a una mejora de un orden de magnitud hacia la disponibilidad del 100%. Para sistemas de servicio, esta métrica se calcula tradicionalmente en función de la proporción de tiempo en que el sistema está disponible (ver Disponibilidad basada en el tiempo).

Disponibilidad Basada en el Tiempo
Usando esta fórmula a lo largo de un año, podemos calcular el número aceptable de minutos de inactividad para alcanzar un determinado número de "nueves" de disponibilidad. Por ejemplo, un sistema con un objetivo de disponibilidad del 99.99% puede estar inactivo por hasta 52.56 minutos en un año y aún mantenerse dentro de su objetivo de disponibilidad.

Disponibilidad Agregada
En Google, sin embargo, una métrica basada en el tiempo no suele ser significativa debido a que nuestros servicios están distribuidos globalmente. Nuestro enfoque de aislamiento de fallos hace que sea muy probable que al menos una parte del tráfico de un servicio se esté atendiendo en alguna parte del mundo en un momento dado (es decir, al menos estamos "parcialmente operativos" en todo momento). Por lo tanto, en lugar de utilizar métricas basadas en el tiempo de actividad, definimos la disponibilidad en términos de la tasa de éxito de las solicitudes. La disponibilidad agregada muestra cómo se calcula esta métrica basada en el rendimiento a lo largo de una ventana móvil (es decir, la proporción de solicitudes exitosas en una ventana de un día).

Por ejemplo, un sistema que maneja 2.5 millones de solicitudes en un día con un objetivo diario de disponibilidad del 99.99% puede servir hasta 250 errores y aún así alcanzar su objetivo para ese día.

En una aplicación típica, no todas las solicitudes son iguales: fallar en una solicitud de registro de un nuevo usuario es diferente de fallar en una solicitud que consulta si hay nuevos correos electrónicos en segundo plano. Sin embargo, en muchos casos, calcular la disponibilidad como la tasa de éxito de las solicitudes sobre todas las solicitudes es una aproximación razonable del tiempo de inactividad no planificado, desde la perspectiva del usuario final.

Cuantificación del Tiempo de Inactividad No Planificado
Cuantificar el tiempo de inactividad no planificado como una tasa de éxito de solicitudes también hace que esta métrica de disponibilidad sea más aplicable a sistemas que no suelen atender directamente a los usuarios finales. La mayoría de los sistemas que no son de servicio (como sistemas de procesamiento por lotes, pipelines, almacenamiento y transaccionales) tienen una noción bien definida de unidades de trabajo exitosas y fallidas. De hecho, aunque los sistemas discutidos en este capítulo son principalmente servicios para consumidores e infraestructura, muchos de los mismos principios se aplican a los sistemas no orientados al servicio con mínimas modificaciones.

Por ejemplo, un proceso por lotes que extrae, transforma e inserta los contenidos de una base de datos de un cliente en un data warehouse para permitir un análisis posterior puede estar programado para ejecutarse periódicamente. Utilizando una tasa de éxito de solicitudes definida en términos de registros procesados con éxito o con error, podemos calcular una métrica de disponibilidad útil, a pesar de que el sistema por lotes no se ejecuta constantemente.

Con mayor frecuencia, establecemos objetivos de disponibilidad trimestrales para un servicio y rastreamos nuestro rendimiento en relación con esos objetivos de manera semanal, o incluso diaria. Esta estrategia nos permite gestionar el servicio hacia un objetivo de disponibilidad de alto nivel al buscar, rastrear y corregir desviaciones significativas a medida que surgen inevitablemente. Para más detalles, ver Service Level Objectives (SLOs).

Tolerancia al Riesgo de los Servicios
¿Qué significa identificar la tolerancia al riesgo de un servicio? En un entorno formal o en el caso de sistemas críticos para la seguridad, la tolerancia al riesgo está típicamente integrada directamente en la definición básica del producto o servicio. En Google, la tolerancia al riesgo de los servicios tiende a estar menos definida claramente.

Para identificar la tolerancia al riesgo de un servicio, los SREs deben trabajar con los dueños de producto para convertir un conjunto de objetivos comerciales en objetivos explícitos que puedan ser implementados a través de la ingeniería. En este caso, los objetivos comerciales que nos preocupan tienen un impacto directo en el rendimiento y la confiabilidad del servicio ofrecido. En la práctica, esta traducción es más fácil de decir que de hacer. Mientras que los servicios para consumidores a menudo tienen dueños de producto claramente definidos, es inusual que los servicios de infraestructura (como sistemas de almacenamiento o capas de almacenamiento en caché HTTP de propósito general) tengan una estructura similar de propiedad de producto. A continuación, discutimos los casos de servicios para consumidores e infraestructura.

Identificación de la Tolerancia al Riesgo de Servicios para Consumidores
Nuestros servicios para consumidores a menudo tienen un equipo de producto que actúa como dueño comercial de una aplicación. Por ejemplo, Search, Google Maps y Google Docs tienen sus propios product managers. Estos gerentes de producto son responsables de entender a los usuarios y el negocio, así como de dar forma al producto para su éxito en el mercado. Cuando existe un equipo de producto, generalmente es el mejor recurso para discutir los requisitos de confiabilidad de un servicio. En ausencia de un equipo de producto dedicado, los ingenieros que construyen el sistema a menudo desempeñan este papel, consciente o inconscientemente.

Hay muchos factores a considerar al evaluar la tolerancia al riesgo de los servicios, tales como los siguientes:

¿Qué nivel de disponibilidad se requiere?
¿Diferentes tipos de fallos tienen diferentes efectos en el servicio?
¿Cómo podemos usar el costo del servicio para ubicar un servicio en el continuo de riesgo?
¿Qué otras métricas del servicio son importantes de tener en cuenta?

Nivel Objetivo de Disponibilidad
El nivel objetivo de disponibilidad para un servicio de Google depende generalmente de la función que proporciona y de cómo se posiciona en el mercado. A continuación, se incluyen algunos aspectos a considerar:

Expectativas de los usuarios: ¿Qué nivel de servicio esperan los usuarios?
Impacto en los ingresos: ¿Este servicio está directamente vinculado a los ingresos, ya sea de Google o de nuestros clientes?
Modelo de negocio: ¿Es un servicio pago o es gratuito?
Competencia: Si hay competidores en el mercado, ¿qué nivel de servicio ofrecen ellos?
Audiencia objetivo: ¿Este servicio está dirigido a consumidores o a empresas?
Consideremos el caso de Google Apps for Work (ahora Google Workspace). La mayoría de sus usuarios son empresas, algunas grandes y otras pequeñas. Estas empresas dependen de los servicios de Google Apps for Work (Gmail, Calendar, Drive, Docs, etc.) para proporcionar herramientas que permiten a sus empleados realizar su trabajo diario. En otras palabras, una interrupción de un servicio de Google Apps for Work no solo afecta a Google, sino también a todas las empresas que dependen críticamente de nosotros. Para un servicio típico de Google Apps for Work, podríamos establecer un objetivo de disponibilidad trimestral del 99.9% para los usuarios externos, respaldado por un objetivo interno más alto y un contrato que estipula penalidades si no cumplimos con el objetivo externo.

Por otro lado, YouTube proporciona un conjunto de consideraciones diferentes. Cuando Google adquirió YouTube en 2006, tuvimos que decidir el nivel adecuado de disponibilidad para el sitio web. En ese momento, YouTube estaba enfocado en consumidores y se encontraba en una fase de crecimiento y cambio rápido. Aunque YouTube ya tenía un gran producto, el desarrollo de nuevas características era más importante que maximizar la disponibilidad. Por lo tanto, establecimos un objetivo de disponibilidad más bajo para YouTube en comparación con nuestros productos empresariales.

Tipos de Fallos
El tipo esperado de fallos para un servicio es otra consideración importante. Es necesario preguntarse:

Resiliencia del negocio: ¿Qué tan resiliente es nuestro negocio al tiempo de inactividad del servicio?
Impacto de los fallos: ¿Es peor para el servicio tener una tasa constante de fallos bajos o sufrir interrupciones completas ocasionales?
Aunque ambos tipos de fallos pueden resultar en el mismo número absoluto de errores, su impacto en el negocio puede ser muy diferente.

Un ejemplo ilustrativo de la diferencia entre interrupciones totales y parciales surge naturalmente en sistemas que manejan información privada. Consideremos una aplicación de gestión de contactos. Compararíamos una falla intermitente en la que las imágenes de perfil no se muestran con un caso donde los contactos privados de un usuario se muestran a otro usuario. El primer caso representa una mala experiencia para el usuario, y los SREs trabajarían para solucionar el problema rápidamente. En el segundo caso, el riesgo de exponer datos privados podría socavar gravemente la confianza básica del usuario. Como resultado, sería apropiado detener completamente el servicio mientras se depura y se limpia el sistema.

Por otro lado, en algunos servicios de Google es aceptable tener interrupciones regulares durante las ventanas de mantenimiento planificadas. Hace algunos años, el Ads Frontend era uno de esos servicios. Este sistema es utilizado por anunciantes y editores de sitios web para configurar y monitorear sus campañas publicitarias. Dado que la mayor parte de este trabajo ocurre durante el horario laboral, determinamos que interrupciones regulares y planificadas en forma de ventanas de mantenimiento serían aceptables, y estas interrupciones programadas no se contabilizan como tiempo de inactividad no planificado.

Costo
El costo es a menudo el factor clave para determinar el objetivo adecuado de disponibilidad de un servicio. El caso de Ads (publicidad) es particularmente adecuado para realizar este intercambio, ya que los éxitos y fallos de las solicitudes se traducen directamente en ingresos ganados o perdidos. Al determinar el objetivo de disponibilidad para cada servicio, planteamos preguntas como:

Si construyéramos y operáramos estos sistemas con un nueve adicional de disponibilidad, ¿cuál sería nuestro aumento incremental en los ingresos?
¿Este ingreso adicional compensa el costo de alcanzar ese nivel de confiabilidad?
Para hacer más concreta esta ecuación de compensación, consideremos el siguiente análisis de costo/beneficio para un servicio donde cada solicitud tiene el mismo valor:

Mejora propuesta en el objetivo de disponibilidad: 99.9% → 99.99%
Aumento propuesto en disponibilidad: 0.09%
Ingresos del servicio: $1M
Valor de la mejora en la disponibilidad: $1M * 0.0009 = $900
En este caso, si el costo de mejorar la disponibilidad en un nueve es menor a $900, vale la pena la inversión. Si el costo es mayor a $900, los costos superarán el aumento proyectado en ingresos.

Cuando no existe una función simple de traducción entre confiabilidad e ingresos, puede ser más difícil establecer estos objetivos. Una estrategia útil puede ser considerar la tasa de error de fondo de los ISPs en Internet. Si las fallas se miden desde la perspectiva del usuario final y es posible reducir la tasa de error del servicio por debajo de la tasa de error de fondo, esos errores se perderán dentro del "ruido" de la conexión a Internet de un usuario. Aunque existen diferencias significativas entre ISPs y protocolos (por ejemplo, TCP versus UDP, IPv4 versus IPv6), hemos medido que la tasa de error de fondo típica de los ISPs está entre el 0.01% y el 1%.

Otras Métricas del Servicio
Examinar la tolerancia al riesgo de los servicios en relación con métricas además de la disponibilidad puede ser muy útil. Entender qué métricas son importantes y cuáles no lo son nos proporciona margen de maniobra al intentar asumir riesgos bien pensados.

Un ejemplo ilustrativo proviene de la latencia del servicio para nuestros sistemas de Ads. Cuando Google lanzó Web Search, una de sus principales características distintivas fue la velocidad. Al introducir AdWords, que muestra anuncios junto a los resultados de búsqueda, un requisito clave fue que los anuncios no debían ralentizar la experiencia de búsqueda. Este requisito ha impulsado los objetivos de ingeniería en cada generación de sistemas de AdWords y se trata como una invariante.

AdSense, el sistema de anuncios de Google que sirve anuncios contextuales en respuesta a solicitudes de código JavaScript que los editores insertan en sus sitios web, tiene un objetivo de latencia muy diferente. El objetivo de latencia para AdSense es evitar ralentizar la carga de la página de terceros al insertar anuncios contextuales. Por lo tanto, el objetivo específico de latencia depende de la velocidad a la que se renderiza la página del editor. Esto significa que los anuncios de AdSense pueden servirse generalmente con una latencia de cientos de milisegundos más que los anuncios de AdWords.

Este requisito de latencia menos estricto nos ha permitido hacer compensaciones inteligentes en la provisión (determinando la cantidad y ubicación de los recursos de servicio que utilizamos), lo que nos ahorra costos sustanciales en comparación con una provisión ingenua. En otras palabras, dada la relativa insensibilidad del servicio de AdSense a cambios moderados en el rendimiento de la latencia, podemos consolidar el servicio en menos ubicaciones geográficas, reduciendo nuestros costos operativos.

Identificación de la Tolerancia al Riesgo de los Servicios de Infraestructura
Los requisitos para construir y operar componentes de infraestructura difieren de los de los productos para consumidores en varios aspectos. Una diferencia fundamental es que, por definición, los componentes de infraestructura tienen múltiples clientes, a menudo con necesidades variables.

Nivel Objetivo de Disponibilidad
Consideremos Bigtable [Cha06], un sistema de almacenamiento distribuido a gran escala para datos estructurados. Algunos servicios para consumidores sirven datos directamente desde Bigtable en el camino de una solicitud de usuario, por lo que requieren baja latencia y alta confiabilidad. Otros equipos usan Bigtable como un repositorio de datos que emplean para realizar procesamiento sin conexión (offline).

Otros Equipos que Usan Bigtable
Algunos equipos usan Bigtable como un repositorio de datos para realizar análisis offline (por ejemplo, usando MapReduce) de manera regular. Estos equipos tienden a preocuparse más por el rendimiento que por la confiabilidad. La tolerancia al riesgo para estos dos casos de uso es muy distinta.

Una forma de satisfacer las necesidades de ambos casos de uso sería diseñar todos los servicios de infraestructura para ser ultra-confiables. Sin embargo, dado que estos servicios de infraestructura tienden a agregar enormes cantidades de recursos, este enfoque suele ser demasiado costoso en la práctica. Para entender las necesidades de los diferentes tipos de usuarios, se puede observar el estado deseado de la cola de solicitudes para cada tipo de usuario de Bigtable.

Tipos de Fallos
El usuario que necesita baja latencia quiere que las colas de solicitudes de Bigtable estén (casi siempre) vacías para que el sistema pueda procesar cada solicitud pendiente inmediatamente al llegar. (De hecho, las colas ineficientes son a menudo una causa de alta latencia en los extremos). El usuario que realiza análisis offline se preocupa más por el rendimiento del sistema, por lo que prefiere que las colas de solicitudes nunca estén vacías. Para optimizar el rendimiento, el sistema Bigtable nunca debería estar inactivo esperando la próxima solicitud.

Como puedes ver, el éxito y el fracaso son antitéticos para estos dos tipos de usuarios. El éxito para el usuario de baja latencia es un fracaso para el usuario que realiza análisis offline, y viceversa.

Costo
Una forma de satisfacer estas restricciones competentes de manera rentable es dividir la infraestructura y ofrecerla en múltiples niveles de servicio independientes. En el ejemplo de Bigtable, podemos construir dos tipos de clústeres: clústeres de baja latencia y clústeres de alto rendimiento.

Los clústeres de baja latencia están diseñados para ser operados y utilizados por servicios que necesitan baja latencia y alta confiabilidad. Para garantizar colas de solicitudes cortas y satisfacer requisitos más estrictos de aislamiento de clientes, el sistema Bigtable puede aprovisionarse con una cantidad sustancial de capacidad adicional, reduciendo la contención y aumentando la redundancia.

Los clústeres de alto rendimiento, por otro lado, pueden aprovisionarse para operar a plena capacidad con menos redundancia, optimizando el rendimiento sobre la latencia. En la práctica, podemos satisfacer estas necesidades más relajadas a un costo mucho más bajo, quizás del 10% al 50% del costo de un clúster de baja latencia.

Dado que Bigtable opera a gran escala, este ahorro de costos se vuelve significativo rápidamente.

Estrategia Clave: Niveles Delineados de Servicio
La estrategia clave con respecto a la infraestructura es ofrecer servicios con niveles de servicio claramente delineados, lo que permite a los clientes tomar las decisiones adecuadas sobre el riesgo y el costo al construir sus sistemas. Al delinear explícitamente los niveles de servicio, los proveedores de infraestructura pueden externalizar efectivamente las diferencias de costo que conlleva proporcionar un servicio a un determinado nivel, motivando a los clientes a elegir el nivel de servicio con el costo más bajo que aún cumpla con sus necesidades.

Por ejemplo, Google+ puede decidir poner datos críticos para la privacidad del usuario en un datastore de alta disponibilidad y consistencia global (como Spanner, un sistema SQL globalmente replicado), mientras que los datos opcionales (que no son críticos pero mejoran la experiencia del usuario) pueden colocarse en un datastore más barato, menos confiable y eventualmente consistente (como Bigtable, con replicación de mejor esfuerzo).

Es importante destacar que podemos ejecutar múltiples clases de servicios utilizando hardware y software idénticos. Podemos proporcionar garantías de servicio muy diferentes ajustando diversas características, como la cantidad de recursos, el grado de redundancia, las restricciones geográficas de aprovisionamiento y, de manera crítica, la configuración del software de infraestructura.

Ejemplo: Infraestructura de Frontend
Para demostrar que los principios de evaluación de tolerancia al riesgo no solo se aplican a la infraestructura de almacenamiento, veamos otra gran clase de servicios: la infraestructura de frontend de Google. Esta infraestructura consiste en sistemas de proxy inverso y balanceo de carga que operan cerca del borde de nuestra red. Estos sistemas, entre otras funciones, sirven como uno de los puntos finales de las conexiones desde los usuarios finales (por ejemplo, terminan la conexión TCP desde el navegador del usuario). Dado su papel crítico, diseñamos estos sistemas para ofrecer un nivel extremadamente alto de confiabilidad.

A diferencia de muchos servicios de consumo que pueden mitigar la visibilidad de la falta de confiabilidad en los backends, estos sistemas de infraestructura no tienen esa ventaja. Si una solicitud nunca llega al servidor frontend de la aplicación, se pierde por completo. Por lo tanto, garantizar la alta disponibilidad y confiabilidad de estos sistemas es fundamental para el funcionamiento general del ecosistema de servicios de Google.

Gestión de la Infiabilidad a través de Presupuestos de Errores
Hemos explorado cómo identificar la tolerancia al riesgo tanto para los servicios de consumo como para los servicios de infraestructura. El siguiente paso es utilizar ese nivel de tolerancia para gestionar la infiabilidad mediante el uso de presupuestos de errores. Estos presupuestos permiten a los equipos equilibrar el desarrollo rápido de nuevas características con la confiabilidad, al proporcionar una cantidad controlada de margen para la infiabilidad, siempre que no se exceda el presupuesto definido.

Motivación para los Presupuestos de Errores
Otros capítulos de este libro discuten cómo pueden surgir tensiones entre los equipos de desarrollo de productos y los equipos de SRE, dado que suelen ser evaluados según métricas diferentes. El rendimiento de los desarrolladores de productos se evalúa principalmente en función de la velocidad del producto, lo que genera un incentivo para lanzar nuevo código lo más rápido posible. Mientras tanto, el rendimiento de los SREs se evalúa (como es de esperar) según la confiabilidad del servicio, lo que genera un incentivo para resistir un alto ritmo de cambios. La asimetría de información entre los dos equipos amplifica aún más esta tensión inherente: los desarrolladores tienen más visibilidad sobre el tiempo y esfuerzo necesarios para escribir y lanzar su código, mientras que los SREs tienen más visibilidad sobre la confiabilidad del servicio y el estado de la producción en general.

Estas tensiones a menudo se reflejan en diferentes opiniones sobre el esfuerzo que debe invertirse en las prácticas de ingeniería. La siguiente lista presenta algunas tensiones típicas:

Tolerancia a fallos del software: ¿Cuánto deberíamos reforzar el software para eventos inesperados? Si es demasiado poco, tendremos un producto frágil e inutilizable. Si es demasiado, tendremos un producto que nadie quiere usar (aunque funcione muy establemente).
Pruebas: Con pocas pruebas, se tienen interrupciones embarazosas, filtraciones de datos privados o una serie de otros eventos que atraen la atención de los medios. Con demasiadas pruebas, podrías perder el mercado.
Frecuencia de implementación: Cada implementación es arriesgada. ¿Cuánto deberíamos trabajar en reducir ese riesgo, versus hacer otras tareas?
Duración y tamaño de la fase canary: Es una práctica recomendada probar una nueva versión en un pequeño subconjunto de la carga de trabajo, lo que a menudo se denomina canarying. ¿Cuánto tiempo debemos esperar y qué tan grande debe ser el canary?
Usualmente, los equipos preexistentes han llegado a un balance informal sobre dónde se encuentra el límite entre el riesgo y el esfuerzo. Sin embargo, raramente se puede probar que este balance es óptimo y, a menudo, es simplemente un reflejo de las habilidades de negociación de los ingenieros involucrados. Tampoco deberían tomarse decisiones basadas en política, miedo o esperanza. (De hecho, el lema no oficial de Google SRE es "La esperanza no es una estrategia"). Nuestro objetivo es definir una métrica objetiva, acordada por ambas partes, que pueda guiar las negociaciones de manera reproducible. Cuanto más basada en datos esté la decisión, mejor.

Formación de un Presupuesto de Errores
Para basar estas decisiones en datos objetivos, ambos equipos definen conjuntamente un presupuesto de errores trimestral basado en el objetivo de nivel de servicio (SLO) del servicio. El presupuesto de errores proporciona una métrica clara y objetiva que determina cuán poco confiable se permite que sea el servicio dentro de un trimestre. Esta métrica elimina la política de las negociaciones entre los SREs y los desarrolladores de productos al decidir cuánto riesgo permitir.

El proceso es el siguiente:

Product Management define un SLO, que establece la expectativa de cuánto tiempo de actividad debería tener el servicio por trimestre.
El tiempo de actividad real es medido por una tercera parte neutral: nuestro sistema de monitoreo.
La diferencia entre estos dos números es el presupuesto de cuánta "infiabilidad" queda para el trimestre.
Siempre que el tiempo de actividad medido esté por encima del SLO—es decir, mientras quede presupuesto de errores—se pueden lanzar nuevas versiones.
Por ejemplo, supongamos que el SLO de un servicio es servir con éxito el 99.999% de todas las consultas por trimestre. Esto significa que el presupuesto de errores del servicio es una tasa de fallos del 0.001% para un trimestre dado. Si un problema nos hace fallar el 0.0002% de las consultas esperadas para el trimestre, ese problema consume el 20% del presupuesto de errores trimestral del servicio.

Beneficios de los Presupuestos de Errores
El principal beneficio de un presupuesto de errores es que proporciona un incentivo común que permite tanto al equipo de desarrollo de productos como al equipo de SRE enfocarse en encontrar el equilibrio adecuado entre innovación y confiabilidad.

Muchos productos usan este ciclo de control para gestionar la velocidad de lanzamiento: siempre que se cumplan los SLOs del sistema, los lanzamientos pueden continuar. Si las violaciones del SLO ocurren con la suficiente frecuencia como para agotar el presupuesto de errores, los lanzamientos se detienen temporalmente mientras se invierten recursos adicionales en pruebas y desarrollo del sistema para hacerlo más resistente, mejorar su rendimiento, etc. Existen enfoques más sutiles y efectivos que esta simple técnica de encender/apagar: por ejemplo, reducir la velocidad de los lanzamientos o revertirlos cuando el presupuesto de errores por violación del SLO está a punto de agotarse.

Por ejemplo, si el equipo de desarrollo de productos quiere reducir las pruebas o aumentar la velocidad de los lanzamientos y el equipo de SRE se resiste, el presupuesto de errores guía la decisión. Cuando el presupuesto es amplio, los desarrolladores pueden tomar más riesgos. Cuando el presupuesto está casi agotado, los mismos desarrolladores presionarán para hacer más pruebas o reducir la velocidad de los lanzamientos, ya que no quieren arriesgarse a consumir todo el presupuesto y detener el lanzamiento. En efecto, el equipo de desarrollo se autogestiona. Ellos conocen el presupuesto y pueden manejar su propio riesgo. (Por supuesto, esto depende de que el equipo de SRE tenga la autoridad para detener los lanzamientos si se rompe el SLO).

¿Qué sucede si un corte de red o una falla en el centro de datos reduce el SLO medido? Estos eventos también consumen parte del presupuesto de errores. Como resultado, el número de nuevos lanzamientos puede reducirse para el resto del trimestre. Todo el equipo apoya esta reducción porque todos comparten la responsabilidad del tiempo de actividad.

El presupuesto también ayuda a resaltar algunos de los costos de tener objetivos de confiabilidad excesivamente altos, tanto en términos de inflexibilidad como de innovación lenta. Si el equipo tiene problemas para lanzar nuevas características, pueden optar por aflojar el SLO (aumentando así el presupuesto de errores) para fomentar la innovación.

Perspectivas Clave
Gestionar la confiabilidad del servicio es principalmente gestionar el riesgo, y gestionar el riesgo puede ser costoso.

El 100% de confiabilidad probablemente nunca es el objetivo adecuado: no solo es imposible de lograr, sino que suele ser más de lo que los usuarios de un servicio desean o notan. Es importante adaptar el perfil del servicio al riesgo que el negocio está dispuesto a asumir.

Un presupuesto de errores alinea los incentivos y enfatiza la propiedad conjunta entre SRE y el equipo de desarrollo de productos. Los presupuestos de errores facilitan la toma de decisiones sobre la tasa de lanzamientos, desactivan de manera efectiva las discusiones sobre interrupciones con las partes interesadas y permiten que varios equipos lleguen a las mismas conclusiones sobre el riesgo en producción sin conflictos.

Capítulo 4 - Objetivos de Nivel de Servicio (SLOs)
Escrito por Chris Jones, John Wilkes y Niall Murphy con Cody Smith | Editado por Betsy Beyer

Es imposible gestionar un servicio correctamente, y mucho menos de manera efectiva, sin entender qué comportamientos realmente importan para ese servicio y cómo medir y evaluar esos comportamientos. Con este fin, queremos definir y entregar un nivel de servicio dado a nuestros usuarios, ya sea que utilicen una API interna o un producto público.

Utilizamos la intuición, la experiencia y una comprensión de lo que los usuarios desean para definir indicadores de nivel de servicio (SLIs), objetivos de nivel de servicio (SLOs) y acuerdos de nivel de servicio (SLAs). Estas mediciones describen las propiedades básicas de las métricas que importan, qué valores queremos que tengan esas métricas y cómo reaccionaremos si no podemos proporcionar el servicio esperado. Elegir métricas adecuadas impulsa la acción correcta si algo sale mal y también brinda confianza a los equipos de SRE de que un servicio está saludable.

Este capítulo describe el marco que utilizamos para abordar los problemas de modelado de métricas, selección de métricas y análisis de métricas. Gran parte de esta explicación sería bastante abstracta sin un ejemplo, por lo que utilizaremos el servicio Shakespeare descrito en un capítulo anterior como ilustración.

Terminología de Niveles de Servicio
Muchos lectores están probablemente familiarizados con el concepto de SLA, pero los términos SLI y SLO también merecen una definición cuidadosa, ya que el término SLA se ha sobrecargado y ha tomado diversos significados según el contexto. Preferimos separar esos significados para mayor claridad.

Indicadores
Un SLI (Service Level Indicator) es un indicador de nivel de servicio, una medida cuantitativa cuidadosamente definida de algún aspecto del nivel de servicio proporcionado.

La mayoría de los servicios consideran la latencia de solicitudes—cuánto tiempo tarda en devolver una respuesta—como un SLI clave. Otros SLIs comunes incluyen la tasa de error, a menudo expresada como una fracción de todas las solicitudes recibidas, y el rendimiento del sistema, típicamente medido en solicitudes por segundo. Las mediciones suelen agregarse: es decir, se recopilan datos brutos durante una ventana de medición y luego se convierten en una tasa, promedio o percentil.

Idealmente, el SLI mide directamente un nivel de servicio de interés, pero a veces solo está disponible un proxy, ya que la medida deseada puede ser difícil de obtener o interpretar. Por ejemplo, la latencia del lado del cliente a menudo es la métrica más relevante para el usuario, pero podría ser posible medir solo la latencia en el servidor.

Otro tipo de SLI importante para los SREs es la disponibilidad, o la fracción del tiempo en que un servicio es utilizable. A menudo se define en términos de la fracción de solicitudes bien formadas que tienen éxito, a veces llamada rendimiento. (La durabilidad—la probabilidad de que los datos se mantengan a largo plazo—es igualmente importante para los sistemas de almacenamiento de datos). Aunque el 100% de disponibilidad es imposible, la disponibilidad cercana al 100% suele ser alcanzable, y la industria comúnmente expresa estos valores de alta disponibilidad en términos del número de "nueves" en el porcentaje de disponibilidad. Por ejemplo, disponibilidades del 99% y 99.999% se pueden denominar como "2 nueves" y "5 nueves" de disponibilidad, respectivamente. El objetivo actual publicado para la disponibilidad de Google Compute Engine es "tres nueves y medio"—99.95% de disponibilidad.

Objetivos
Un SLO (Service Level Objective) es un objetivo de nivel de servicio, un valor objetivo o un rango de valores para un nivel de servicio medido por un SLI (Service Level Indicator). Una estructura natural para los SLOs es: SLI ≤ objetivo, o bien límite inferior ≤ SLI ≤ límite superior. Por ejemplo, podríamos decidir que devolveremos los resultados de búsqueda de Shakespeare "rápidamente" y adoptar un SLO en el que la latencia promedio de las solicitudes de búsqueda sea inferior a 100 milisegundos.

Elegir un SLO adecuado es complejo. Para empezar, ¡no siempre puedes elegir su valor! Para las solicitudes HTTP entrantes desde el mundo exterior a tu servicio, la métrica de consultas por segundo (QPS) está esencialmente determinada por los deseos de tus usuarios, y no puedes establecer un SLO para eso.

Por otro lado, puedes decidir que deseas que la latencia promedio por solicitud sea inferior a 100 milisegundos, y establecer tal objetivo podría motivarte a escribir un frontend con comportamientos de baja latencia o a adquirir ciertos tipos de equipos de baja latencia. (Obviamente, 100 milisegundos es un valor arbitrario, pero en general, los números más bajos de latencia son buenos. Existen razones convincentes para creer que rápido es mejor que lento, y que una latencia experimentada por el usuario por encima de ciertos valores ahuyenta a las personas. Ver más detalles en "Speed Matters" [Bru09]).

Nuevamente, esto es más sutil de lo que parece, ya que esos dos SLIs, QPS y latencia, podrían estar conectados tras bambalinas: mayor QPS a menudo conduce a mayores latencias, y es común que los servicios tengan un "desplome" de rendimiento más allá de cierto umbral de carga.

Elegir y publicar los SLOs para los usuarios establece expectativas claras sobre el rendimiento de un servicio. Esta estrategia puede reducir quejas infundadas sobre, por ejemplo, que el servicio es "lento". Sin un SLO explícito, los usuarios a menudo desarrollan sus propias creencias sobre el rendimiento deseado, que pueden no estar relacionadas con las expectativas de quienes diseñan y operan el servicio. Esta dinámica puede llevar tanto a una sobre-reliancia en el servicio, cuando los usuarios creen incorrectamente que el servicio será más disponible de lo que realmente es (como sucedió con Chubby), como a una sub-reliancia, cuando los usuarios potenciales creen que un sistema es menos confiable de lo que realmente es.

Ejemplo: La Interrupción Planificada Global de Chubby
Escrito por Marc Alvidrez

Chubby [Bur06] es el servicio de bloqueo de Google para sistemas distribuidos débilmente acoplados. En el caso global, distribuimos instancias de Chubby de manera que cada réplica esté en una región geográfica diferente. Con el tiempo, descubrimos que las fallas en la instancia global de Chubby generaban constantemente interrupciones de servicio, muchas de las cuales eran visibles para los usuarios finales. Como resultado, las verdaderas interrupciones globales de Chubby eran tan infrecuentes que los propietarios de los servicios comenzaron a agregar dependencias a Chubby asumiendo que nunca fallaría. Su alta confiabilidad proporcionaba una falsa sensación de seguridad, ya que los servicios no podían funcionar correctamente cuando Chubby no estaba disponible, por raro que fuera el caso.

La solución a este escenario de Chubby es interesante: SRE se asegura de que Chubby global cumpla, pero no exceda significativamente su objetivo de nivel de servicio. En cualquier trimestre, si una falla real no ha reducido la disponibilidad por debajo del objetivo, se sintetiza una interrupción controlada al desconectar intencionalmente el sistema. De esta manera, podemos eliminar las dependencias poco razonables en Chubby poco después de que se agreguen, forzando a los propietarios de servicios a enfrentarse con la realidad de los sistemas distribuidos cuanto antes.

Acuerdos (SLAs)
Finalmente, los SLAs (Service Level Agreements) son contratos explícitos o implícitos con tus usuarios que incluyen las consecuencias de cumplir (o no cumplir) los SLOs que contienen. Las consecuencias son más fácilmente reconocibles cuando son financieras—como un reembolso o una penalización—pero pueden tomar otras formas. Una forma sencilla de diferenciar entre un SLO y un SLA es preguntar: "¿Qué pasa si no se cumplen los SLOs?" Si no hay una consecuencia explícita, es casi seguro que estás viendo un SLO.

Los equipos de SRE no suelen involucrarse en la construcción de los SLAs, ya que estos están estrechamente ligados a decisiones comerciales y de producto. Sin embargo, SRE sí se involucra en ayudar a evitar que se activen las consecuencias de no cumplir con los SLOs. También pueden ayudar a definir los SLIs, ya que debe haber una forma objetiva de medir los SLOs en el acuerdo, o de lo contrario surgirán desacuerdos.

Un ejemplo importante de un servicio sin SLA público es Google Search. Queremos que todos usen Search de manera fluida y eficiente, pero no hemos firmado un contrato con todo el mundo. Aun así, si Search no está disponible, hay consecuencias: la reputación de Google se ve afectada y se pierde ingresos por publicidad. Sin embargo, muchos otros servicios de Google, como Google for Work, sí tienen SLAs explícitos con sus usuarios.

Tanto si un servicio tiene SLA como si no, es valioso definir SLIs y SLOs y usarlos para gestionar el servicio.

Indicadores en la Práctica
Dado que hemos explicado la importancia de elegir métricas apropiadas para medir tu servicio, ¿cómo puedes identificar qué métricas son significativas para tu servicio o sistema?

¿Qué es lo que tú y tus usuarios valoran?
No deberías utilizar todas las métricas que puedas rastrear en tu sistema de monitoreo como un SLI. Entender lo que tus usuarios desean del sistema ayudará a seleccionar cuidadosamente algunos indicadores clave. Elegir demasiados indicadores puede hacer que sea difícil prestar atención a los indicadores importantes, mientras que elegir muy pocos puede dejar comportamientos significativos del sistema sin examinar.

Normalmente, encontramos que un puñado de indicadores representativos es suficiente para evaluar y analizar la salud de un sistema.

Categorías Comunes de SLIs
Los servicios tienden a caer en algunas categorías amplias en cuanto a los SLIs que consideran relevantes:

Sistemas de atención al usuario, como los frontends de búsqueda de Shakespeare, suelen centrarse en la disponibilidad, la latencia y el rendimiento. En otras palabras:

¿Podemos responder a la solicitud?
¿Cuánto tiempo tomó responder?
¿Cuántas solicitudes se pueden manejar?
Sistemas de almacenamiento tienden a enfatizar la latencia, la disponibilidad y la durabilidad. Es decir:

¿Cuánto tiempo lleva leer o escribir datos?
¿Podemos acceder a los datos cuando los necesitamos?
¿Siguen los datos disponibles cuando los necesitamos?
Sistemas de grandes volúmenes de datos, como las pipelines de procesamiento de datos, suelen preocuparse por el rendimiento y la latencia de extremo a extremo. En otras palabras:

¿Cuántos datos se están procesando?
¿Cuánto tiempo tarda en procesarse desde la ingestión hasta la finalización?
Todos los sistemas deberían preocuparse por la exactitud: ¿Se devolvió la respuesta correcta? ¿Se recuperaron los datos correctos? ¿Se hizo el análisis correcto? Aunque la exactitud suele ser más una propiedad de los datos que del sistema en sí, es un buen indicador de la salud del sistema, aunque generalmente no es responsabilidad directa de SRE garantizarla.

Recolección de Indicadores
Muchas métricas de indicadores se recopilan de manera más natural en el lado del servidor, utilizando un sistema de monitoreo como Borgmon o Prometheus, o con análisis periódicos de registros. Un ejemplo es medir las respuestas HTTP 500 como una fracción de todas las solicitudes. Sin embargo, algunos sistemas deben instrumentarse con recolección del lado del cliente, ya que no medir el comportamiento en el cliente puede pasar por alto una serie de problemas que afectan a los usuarios pero no a las métricas del servidor. Por ejemplo, centrarse únicamente en la latencia de respuesta del backend de búsqueda de Shakespeare podría no detectar una mala latencia percibida por el usuario debido a problemas con el JavaScript de la página. En este caso, medir cuánto tiempo tarda una página en ser utilizable en el navegador es un mejor proxy de lo que el usuario realmente experimenta.

Agregación
Para simplificar y mejorar la usabilidad, a menudo agregamos las mediciones brutas. Sin embargo, esto debe hacerse con cuidado.

Algunas métricas parecen ser directas, como el número de solicitudes por segundo atendidas, pero incluso esta medición implica la agregación de datos durante una ventana de medición. ¿Se obtiene la medición cada segundo o promediando solicitudes durante un minuto? El segundo enfoque puede ocultar picos mucho más altos en las tasas de solicitudes que duran solo unos segundos. Consideremos un sistema que atiende 200 solicitudes por segundo en segundos pares y 0 solicitudes en los impares. Tiene la misma carga promedio que uno que atiende constantemente 100 solicitudes por segundo, pero su carga instantánea es el doble.

De manera similar, promediar las latencias de solicitud puede parecer atractivo, pero oculta un detalle importante: es completamente posible que la mayoría de las solicitudes sean rápidas, pero que haya un conjunto de solicitudes con una latencia mucho mayor.

La mayoría de las métricas se deben considerar como distribuciones, en lugar de promedios. Por ejemplo, para un SLI de latencia, algunas solicitudes serán atendidas rápidamente, mientras que otras tardarán más—y a veces mucho más. Un promedio simple puede ocultar estas latencias extremas (conocidas como tail latencies), así como cambios en ellas. La Figura 4-1 proporciona un ejemplo: aunque una solicitud típica se atiende en unos 50 ms, ¡el 5% de las solicitudes son 20 veces más lentas! Monitorear y generar alertas solo en función de la latencia promedio no mostraría cambios en el comportamiento a lo largo del día, cuando de hecho hay cambios significativos en la latencia extrema (la línea superior).

Figura 4-1: Percentiles de latencia al 50.º, 85.º, 95.º y 99.º para un sistema
Es importante observar que el eje Y tiene una escala logarítmica. Utilizar percentiles para los indicadores permite analizar la forma de la distribución y sus diferentes atributos. Por ejemplo, un percentil alto, como el 99.º o el 99.9.º, te muestra un valor máximo plausible, mientras que el percentil 50 (también conocido como mediana) destaca el caso típico. Cuanto mayor es la varianza en los tiempos de respuesta, más se ve afectada la experiencia típica del usuario por el comportamiento de la "cola larga" de solicitudes lentas, un efecto que se agrava bajo altas cargas debido a los efectos de colas.

Los estudios de usuarios han mostrado que las personas prefieren un sistema ligeramente más lento en lugar de uno con alta variabilidad en los tiempos de respuesta. Por esta razón, algunos equipos de SRE se enfocan únicamente en los valores percentiles altos, bajo el argumento de que si el comportamiento del percentil 99.9 es bueno, la experiencia típica ciertamente será satisfactoria.

Nota sobre Falacias Estadísticas
Generalmente preferimos trabajar con percentiles en lugar de con la media (promedio aritmético) de un conjunto de valores. Esto permite considerar la cola larga de puntos de datos, que a menudo tienen características significativamente diferentes (y más interesantes) que la media. Dado que los sistemas computacionales tienden a generar datos sesgados (por ejemplo, ninguna solicitud puede tener una respuesta en menos de 0 ms, y un tiempo de espera de 1,000 ms significa que no puede haber respuestas exitosas con valores superiores al tiempo de espera), no podemos asumir que la media y la mediana sean iguales o cercanas.

Es crucial no asumir que nuestros datos están normalmente distribuidos sin verificarlos primero, ya que las intuiciones y aproximaciones estándar pueden no ser válidas. Por ejemplo, si la distribución no es la esperada, un proceso que actúe sobre los valores atípicos (por ejemplo, reiniciar un servidor con latencias de solicitudes altas) podría actuar con demasiada frecuencia o no lo suficiente.

Estandarización de Indicadores
Recomendamos estandarizar las definiciones comunes para los SLIs, de modo que no tengas que razonarlas desde cero cada vez. Cualquier característica que se ajuste a las plantillas de definición estándar puede omitirse de la especificación de un SLI individual. Ejemplos:

Intervalos de agregación: "Promediado cada 1 minuto"
Regiones de agregación: "Todas las tareas en un clúster"
Frecuencia de mediciones: "Cada 10 segundos"
Solicitudes incluidas: "Peticiones HTTP GET desde trabajos de monitoreo en caja negra"
Cómo se adquieren los datos: "A través de nuestro monitoreo, medido en el servidor"
Latencia de acceso a datos: "Tiempo hasta el último byte"
Para ahorrar esfuerzo, construye un conjunto de plantillas de SLI reutilizables para cada métrica común. Esto también facilita que todos entiendan qué significa un SLI específico.

Objetivos en la Práctica
Al definir objetivos de nivel de servicio (SLOs), es crucial comenzar pensando en lo que tus usuarios realmente valoran, no solo en lo que puedes medir fácilmente. A menudo, lo que los usuarios consideran importante es difícil o imposible de medir directamente, por lo que tendrás que aproximarte a sus necesidades de alguna manera. Sin embargo, si simplemente comienzas con lo que es fácil de medir, terminarás con SLOs menos útiles. Por lo tanto, a veces hemos descubierto que trabajar desde los objetivos deseados hacia atrás para encontrar los indicadores específicos es más efectivo que elegir indicadores primero y luego establecer objetivos.

Definición de Objetivos
Para garantizar la máxima claridad, los SLOs deben especificar cómo se miden y bajo qué condiciones son válidos. Por ejemplo:

SLO explícito: "El 99% (promediado durante 1 minuto) de las llamadas Get RPC se completarán en menos de 100 ms (medido en todos los servidores backend)".
SLO simplificado (usando valores por defecto): "El 99% de las llamadas Get RPC se completarán en menos de 100 ms".
Si la forma de las curvas de rendimiento es importante, puedes especificar múltiples objetivos para diferentes percentiles:

El 90% de las llamadas Get RPC se completarán en menos de 1 ms.
El 99% de las llamadas Get RPC se completarán en menos de 10 ms.
El 99.9% de las llamadas Get RPC se completarán en menos de 100 ms.
Si tienes usuarios con cargas de trabajo heterogéneas, como una pipeline de procesamiento por lotes que se preocupa por el rendimiento y un cliente interactivo que se preocupa por la latencia, podría ser apropiado definir objetivos separados para cada tipo de carga de trabajo:

El 95% de las llamadas Set RPC de los clientes de rendimiento se completarán en menos de 1 segundo.
El 99% de las llamadas Set RPC de los clientes de latencia con cargas útiles de menos de 1 kB se completarán en menos de 10 ms.
Permitir Presupuestos de Errores
Es poco realista y no deseable insistir en que los SLOs se cumplan el 100% del tiempo. Hacerlo puede reducir la tasa de innovación, requerir soluciones demasiado conservadoras o ambas. En su lugar, es mejor permitir un presupuesto de errores, que defina a qué ritmo se pueden incumplir los SLOs, y rastrearlo de manera diaria o semanal. Este presupuesto se puede utilizar como insumo para decidir cuándo desplegar nuevas versiones. (Un presupuesto de errores es simplemente un SLO para cumplir otros SLOs).

Selección de Objetivos
Elegir los SLOs adecuados no es una actividad puramente técnica, ya que tiene implicaciones tanto para el producto como para el negocio. Debe haber un equilibrio entre las características del producto y los recursos disponibles, como el personal, el tiempo de comercialización y el hardware. Algunos consejos útiles:

No elijas un objetivo basado solo en el rendimiento actual: Adoptar valores sin reflexión puede forzarte a mantener un sistema que requiere esfuerzos heroicos para cumplir sus objetivos, o que no puede mejorar sin rediseño significativo.
Mantén la simplicidad: Las agregaciones complicadas pueden oscurecer los cambios en el rendimiento del sistema y son más difíciles de razonar.
Evita los absolutos: Es irreal pedir un sistema que escale "infinitamente" sin aumento de latencia o que esté "siempre" disponible.
Ten la menor cantidad de SLOs posible: Elige solo los suficientes para proporcionar una buena cobertura de los atributos de tu sistema. Si no puedes justificar un SLO, probablemente no vale la pena tenerlo.
La perfección puede esperar: Es mejor comenzar con un objetivo amplio y ajustarlo con el tiempo que elegir uno demasiado estricto que deba relajarse más adelante.
Lecciones Clave
Los SLOs son fundamentales para priorizar el trabajo de los equipos de SRE y de desarrollo de productos, ya que reflejan lo que les importa a los usuarios.
Un buen SLO es una herramienta útil para el equipo de desarrollo, mientras que un SLO mal definido puede resultar en un esfuerzo desperdiciado o en un producto deficiente.
Los SLOs son una palanca poderosa: deben utilizarse de manera sabia para equilibrar el rendimiento del servicio y las expectativas de los usuarios.

Medidas de Control
Los SLIs y SLOs son elementos cruciales en los bucles de control utilizados para gestionar sistemas. Este proceso suele seguir cuatro pasos:

Monitorizar y medir los SLIs del sistema.
Comparar los SLIs con los SLOs y decidir si es necesario actuar.
Si es necesario actuar, determinar qué debe hacerse para cumplir con el objetivo.
Tomar acción.
Por ejemplo, si en el paso 2 se detecta que la latencia de las solicitudes está aumentando y que se incumplirá el SLO en unas horas a menos que se haga algo, el paso 3 podría incluir probar la hipótesis de que los servidores están saturados de CPU y decidir agregar más servidores para distribuir la carga. Sin el SLO, no sabrías si es necesario tomar medidas ni cuándo hacerlo.

Los SLOs Establecen Expectativas
Publicar SLOs establece expectativas claras sobre el comportamiento del sistema. Los usuarios (y potenciales usuarios) a menudo quieren saber qué pueden esperar de un servicio para comprender si es adecuado para su caso de uso. Por ejemplo, un equipo que desee construir un sitio web para compartir fotos podría evitar usar un servicio que promete alta durabilidad y bajo costo a cambio de una disponibilidad ligeramente inferior, aunque ese mismo servicio podría ser perfecto para un sistema de gestión de archivos.

Para establecer expectativas realistas para tus usuarios, considera utilizar una o ambas de las siguientes tácticas:

Mantén un Margen de Seguridad
Usar un SLO interno más estricto que el SLO anunciado a los usuarios te da margen para responder a problemas crónicos antes de que se vuelvan visibles externamente. Un buffer en el SLO también te permite acomodar cambios en el sistema que priorizan otros atributos, como el costo o la facilidad de mantenimiento, sin tener que decepcionar a los usuarios.

No Sobrecumplas
Los usuarios dependen de la realidad de lo que ofreces, no solo de lo que dices que entregarás. Si el rendimiento real de tu servicio es mucho mejor que su SLO declarado, los usuarios comenzarán a depender de ese rendimiento actual. Puedes evitar una sobredependencia deliberadamente tomando el sistema offline ocasionalmente (como lo hizo Google con el servicio Chubby, introduciendo interrupciones planificadas), limitando algunas solicitudes o diseñando el sistema para que no sea más rápido bajo cargas ligeras.

¿Invertir o Cambiar Prioridades?
Comprender qué tan bien un sistema está cumpliendo con sus expectativas ayuda a decidir si es necesario invertir en hacerlo más rápido, disponible o resistente. Por otro lado, si el servicio está funcionando bien, el equipo puede enfocarse en otras prioridades, como pagar deuda técnica, agregar nuevas características o lanzar nuevos productos.

Acuerdos en la Práctica
Elaborar un SLA (Service Level Agreement) requiere que los equipos de negocio y legales seleccionen las consecuencias y penalidades apropiadas para un incumplimiento. El rol del equipo de SRE es ayudarlos a entender la probabilidad y la dificultad de cumplir con los SLOs contenidos en el SLA. Muchos de los consejos sobre la construcción de SLOs también son aplicables a los SLAs.

Es recomendable ser conservador en lo que se promete a los usuarios, ya que, cuanto más amplio sea el grupo de usuarios, más difícil será cambiar o eliminar SLAs que resulten ser poco sabios o difíciles de cumplir.

Es importante notar que, cuando la gente habla de una "violación de SLA", en realidad casi siempre se refieren a un SLO no cumplido. Una verdadera violación de SLA podría desencadenar una disputa legal por incumplimiento de contrato.

Consejos para SLAs:
Ser conservador en lo que se promete: Es mejor prometer menos y cumplir consistentemente, que prometer demasiado y no cumplir.

Apoyar a los equipos legales y de negocio: El equipo de SRE debe ayudar a estos equipos a evaluar la viabilidad técnica de los SLOs antes de comprometerse a un SLA.

Evitar la sobrepromesa: SLAs demasiado ambiciosos son difíciles de cambiar, y si no se pueden cumplir, pueden tener consecuencias legales y financieras serias.

Capítulo 5 - Eliminando el Trabajo Repetitivo (Toil)
Escrito por Vivek Rau | Editado por Betsy Beyer

"Si un operador humano necesita intervenir en tu sistema durante las operaciones normales, tienes un error."
— Carla Geisser, Google SRE

En SRE, nuestro objetivo es dedicar tiempo a proyectos de ingeniería a largo plazo en lugar de a trabajo operativo repetitivo. Debido a que el término "trabajo operativo" puede interpretarse de manera errónea, usamos un término específico: toil (trabajo repetitivo y monótono).

Definición de Toil
Toil no es simplemente "trabajo que no me gusta hacer". Tampoco es equivalente a tareas administrativas o trabajos monótonos. Las preferencias respecto al tipo de trabajo que se considera satisfactorio varían de una persona a otra, y algunas personas incluso disfrutan del trabajo manual y repetitivo. También hay tareas administrativas que deben realizarse, pero no deben clasificarse como toil; estas se consideran sobrecarga. La sobrecarga incluye tareas como reuniones de equipo, establecimiento y evaluación de objetivos, y trámites de recursos humanos.

El trabajo que parece tedioso, pero tiene valor a largo plazo, tampoco es toil. Por ejemplo, limpiar toda la configuración de alertas de tu servicio puede ser tedioso, pero no es toil.

¿Qué es Toil?
Toil es el tipo de trabajo asociado con la operación de un servicio de producción que tiende a ser manual, repetitivo, automatizable, táctico, sin valor duradero, y que crece linealmente a medida que crece el servicio. No todas las tareas clasificadas como toil tienen todas estas características, pero cuanto más se ajuste una tarea a las siguientes descripciones, más probable es que sea toil:

Manual: Esto incluye tareas como ejecutar manualmente un script que automatiza un proceso. Aunque ejecutar un script puede ser más rápido que realizar cada paso manualmente, el tiempo que un humano dedica a ejecutarlo sigue siendo tiempo de toil.

Repetitivo: Si estás realizando una tarea por primera o segunda vez, no es toil. Toil es el trabajo que haces una y otra vez. Resolver un problema novedoso o crear una nueva solución no es toil.

Automatizable: Si una máquina puede realizar la tarea tan bien como un humano, o si la necesidad de la tarea puede ser eliminada mediante diseño, es toil. Si el juicio humano es esencial para la tarea, probablemente no es toil.

Táctico: Toil es reactivo, impulsado por interrupciones, en lugar de ser estratégico y proactivo. Atender alertas del sistema es toil. Puede que nunca logremos eliminar por completo este tipo de trabajo, pero debemos trabajar continuamente para minimizarlo.

Sin valor duradero: Si el estado del servicio no cambia después de completar la tarea, probablemente fue toil. Si la tarea produjo una mejora permanente, no fue toil, incluso si involucró trabajo pesado, como revisar código y configuraciones heredadas.

Escala lineal con el crecimiento del servicio: Si el trabajo de una tarea crece de forma lineal con el tamaño del servicio, el volumen de tráfico o el número de usuarios, probablemente es toil. Un servicio bien diseñado puede crecer significativamente sin requerir trabajo adicional, salvo esfuerzos únicos para agregar recursos.

La eliminación de toil es un principio clave en SRE, ya que libera tiempo para enfocarse en proyectos que aporten valor a largo plazo al servicio.